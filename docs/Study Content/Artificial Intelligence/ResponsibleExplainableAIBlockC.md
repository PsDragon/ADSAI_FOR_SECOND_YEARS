---
sort: 7
---

# __Responsible & Explainable AI__

## __Assignment__

For this part of the Project Brief, you are going to identify, and describe the limitations of an AI algorithm in terms of fairness, transparency, and interpretability, and subsequently apply methods which address these limitations.

<img src="./images/grad_cam.jpg" alt="SQL meme" width="600"/> \
*Figure 1. Example of feature attribution with Grad-CAM.*

__Requirements:__

- Identify, and describe instances of bias in the dataset. See content on the different types of bias presented in the works of Mehrabi et al. (2019), Suresh and Guttag (2019), Khan et al. (2021).  
- Identify, and describe the limitations in terms of transparency and interpretability of the AI algorithm. See article by Tsimenidis (2020), and Google's Responsible AI post on interpretability.
- Apply at least one responsible AI method that adresses the datasets' limitation in terms of bias, and subsequently evaluate its impact.
- Apply at least one explainable AI method that adresses the algorithm's limitations in terms of transparancy and interpretability, and subsequently evaluate its impact. See workshop examples, the article by Linardatos et al. (2021), and the book by Molnar (2020).
- Apply multiple responsible and explainable AI methods that adresses limitations in terms of bias, transparancy and interpretability, and subsequently contrast their impact on the classification task. See previously mentioned literature.

__Deliverable(s):__

- A Jupyter Notebook that contains relevant code and explanations.

The Jupyter notebook is to be uploaded to Github no later than 5pm on last DataLab day. Confer with a lecturer beforehand if you are handing in something other than a Jupyter Notebook.

***

## __Literature__

Friedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems (TOIS), 14(3), 330-347. (Bias in Computer Systems.pdf (cornell.edu)

Karanasiou, A. P., & Pinotsis, D. A. (2017). A study into the layers of automated decision-making: emergent normative and legal aspects of deep learning. International Review of Law, Computers & Technology, 31(2), 170-187.

Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2021). Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1), 18.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019). A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635.

Meta-Learning. (2020, September). Retrieved July 09, 2021, from https://meta-learning.fastforwardlabs.com/#model-agnostic-meta-learning-(maml)

Minsky, M. L. (1991). Logical versus analogical or symbolic versus connectionist or neat versus scruffy. AI magazine, 12(2), 34-34. (https://www.aaai.org/ojs/index.php/aimagazine/article/view/894/812)

Mohamed, S., Png, M. T., & Isaac, W. (2020). Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology, 33(4), 659-684.

Molnar, C. (2020). Interpretable machine learning. https://christophm.github.io/interpretable-ml-book/.

Responsible AI practices: Interpretability. (n.d.). Retrieved July 09, 2021, from https://ai.google/responsibilities/responsible-ai-practices/?category=interpretability

Suresh, H., & Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002.

Tsimenidis, S. (2020). Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning. arXiv preprint arXiv:2012.15754.
