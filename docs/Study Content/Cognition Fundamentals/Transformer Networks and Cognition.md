---
sort: 10
---

# Transformer Networks and Cognition
The connection between human cognition and transformer networks lies in the ability of transformer networks to model and process complex sequential data, such as natural language. Furthermore, both sample the most relevant information previously observed for further perceptual processing and generate information based on it.

Human cognition involves a wide range of processes related to perception, attention, memory, and decision-making, all of which involve the processing of complex and dynamic streams of information. Similarly, transformer networks are designed to process sequential data by attending to different parts of the sequence at different times and encoding this information in a way that can be used for downstream tasks such as language generation or image classification.

Transformer networks are modeled after the architecture of the human brain, with layers of processing nodes that gradually build up representations of the input data. This process is similar to the way that humans process information, with neural connections forming and strengthening over time as new information is learned and integrated into existing knowledge. One key advantage of transformer networks is their ability to learn and adapt to new information and contexts over time. This is similar to the way that humans continually update their mental models of the world in response to new experiences and information.

Therefore, the connection between human cognition and transformer networks lies in their shared ability to modulate attention (or mymic it in the case of transformer networks). Which allows both to process complex and dynamic streams of information in a flexible and adaptive way. While there are still many open questions about the nature of this connection, ongoing research in fields such as cognitive neuroscience and artificial intelligence. The developments in the fields of computer vision (DALL-E) and natural language processing (chapGPT) is helping to shed new light on the ways that these two systems intersect and inform each other.

## Learning Objectives: 
1. Understand and evaluate the simmilarities and distictions between transformer networks used in state-of-the-art large language models and human cognitive system in the context of language aquisition & auditory perception. 


## Lecture - Transformer Networks and Cognition
Live lecture which is given on the 3rd of March from 10:00 till 11:00. You can find the slides [here](LINK) and after the lecture, there should be [a recording available here!](LINK)

Relevant literature to research:
1. [Alishahi, A. (2010). Computational modeling of human language acquisition. Synthesis Lectures on Human Language Technologies, 3(1), 1-107.](https://github.com/BredaUniversityADSAI/ADS-AI/blob/d42ffd034a2d1a35752d7c83c30300288a4ae153/docs/Study%20Content/Cognition%20Fundamentals/assets/sources/Computational%20Modeling%20of%20Human%20Language%20Acquisition.pdf)
2. [Summary of Concepts and Terms](https://github.com/BredaUniversityADSAI/ADS-AI/blob/260f3f788372cfb55e2a4ba2956800de1b3b14a8/docs/Study%20Content/Cognition%20Fundamentals/assets/Summary%20of%20Concepts%20and%20Terms.docx): Highly recommended read which summarizes most important concepts discussed in the literature regarding cognition. It also contains various important models you can use to create cognitive systems; or systems to deal with information processing in general.
3. [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://github.com/BredaUniversityADSAI/ADS-AI/blob/8fcd25ff6aa725d6aeaafab2657413b6304b174a/docs/Study%20Content/Cognition%20Fundamentals/assets/sources/Attention%20is%20all%20you%20need%20-%20Transformer%20Networks%20Introduction.pdf)