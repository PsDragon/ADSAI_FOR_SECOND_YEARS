---
sort: 10
---

# Week 3, Day 5, DataLab: MLP from scratch 2

In the previous DataLab you started working on `W3-DL-MLP-from-scratch-Student-Notebook.ipynb` and worked on Tasks 1 to 5. In this DataLab, you will focus on Task 6, gradient descent. Gradient descent algorithm is the essence of neural networks because this is what we call learning.

Here is the outline of the tasks again

- [ ] **Task 1**: Given an MLP architecture (e.g. `[2, 3, 1]`), define a function that returns random weights and biases around zero.

- [ ] **Task 2**: Implement the sigmoid activation function.

- [ ] **Task 3**: Implement the predict function.

- [ ] **Task 4**: Define a function that calculates accuracy.

- [ ] **Task 5**: Define a function that calculates mean square error (loss).

- [ ] **Task 6**: Define a function that implements a naive gradient descent algorithm.
