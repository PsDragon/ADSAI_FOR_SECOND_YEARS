---
sort: 10
---

# Week 3, Day 5, DataLab: MLP from scratch 2

In the previous DataLab you started working on `W3-DL-MLP-from-scratch.ipynb` (Tasks 1 to 5). In this DataLab, you will focus on Task 6, gradient descent. Gradient descent algorithm is the essence of neural networks because this is what we call learning.

Here is the link to notebook again:

[<img src="./images/githubbadge.png" alt="GitHub" width="15%"/>](https://github.com/BredaUniversityADSAI/2022-23-Y1-BlockC/blob/main/DataLabs/W3/W3-DL-MLP-from-scratch.ipynb)

Outline:

- [ ] **Task 1**: Given an MLP architecture (e.g. `[2, 3, 1]`), define a function that returns random weights and biases around zero.

- [ ] **Task 2**: Implement the sigmoid activation function.

- [ ] **Task 3**: Implement the predict function.

- [ ] **Task 4**: Define a function that calculates accuracy.

- [ ] **Task 5**: Define a function that calculates mean square error (loss).

- [ ] **Task 6: Define a function that implements a naive gradient descent algorithm.**
