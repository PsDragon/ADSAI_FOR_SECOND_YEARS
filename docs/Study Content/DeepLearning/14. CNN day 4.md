---
sort: 14
---

# Week 4, Day 4: Working with small datasets

As you know, more data - if properly collected - increases the performance of machine learning models. There is no clear definition of a small dataset because it depends on the problem, dataset and the model. But roughly speaking an image dataset with less that 10,000 images can be considered small. Large datasets contain millions of images. It is very common to encounter small datasets in the real world. Therefore it is important to learn how to work with small datasets. 

Best way to handle small datasets is to not work with small datasets. Try to collect more data if possible. I was once working with a client who had around 100-200 images. I started accepted the project but requested more data whenever possible. At the end of the project we had more than 2000 images. It is still not much but way better than what we started with.

## 1) Collecting more data

You can always ask your client to provide more data, they might have it, or they might collect it. Moreover, there are lots of online resources you can use to collect data. Always avoid violating copyright terms.

- [ ] Downloading images from [online search](https://www.youtube.com/watch?v=OQydrlSzxnE).
- [ ] Google has a [dataset search](https://datasetsearch.research.google.com/) engine.
- [ ] There are journals such as [scientific data](https://www.nature.com/sdata/).
- [ ] There are platforms such as [zenodo](https://zenodo.org/).

## 2) Data augmentation

Data augmentation is a technique used to artificially increase the size of a dataset by generating modified versions of existing data. This is often used in machine learning when the original dataset is small, to prevent overfitting and improve the generalization of the model.

There are many ways to augment data, depending on the type of data and the task at hand. Some common techniques for image data include:

- Flipping the image horizontally or vertically
- Rotating the image by a certain degree
- Cropping the image to a different aspect ratio
- Adding noise or blur to the image

For text data, common augmentation techniques include:

- Synonym replacement: replacing a word with a synonym
- Random insertion: inserting a random word or phrase
- Random swap: swapping two adjacent words or phrases

Data augmentation can be a very effective way to increase the size and diversity of a dataset, but it is important to be careful not to introduce unrealistic or biased examples. It is also important to ensure that the augmented data is still representative of the real-world data that the model will be used on.

## 3) Transfer learning

In 2013, Matthew Zeiler and Rob Fergus published an influential paper titled [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901). They introduced a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. It turns out that the first layers learn simple features (e.g. edges) whereas last layers learn complex features (e.g. faces) related to the task. Since almost all objects share features such as edges, color, shape, we might be able to take a neural network trained on a very large dataset and adjust it for our own problem. This is called transfer learning. It is particularly useful when the second task has a small amount of training data, as it allows the model to learn from the knowledge gained on the first task and transfer it to the second task.

Transfer learning has proven to be very successful in a variety of applications, including image classification, natural language processing, and speech recognition. For example, a model trained on a large dataset of images of animals can be fine-tuned to classify a small dataset of images of plants with a high level of accuracy.

There are two main approaches to transfer learning: feature-based transfer and fine-tuning. In feature-based transfer, the lower layers of the model are used to extract features from the input data, while the upper layers are re-trained on the new task using these features. In fine-tuning, the entire model is re-trained on the new task, with the weights from the original model serving as initial weights.

Overall, transfer learning is a powerful tool for improving the performance of machine learning models in a wide range of applications.

### 3.1) Keras example

First, we need to choose a pre-trained model to use as the base model. For this example, we will use the VGG16 model, which is a popular choice for image classification tasks. We will also use the ImageNet dataset, which the VGG16 model was trained on, as our training data.

```
from keras.applications import VGG16

# Load the VGG16 model
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
```


Next, we will add a few layers on top of the base model to create a new model that is suited for our classification task. We will use the base model as a feature extractor, and only train the added layers.

```
from keras.layers import Flatten, Dense
from keras.models import Model

# Add a new top layer
x = base_model.output
x = Flatten()(x)
predictions = Dense(num_classes, activation='softmax')(x)

# This is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)
```

Now, we will freeze the base model so that the weights of the pre-trained model are not updated during training. We will only train the added layers.

```
# Freeze the base model
for layer in base_model.layers:
    layer.trainable = False
```

Finally, we can compile and train the model as we would any other Keras model.

```
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=num_epochs)
```

## 4) Assignment

From the book DL with Python

- [ ] Read Chapter 8 Section 3: Leveraging a pretrained model


