---
sort: 6
---

# Introduction to TensorFlow

The goal of this unit is to take the abstract concepts about deep learning you have learned and translate them into Python using TensorFlow. In this section, you will find code your own neural network and investigate ways to fine-tune your program with hyperparameters.

After this unit, you will be able to:

1. Translate abstract deep learning concepts into Python
2. Design a neural network from scratch
3. Use deep learning models to solve regression problems
4. Tune the hyperparameters of your model and improve its performance through iterative testing


## Implementing Neural Networks

A neural network, just like any machine learning method, learns how to perform tasks by processing data and adjusting its model to best predict the desired outcome. 

Most popular machine learning tasks are:

**Classification:** given data and true labels or categories for each data point, train a model that predicts for each data example what its label should be. For example, given data of previous fire hazards, our model can learn how to predict whether a fire will occur for a given day in the future, with all the factors taken into account.

**Regression:** given data and true continuous value for each data point, train a model that can predict values for each data example. For example, given the previous stock market data, we can build a regression model that forecasts what the stock market price will be at a specific point in time when the data is available.

Parametric models such as neural networks are described by parameters - configuration variables representing the modelâ€™s knowledge. We can tweak the parameters using the training data and we can evaluate the performance of the model using hold-out test data the model has not seen during training.

<figure>
    <img src=".\assets\introduction_diagram.PNG" />
</figure>
<br>

Take a look at the main components of a neural network learning pipeline depicted in the workspace:

1. Input data: this is used to train a neural network model you need to provide it with some training data.

2. An optimizer: this is an algorithm that based on the training data adjusts the parameters of the network in order to perform the task at hand.

3. A loss or cost function: this informs the optimizer whether it is doing a good job on the training data and how to adjust the parameters in the right direction.

4. Evaluation metrics: these tell us how well the current model performs on validation data. For example, mean absolute error for regression tells us how far the predictions are on average from the true values.