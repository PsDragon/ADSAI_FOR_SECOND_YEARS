---
sort: 7
---

# Neural Network model

## Neural Network model: layers

Layers are the building blocks of neural networks and can contain 1 or more neurons. Each layer is associated with parameters: weights, and bias, that are tuned during the learning. A fully-connected layer in which all neurons connect to all neurons in the next layer is created the following way in TensorFlow:

```python
from tensorflow.keras import layers
# we chose 3 neurons here
layer = layers.Dense(3) 
```
This layer looks like this graphically:

<figure>
    <img src=".\assets\layers_diagram.svg" />
</figure>
<br>

Pay attention to the dimensions of the weight and bias parameter matrices. Since we chose to create a layer with three neurons, the number of outputs of this layer is 3. Hence, the bias parameter would be a vector of (3, 1) dimensions. But what is the first dimension of the weights matrix? Without knowing how many features or input nodes are in the previous layer, we have no way of knowing! For that reason, with the following code:

```python
print(layer.weights)
```

we get an empty array since no input layer is specified. However, if we write:

```python
# 13388 samples, 11 features as in our dataset
input = tf.ones((1338, 11))
# a fully-connected layer with 3 neurons
layer = layers.Dense(3) 
# calculate the outputs
output = layer(input) 
# print the weights
print(layer.weights) 
```

we get that the weight matrix has shape = (11, 3) and the bias matrix has shape=(3,). Compare these weights with the diagram above to make sure you can associate the resulting shapes to it.

Fortunately, we don’t have to worry about this. TensorFlow will determine the shapes of the weight matrix and bias matrix automatically the moment it encounters the first input.

## Neural Network model: input layer

Inputs to a neural network are usually not considered the actual transformative layers. They are merely placeholders for data. In Keras, an input for a neural network can be specified with a tf.keras.layers.InputLayer object.

The following code initializes an input layer for a DataFrame my_data that has 15 columns:

```python
from tensorflow.keras.layers import InputLayer
my_input = InputLayer(input_shape=(15,))
```

Notice that the input_shape parameter has to have its first dimension equal to the number of features in the data. You don’t need to specify the second dimension: the number of samples or batch size.

The following code avoids hard-coding with using the .shape property of the my_data DataFrame:

```python
#get the number of features/dimensions in the data
num_features = my_data.shape[1] 
#without hard-coding
my_input = tf.keras.layers.InputLayer(input_shape=(num_features,)) 
```

The following code adds this input layer to a model instance my_model:

```python
my_model.add(my_input)
```

The following code prints a useful summary of a model instance my_model:

```python
>print(my_model.summary())
>0
```

As you can see, the summary shows that the total number of parameters is 0. This shows you that the input layer has no trainable parameters and is just a placeholder for data.


## Neural Network model: output layers
The output layer shape depends on your task. In the case of regression, we need one output for each sample. For example, if your data has 100 samples, you would expect your output to be a vector with 100 entries - a numerical prediction for each sample.

In our case, we are doing regression and wish to predict one number for each data point: the medical cost billed by health insurance indicated in the charges column in our data. Hence, our output layer has only one neuron.

The following command adds a layer with one neuron to a model instance my_model:

```python
from tensorflow.keras.layers import Dense
my_model.add(Dense(1))
```

Notice that you don’t need to specify the input shape of this layer since Tensorflow with Keras can automatically infer its shape from the previous layer.

## Neural network model: hidden layers

So far we have added one input layer and one output layer to our model. If you think about it, our model currently represents a linear regression. To capture more complex or non-linear interactions among the inputs and outputs neural networks, we’ll need to incorporate hidden layers.

The following command adds a hidden layer to a model instance my_model:

```python
from tensorflow.keras.layers import Dense
my_model.add(Dense(64, activation='relu'))
```

We chose 64 (2^6) to be the number of neurons since it makes optimization more efficient due to the binary nature of computation.

With the activation parameter, we specify which activation function we want to have in the output of our hidden layer. There are a number of activation functions such as softmax, sigmoid, but ReLU (relu) (Rectified Linear Unit) is very effective in many applications and we’ll use it here.

Adding more layers to a neural network naturally increases the number of parameters to be tuned. With every layer, there are associated weight and bias vectors.

In following diagram below we show the size of parameter vectors with each layer. In our case, the 1st layer’s weight matrix (red) has shape (11, 64) because we feed 11 features to 64 hidden neurons. The output layer (purple) has the weight matrix of shape (64, 1) because we have 64 input units and 1 neuron in the final layer.

<figure>
    <img src=".\assets\hidden_layers_diagram.svg" />
</figure>
<br>


