---
sort: 8
---

# Optimizers

As we mentioned, our goal is for the network to effectively adjust its weights or parameters in order to reach the best performance. Keras offers a variety of optimizers such as SGD (Stochastic Gradient Descent optimizer), Adam, RMSprop, and others.

We’ll start by introducing the Adam optimizer:

```python
from tensorflow.keras.optimizers import Adam
opt = Adam(learning_rate=0.01)
```

The learning rate determines how big of jumps the optimizer makes in the parameter space (weights and bias) and it is considered a hyperparameter that can be also tuned. While model parameters are the ones that the model uses to make predictions, hyperparameters determine the learning process (learning rate, number of iterations, optimizer type).

If the learning rate is set too high, the optimizer will make large jumps and possibly miss the solution. On the other hand, if set too low, the learning process is too slow and might not converge to a desirable solution with the allotted time. Here we’ll use a value of 0.01, which is often used.

Once the optimizer algorithm is chosen, a model instance my_model is compiled with the following code:

```python
my_model.compile(loss='mse',  metrics=['mae'], optimizer=opt)
```

loss denotes the measure of learning success and the lower the loss the better the performance. In the case of regression, the most often used loss function is the Mean Squared Error mse (the average squared difference between the estimated values and the actual value).

Additionally, we want to observe the progress of the Mean Absolute Error (mae) while training the model because MAE can give us a better idea than mse on how far off we are from the true values in the units we are predicting. In our case, we are predicting charge in dollars and MAE will tell us how many dollars we’re off, on average, from the actual values as the network is being trained.

