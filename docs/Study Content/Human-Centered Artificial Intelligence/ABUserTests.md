---
sort: 6
---

# A/B User Tests
Today we will dive deeper into user feedback and user testing. How do we really know what works and what doesn't? This is one of the most difficult topic in UX-design and all the more within HCAI. How do we know what results from what? How do we isolate a given factor an pinpoint it as the cause for a given functionality? 

Then we are going to take our new knowledge; as well as the feedback on your design collected last datalab, and apply it to our design to create a new vertical slice for our wireframe prototype!

## Learning Objectives:
After this module, you'll be able to:
1. create and conduct various user tests in UX;
2. understand the basic principles of A/B testing
3. design an A/B test
4. deploy a vertical slice for your wireframe prototype for a user test

Table of contents:
1. User Testing: 2 hours
2. A/B testing: 4 hour
3. Wireframe Iteration: 2 hours


**Now, open your worklog and plan the different tasks for today in there!**

## Questions?

If you have questions or issues regarding the course material, please post a message on the 'Q&A' channel in Teams. The channel will be monitored by mentors but also please help your peers if you have the answer to their question. 

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
Tip: Note down any important HCAI related questions you might have! You can ask them during Datalab in person to the responsible lecturer for this module: Bram Heijligers
 </div>

Good luck!



## 1) User Testing
All products should include some form of user testing, whether they are digital or not, interactive or non-interactive: as a designer you generally know too much about the product and technology relating to your own project. You cannot simply pretend you don't know these things; that's not how your brain works, and therefore we perform user tests to create a ecologically valid setup to test our designs!

**Do the full LinkedIn Learning course below and save the certificate to your github and evidence it in your learning- and worklog**

<div style="position:relative;height:0;padding-bottom:56.25%"><iframe width="640" height="360" src="https://www.linkedin.com/learning/embed/ux-foundations-usability-testing/usability-testing?autoplay=false&claim=AQHOrBs7sRA7bgAAAYc4G3VwRyjtl09DSc4hm2BGzmz2HsgpG5KVMahFKsi9Q0uLPb8IOCnV8sWLMhQEBEVKvZC2wxi5isYrdN3Z2-b86a29jGkygoTySt1wNGwPpAbxKc4D6T9jbbmqLNY8HgZ0AMRqLzTbCA3zXWfr7dWabvW1B3e3I-xfIZv2VRYQNrh2WYobbvW5txD-pwKwm9ZCAzSeJ_DxS5xUlCg6DUco_9_N0MOCUYvjM8SwkZrgA9YGZr1Mig_zFbqjLHOIlh5ClV8C3_hKhfF3dEhRdz0FRaZIz25uHy9T-pn1T_nGas9YxhDN-T76LtLQi-oYilwh9fkfir_QuGtXrJSc-c-ZiwHFBURsz6D3z90xCNZw6r62GgF_JS50TyOd7T-WC0mRfLkXYXKyfLF3zFurYbndE0Vo1FxK76zGzME3H5f0GtsV17Z7yMpZ6GzmUPIjMHtrFtmPtMakqtJoRrsjtteJ-3CvNwW45SfQubixSj6LzhkdOyLEsE856_4sEkDEoF8987FzKsklDcVGe5xasNJcrEMIcK5eJ21MPoKCFiKnljkIKCsGcsn7DgvvQbMRsUwKQ51sZO73t1uc3HcVTSF-6tgtqeqVlfnFNPQ7g-bclf8CkOgmAVwnL4qNNI_rz9GRswnlV7uc9w3pIoFx7ykwkYjXq6-lvQcMasJCWXfLL5bEJ1oEAZYIigrD4dHdIolW8ww5Bg9DweppUwtFHr_bGktnuTVCCPv3BoAJvIiY858hl2Mjlf4d_KBac5d-ms4C7lJBtydLAn2JdqsMvK3KIIhJh8lfaHWw-zTHcZfmD8l3eDYq0E8fDqbwbpiyb7CNxt1Rwe9JvVLfGx8uHRcM72T8VSBhFEaWd3ybQ6fRLTfyUrnfR-vdhDPMlj5H5a2R6uhguDbv6xJuCoey408xI7tOfOWgWiZsoDaSnNBVFQ37pi_LalToKyRWj29AQus-rnysgjBzZhkUB-0jqd1e5Sn_RtFchsj5pm9L1I4GX_WLO3-zVVPUXesgyDnNxpYzfRNKz-UKk-c6uuKUI3FiJBZZiI0QlZumM72UjYzidNXV9vwjK-5GQl29L2zakSypmdfCoaspRNnicymbBW-BF05K4Clpd0--F8wCxUlYRjYULxdRh2PZUxDK-Km5VcaXpV5Dzuvh_wiuUgPiHUY5e0WhwUiOh6o&lipi=urn%3Ali%3Apage%3Ad_learning_content%3B6%2BWZR3jcSh%2BYV2ReH3Ov5Q%3D%3D&licu" mozallowfullscreen="true" webkitallowfullscreen="true" allowfullscreen="true" frameborder="0" style="position:absolute;width:100%;height:100%;left:0"></iframe></div><p><strong><a href="https://www.linkedin.com/learning/ux-foundations-usability-testing/usability-testing?trk=embed_lil">Usability testing</a></strong> from <strong><a href="https://www.linkedin.com/learning/ux-foundations-usability-testing?trk=embed_lil">UX Foundations: Usability Testing</a></strong> by <strong><a href="https://www.linkedin.com/learning/instructors/chris-nodder?trk=embed_lil">Chris Nodder</a></strong></p>


### DataLab preparation, Excercise 1:
**Make sure to get the certificate an upload it to GitHub and evidence it in both your worklog and learning log!**




## 2) A/B Testing
A/B testing, also known as split testing, is a method of comparing two different versions of a webpage or app to determine which one performs better. The process involves randomly dividing users into two groups, with each group being shown a different version of the page. The results are then analyzed to determine which version is more effective at achieving the desired outcome, such as increasing sales or conversions.  With the A/B test you only test one variable at a time and to ensure that your sample size is large enough to achieve [statistically significant results](https://hbr.org/2016/02/a-refresher-on-statistical-significance). 


Don't worry about your sample size for now, we'll take care of that in Datalab next week. In Year 2, block A we're going to investigate how you can determine an appropriate sample size for a given statistical test.

The video below explains A/B testing in detail.

<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/DUNk4GPZ9bw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>
<!-- blank line -->

*Video 1. 'A/B Testing in Data Science Interviews by a Google Data Scientist.DataInterview.*

### Practical example or the key to success (an interesting read)
Booking.com is a popular online travel agency known for its extensive and successful use of A/B testing to improve its website. The company runs over 1,000 tests per day, which amounts to more than 400,000 tests per year. Booking.com's A/B testing program is considered one of the largest and most sophisticated in the industry.

One of the keys to Booking.com's success is its dedicated A/B testing team. The team consists of experienced data analysts, developers, and designers who work together to identify opportunities for optimization and run tests to validate their hypotheses. The team follows a structured approach to A/B testing, starting with clear hypotheses based on data analysis and user feedback.

Before starting a test, the team defines clear goals and [KPIs](https://www.kpi.org/kpi-basics/) to ensure that the results are meaningful and relevant to the business. They also take care to ensure that the tests are statistically valid by using appropriate sample sizes and testing durations. This helps to minimize false positives and false negatives and ensure that the results are reliable.

![Process](https://assets.website-files.com/614e240a0e0b0fa20bb1471d/62cc7d60213919156901e2e1_48842-0lfi-1skwvnhnrx0n-2.png)

*Figure 1. Booking.comâ€™s Testing Process*

Booking.com's A/B testing program is supported by sophisticated machine learning algorithms and automation tools. These tools help the team to identify patterns and trends in the test results and suggest new hypotheses for further testing. They also help to streamline the testing process by automating tasks such as test setup, data collection, and analysis.

Through its extensive use of A/B testing, Booking.com has been able to continuously optimize its website and improve its user experience. Some of the notable improvements that have resulted from A/B testing include faster page load times, more prominent [calls-to-action](https://en.wikipedia.org/wiki/Call_to_action_(marketing)), and personalized search results. Booking.com's success with A/B testing has made it a model for other businesses looking to improve their online presence and user experience.

### DataLab preparation, Excercise 2. Planning for A/B testing.
**open your Block C Notes - Y1C_2022-23_ADSAI document, create a section called 'Planning for A/B testing' and do the following excercises.**
1. Summarize the steps that you need to conduct for A/B testing. 
2. Open you learning log and take a look at the feedback you collected from your peers and lecturers during Datalab this week. Create an ordered list (best on on top, least valuable one at the bottom) and write down one or more iterating options for your application design. Make sure to iterate on your user stories based on the feedback. If you already incorperated all feedback received so far, just briefly write down here that you already did so and skip this step.
3. Consider the 4 (or more) design iterations you made in proto.io in yesterday's self-study material. Which ones are of particular value do you think? Add them to the ordered  list and argue why it is or isn't the most valuable one. Then consider the iterations you made in excercise two and add those to the list, reorder the list and explain again why which one is best.
4. Now, let's select two design you really want to compare to see which one is best. Follow the outline of the steps for A/B testing; and remember that you want the two prototype versions to only differ in 1 isolated aspect (e.g. one includes a mechanic for the user to provide feedback on the classification; user rates the output provided, the other version doesn't but is the same in all other aspects). Essentially, you will have one baseline wireframe (the one without the design change you want to test); version A, and one wireframe with the design change (a.k.a. hypothesized design improvement) you want to test; version B. 
Selecting what to test can be difficult, but to help your selection you can consider:
- The baseline version of your wireframe you created yesterday;
- the 4 (+) design iterations you created yesterday;
- the iterations to the design you could consider based on step 2 above here (which you should implement in your wireframe then);
- the latest; and most complete, version of your design/wireframe, implement it and either A) compare it to and earlier version, or B) come up with another potential improvement, integrate it in your wireframe and use the new wireframe for the A/B test.

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
Want to collect data outside of the questionnaire setup or research something more specific? [Take a look at this page for more advance methods of feedback collection](https://support.proto.io/hc/en-us/articles/220363528-Getting-feedback)!
 </div>

**Note down which versions you choose, how you made your selection and why you ended up choosing to test these version. Then create and ordered list and use the steps you outlined in step 1 of the excercise to create planning for you A/B test! Make sure your entire test-plan can be conducted in 6 minutes: 4 minutes for the actual user test and 2 minutes for the questionnaire!** 

5. Once you've made your selection and composed your plan:
- Create a folder on Github and name it "AB_test".
- Add your A/B Test plan by creating a Word document named "A/B Test plan" in that folder and copy-pasting you A/B test plan as created in the previous step there.
- Create a folder named "A" and a folder named "B". 
- Prepare both your wireframes by exporting your project in proto.io as a zipped folder (if you haven't done so already at least) and adding the baseline version to folder A and the (hypothesized) improved version to folder B. 

**Check if everything works. If it does, you are almost set to start the A/B test. Only the measurement method remains, let's cover that now!**

### DataLab preparation 3. Measuring A and B using a survey:
Remember the questionnaire you were handed in block A to collect feedback on your dashboard for Data Science? We're now going to user that software again to create our own questionnaire. 
1. Let's start by getting familliar with Qualtrics by doing [the planning and designing a survey tutorial!](https://basecamp.qualtrics.com/) Or, if you think the software is intuitive, go ahead and just start building your questionnaire; it's really quite intuitve: the UX of Qualtrics is (mostly) well-designed!
2. Create a new project on [Qualtrics](https://buas.eu.qualtrics.com/Q/MyProjectsSection)
3. Go for the first option 'Survey' under the header 'From scratch'.
4. Click on 'Get started'
5. Name it: "AB-test_HCAI_Version_X" and create the survey (don't use any template, just a blank project).
6. Now, let's create 1 basic question which is going to be the same question asked to whoever tests version A or B. It has to be the same question, otherwise we can't really compare A and B:
- Create questionnaire matrix
- Select 'likert' scale and allow the user 7 options, ranging from 'Completely Agree' to 'Completely Disagree' with the 4 marking 'neutral'. 
- Add the first question: 'I understood what I could use the app for'
- Add the second question: 'I found the application intuitive to use'
- Add the third question: 'I enjoyed using the application'
- Add the fourth question: 'I thought the application was useful'
- If you want you can add more questions, but please don't add more then 8 as we only have 2 minute per questionnaire.
7. Test your questionnaire does it work? Does it also work in mobile mode? We're going to use our mobile phones to record the data.
8. Save your questionnaire, then create 2 exact copies of it and name them: "AB-test_HCAI_Version_A" and "AB-test_HCAI_Version_B" respectively.
9. Distribute the questionnaires, and generate links for each respective questionnaire which you save to your "A/B Test plan" file in an appropriate location. Then, generate QR codes for version A and B and paste those in the A/B Test plan so you test subject (strickly they're called participants) can easily scan the QR code and fill in the questionnaire after they're done with the test.

**Check if everything works. If it does, you are all set to start the A/B test! Good job!**


## Next up!
Coming Datalab we will actually run our A/B test and do a statistical analysis; a T-test. Then we're going to use the results to iterate on our design and finalize it!

# Resources
## Recommend Literature:
âˆ’	[Interaction Design: beyond human-computer interaction](https://login.proxy1.dom1.nhtv.nl/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=cat01829a&AN=buas.303541695&site=eds-live)   
âˆ’	[UX Fundamentals for Non-UX Professionals : User Experience Principles for Managers, Writers, Designers, and Developers](https://login.proxy1.dom1.nhtv.nl/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1892077&site=eds-live)

## Further Reading:
âˆ’	[Designing with Data](http://shop.oreilly.com/product/0636920026228.do)
âˆ’	[The Design of Everyday Things, Donald A. Norman](https://login.proxy1.dom1.nhtv.nl/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=cat01829a&AN=buas.393706974&site=eds-live)
