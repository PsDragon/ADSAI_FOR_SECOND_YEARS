{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Analysis\n",
    "We're going to conduct an Independent Samples T-test to analyse our A/B test. An Indepdent Samples T-test compares the differences between two means of two different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your Qualtrics results to a .csv file and save it to you github repository. Import your .csv file, inspect it, and clean it where neccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   patient  120 non-null    int64 \n",
      " 1   sex      120 non-null    object\n",
      " 2   agegrp   120 non-null    object\n",
      " 3   A        120 non-null    int64 \n",
      " 4   B        120 non-null    int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 4.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>sex</th>\n",
       "      <th>agegrp</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-45</td>\n",
       "      <td>143</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-45</td>\n",
       "      <td>163</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-45</td>\n",
       "      <td>153</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-45</td>\n",
       "      <td>153</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-45</td>\n",
       "      <td>146</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient   sex agegrp    A    B\n",
       "0        1  Male  30-45  143  153\n",
       "1        2  Male  30-45  163  170\n",
       "2        3  Male  30-45  153  168\n",
       "3        4  Male  30-45  153  142\n",
       "4        5  Male  30-45  146  141"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If on Github, load your data\n",
    "df_A = pd.read.csv(\"PATH\")\n",
    "df_B = pd.read.csv(\"PATH\")\n",
    "\n",
    "# EDA A\n",
    "df_A.info() # Is your data in the right format?\n",
    "df_A.head() # Quick EDA. No? Clean it, you only want the rows and collumns containing likert-score data.\n",
    "\n",
    "# EDA B\n",
    "df_A.info() # Is your data in the right format?\n",
    "df_A.head() # Quick EDA. No? Clean it, you only want the rows and collumns containing likert-score data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest we leave for tomorrow when we actually have our data. But if you are eager to play around a bit you can simply refresh the survey and fill in a couple of responses to create an A and a B version.\n",
    "\n",
    "Now, let's start analysing our gather data! This block we won't dive into inferential statistics since it can get complex quite fast; we'll do that in Year 2, block B. For now, you just need to know that we need to test whether the data is normally distributed and whether the variances of both samples are equal. Otherwise, our statistical tests would not be valid and we can therefore not say that the results we're seeing are due to chance. What we are essentially going to statistically ascertain is whether there is a statistically significant different in the mean of a given variable for version A or B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.9926842451095581, pvalue=0.7841846942901611) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=4.863729003161597, pvalue=0.028383666362347747) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Create an array for each question you are going to compare which saves the sampling difference.\n",
    "sampling_difference = df_A['SameQuestion_DifferentDF'].values - \\\n",
    "                      df_B['SameQuestion_DifferentDF'].values\n",
    "\n",
    "# Then run the shaprio-wilk statistical test to check whether the data is normally distributed (for small samples)\n",
    "normal = stats.shapiro(sampling_difference)\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A['A'],\n",
    "                           df_B['B'])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that dat is in the right format and we know the collumn names. Replace 'A' with the collumn name which holds your original baseline version; A. Replayce 'B' with the collumn name which holds the result of your improved version; B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22456/3618466230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run Independent Samples T-test when assumptions are not violated.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m results = stats.ttest_ind(df_A['A'],\n\u001b[0m\u001b[0;32m      3\u001b[0m                           df_B['B'])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Print the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_A' is not defined"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A['A'],\n",
    "                          df_B['B'])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 Ttest_indResult(statistic=3.0669836819036274, pvalue=0.002412277478078891) \n",
      " In truth, inferential statistics is more complicated then that but for our purposes we'll stick to this.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A['A'],\n",
    "                          df_B['B'],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that was our first t-test. Save the results to your learning log in the week 8 and interpret them there. Were they what you expected? What are you going to change to improve your design if neccesary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "119c259948d333b2ddf4ba2ffb3d68be5171f28660b26be40acacf7136fda808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
