---
sort: 11
---
# Microsoft Azure Machine Learning I (ğŸš§ Work in progress)

Welcome this module on Microsoft Azure Machine Learning! This module is designed to equip you with the skills needed to navigate and effectively utilize Azure ML, a cloud-based tool for training, deploying, and managing ML models. You'll dive into topics ranging from MLOps and Azure ML Workspace management to model training, tuning, and deployment. Plus, we'll tackle advanced topics to elevate your ML expertise. By the end of this module, you'll be able to leverage Azure ML to build and deploy your own ML models in the cloud.

After completing this module you will be able to:

- [ ] Evaluate and apply the levels of MLOps in Azure Machine Learning, using real-world examples to enhance your understanding and implementation of machine learning operations.

- [ ] Utilize an Azure ML Workspace, understanding its functions and benefits in the context of Azure Machine Learning.

- [ ] Execute effective model training and hyperparameter tuning in Azure Machine Learning, understanding the underlying methods and approaches involved.

- [ ] Develop and implement complex machine learning workflows in Azure, including deployment of models and the use of the Azure Machine Learning CLI v2.

## 1. Levels of MLOps and ML OPs Tools ğŸ› ï¸

To streamline and classify the MLOps process, Microsoft has introduced the MLOps Maturity Model. The model outlines five levels of maturity, each of which corresponds to a different stage of implementation and sophistication of MLOps practices within an organization. You have already seen the MLOps Maturity Model in the previous module, but now we take a closer look ğŸ” at the model and how it can be applied in Azure Machine Learning using different Azure tools and services ğŸ› ï¸.

Microsoft's MLOps approach is structured around a maturity model, which helps organizations understand their current maturity level and plan their roadmap for MLOps implementationğŸ—ºï¸. The model comprises five levels:

| Level | Description | 
|---|---|
| Level 0 - No MLOps | At this stage, processes are largely manual and driven by data scientists, with little to no automation in placeğŸš§. |
| Level 1 - DevOps, no MLOps | Similar to Level 0, but some basic integration tests are addedğŸš¦. |
| Level 2 - Automated Training | Training processes are tracked, and model artifacts are captured in a repeatable way, but release processes remain manual, and app integration still relies heavily on data scientistsğŸ“Š. |
| Level 3 - Automated Model Deployment | Automation extends to model deployment, with a CI/CD pipeline set up for automated releases, but human signoff is still requiredâœ…. |
| Level 4 - Full MLOps | At this level, retraining is automated based on metrics from the application, and A/B testing is added to the CI/CD pipeline. Human signoff may still be needed, but unit and integration tests are in place, and the process is largely automatedğŸš€. |


Moving from level 0 (No MLOps) to level 4 (Full MLOps), the model guides the transition from untracked, manual processes to fully automated, version-controlled workflows with sophisticated metric tracking and testing protocols. The goal is to ensure consistent quality control, streamline the machine learning lifecycle, and enable rapid and reliable deployment of ML models.

To learn more about the MLOps Maturity Model with Azure Machine Learning, please visit [this link](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mlops-maturity-model-with-azure-machine-learning/ba-p/3520625) ğŸ”—. 

Focus on the tools and processes that are used at each level. We will introduce the relevant tools and processes in more detail in the following sections.

<div style="text-align: center;">

<img src=./images/azuremlroadmap.png width="1000"/>

</div>

We will be using Azure Machine Learning to manage the lifecycle of machine learning models. This includes creating reproducible machine learning pipelines, registering, packaging, and deploying models, tracking lineage information, sending alerts and notifications for various events in the ML lifecycle, and monitoring ML applications for operational and ML-related issues. 


## 2. Azure Machine Learning [[docs](https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2)] 

Azure Machine Learning (Azure ML) ğŸ§  is a powerful platform designed to handle every stage of the machine learning lifecycle. It provides robust capabilities that streamline and automate the end-to-end process, from managing your data ğŸ“Š to deploying your models ğŸš€. Each component within Azure ML plays a crucial role in the overall workflow and contributes to a more efficient and productive machine learning process.

<div style="text-align: center;">

<img src=https://learn.microsoft.com/en-us/training/modules/intro-to-azure-ml/media/3-people.gif width="600"/>

</div>

The journey starts with __Data__ ğŸ—‚ï¸. As the lifeblood of any machine learning project, Azure ML provides tools to manage and version your datasets. This not only keeps your data organized ğŸ“š but also ensures robust data governance ğŸ›¡ï¸.

Next, we move on to __Jobs__ ğŸ‘·. These are essentially the tasks or operations that perform the model training. Think of them as the workhorses that carry out the heavy lifting in your machine learning projects.

Then, we have __Components__ ğŸ§©. These are the modular building blocks of your machine learning pipelines in Azure. Each step of the process, whether it's data preprocessing, feature extraction, or model training, can be a component. This modular approach enhances flexibility and makes the workflow more manageable.

Up next are __Pipelines__ ğŸ­. Just as in a manufacturing assembly line, pipelines in Azure ML help orchestrate your machine learning tasks. They allow you to automate workflows, from data preprocessing to model deployment, thereby reducing manual intervention and increasing efficiency.

Dealing with software dependencies can often be a hassle, but not with Azure ML's __Environments__ ğŸŒ³. They help track and manage software dependencies, ensuring reproducibility and consistency across different stages of the machine learning lifecycle.

Now, let's talk about __Models__ ğŸ›ï¸. Azure ML offers a robust Model Registry for storing, versioning, and managing all your trained machine learning models. Each model is tracked and versioned, making it easier to manage your models' lifecycle.

Finally, we reach __Endpoints__ ğŸ¯. Once your models are ready to make predictions, they're deployed to endpoints. Models are packaged into Docker images before deployment, and they can be deployed as endpoints in the cloud or locallyğŸŒ. Deployment configurations include providing the models, an entry script, a Machine Learning environment, and other necessary assets. Azure ML supports both real-time scoring with online endpoints and large scale scoring with batch endpoints.

It's important to note that Azure Machine Learning provides the capability to track the end-to-end audit trail of all your machine learning assets using metadata. This can be particularly useful for understanding how models arrive at results for specific inputs, meeting regulatory compliance, and maintaining healthy deploymentsğŸ”’. 

Understanding these components and how they interact is key ğŸ”‘ to leveraging the full potential of Azure ML. The platform's comprehensive capabilities provide you with the tools needed to develop high-quality machine learning solutions, all while maintaining efficiency and productivity ğŸ’ª.

Please complete the following course from Microsoft to learn more about Azure Machine Learning:

- [Intro to Azure ML](https://learn.microsoft.com/en-us/training/modules/intro-to-azure-ml/) ğŸ“š

This course will give you a high-level overview of Azure Machine Learning and its capabilities. It will also introduce you to the various components of Azure ML and how they work together to streamline the machine learning lifecycle. :100:

### 2.1 Azure ML Workspace

All the assets mentioned above are stored in an Azure ML Workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. It provides data scientists, ML engineers, and IT professionals with a single view of all the experiments and models in a workspace. The workspace also provides a centralized place to manage the compute targets you use for training and inference.

Complete the following course from Microsoft to learn more about Azure ML Workspace:
- [Explore the Azure ML Workspace](https://learn.microsoft.com/en-us/training/paths/explore-azure-machine-learning-workspace/) ğŸ“š

This course will give you a high-level overview of the Azure Machine Learning workspace and how to interact with it. You will also be introduced to the Azure ML SDK, which is a Python package that allows you to interact with Azure ML programmatically. In both parts of this course, you will be first be run through the concepts and then be given a hands-on lab to practice what you've learned. Later in the module we will work through an end-to-end example of using Azure ML to train and deploy a machine learning model with the MNIST project that you have been working on. :1234: 

<div style="text-align: center;">
<img src=images/azuremlworkspace.png width="25%"/><br>
Assets available in an Azure ML Workspace
</div>

### 2.2 Data

The first step in any machine learning project is to gather and prepare the data. Azure ML provides tools to manage and version your datasets. This not only keeps your data organized ğŸ“š but also ensures robust data governance ğŸ›¡ï¸. The two key concepts to understand are __Datastores__ and __Data Assets__.

| Concept | Description | Usage |
|---|---|---|
| Datastores | Datastores are references to a location where your data resides ğŸŒ. They act as a map ğŸ—ºï¸ pointing to your data treasure chest ğŸ“¦. They can point to various Azure services like Blob storage, Data Lake Storage, SQL Database, File Share, etc. They maintain the connection string and other authentication information for you, providing secure and private data access. When you create an Azure workspace, a default datastore (an Azure Blob storage account) is automatically created, but you can also create and use other datastores. | Datastores are used when you need to access raw data from various sources, like Azure Blob storage or Azure Data Lake. You'd typically use them early in a project when you're collecting and preparing your data. |
| Data Assets (Datasets) | Datasets are versioned references to specific subsets of data in your Datastore. Each time a Dataset is modified, a new version is created. This versioning allows you to experiment with different versions of your data ğŸ”„, and ensures reproducibility of your experiments. You can think of Datasets as the actual treasure in the chest ğŸº. | Datasets are used when you have data that is ready for use in experiments, and you want to be able to reuse this data across different experiments and pipelines. Datasets are particularly useful in later stages of a project when you are iterating on different models and need consistency and reproducibility in your data. |
| Data Assets (Data References) | Data References are pointers to the data in your Datastore. They act like bookmarks ğŸ”– in a book, allowing you quick access to a specific location in your Datastore. | Data References are used when you want to work with data directly in its original location (like during data exploration or initial analysis). They are particularly useful when you're working with large amounts of data, or when you want to work with data in its raw form without creating a Dataset. |

Remember, Datastores and Data Assets are complementary. Datastores provide a secure and scalable way to access your data, while Data Assets provide an organized and versioned way to manage and reuse subsets of that data. Using both effectively can help ensure that your machine learning projects are both efficient and reproducible.

#### MNIST Example - Look at how you did it with te MNIST example

Let's take an example of using the popular MNIST dataset in Azure ML.

First, we would establish a Datastore that points to the location of the MNIST data in Azure Blob Storage. This is our map ğŸ—ºï¸ to the treasure chest ğŸ“¦.

```python
from azureml.core import Datastore

blob_datastore = Datastore.register_azure_blob_container(
    workspace=ws, 
    datastore_name='my_blob_store', 
    container_name='mnist-container',
    account_name='my-storage-account',
    account_key='my-storage-account-key'
)
```

Next, we create a Dataset, which is a versioned reference to a specific subset of the data in the Datastore. This is our actual treasure ğŸº.

```python
from azureml.core import Dataset

blob_ds = Dataset.Tabular.from_delimited_files(path=(blob_datastore, 'mnist/train.csv'))
blob_ds = blob_ds.register(workspace=ws, 
                           name='mnist_train',
                           description='MNIST training data',
                           create_new_version=True)
```

We now have a Datastore pointing to our raw data, and a Dataset representing a versioned and reusable subset of that data for use in our machine learning experiments!

Please complete the following course from Microsoft and refer to the linked documentation page to learn more about Azure ML Datastores and Data Assets:

- [Working with Data in Azure ML](https://learn.microsoft.com/en-us/training/paths/work-data-azure-machine-learning/):books:
- [Azure Machine Learning Documentation - Tracking Dataset](https://docs.microsoft.com/azure/machine-learning/service/how-to-version-track-datasets).

Remember, understanding the difference and interplay between Datastores and Data Assets is key to efficiently managing data in Azure ML. 

Stay curious and keep exploring! ğŸš€

### 2.3 Compute

Explain what a compute target is and how to list and select compute targets.

Make the lesson on creating compute optional.

https://learn.microsoft.com/en-us/training/paths/work-compute-azure-machine-learning/

Use a compute cluster
There are three main scenarios in which you'll want to use a compute cluster:

Running a pipeline job you built in the Designer.
Running an Automated Machine Learning job.
Running a script as a job.
In each of these scenarios, a compute cluster is ideal as a compute cluster will automatically scale up when a job is submitted, and automatically shut down when a job is completed.

A compute cluster will also allow you to train multiple models in parallel, which is a common practice when using Automated Machine Learning.

You can run a Designer pipeline job and an Automated Machine Learning job through the Azure Machine Learning studio. When you submit the job through the studio, you can set the compute target to the compute cluster you created.

When you prefer a code-first approach, you can set the compute target to your compute cluster by using the Python SDK.

For example, when you run a script as a command job, you can set the compute target to your compute cluster with the following code:

```python
from azure.ai.ml import command

# configure job
job = command(
    code="./src",
    command="python diabetes-training.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="cpu-cluster",
    display_name="train-with-cluster",
    experiment_name="diabetes-training"
    )

# submit job
returned_job = ml_client.create_or_update(job)
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

After submitting a job that uses a compute cluster, the compute cluster will scale out to one or more nodes. Resizing will take a few minutes, and your job will start running once the necessary nodes are provisioned. When a job's status is preparing, the compute cluster is being prepared. When the status is running, the compute cluster is ready, and the job is running.


### Environments

https://learn.microsoft.com/en-us/training/modules/work-environments-azure-machine-learning/





