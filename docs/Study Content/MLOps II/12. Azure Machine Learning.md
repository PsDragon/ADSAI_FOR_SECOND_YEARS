---
sort: 11
---
# Microsoft Azure Machine Learning I 

Welcome this module on Microsoft Azure Machine Learning! This module is designed to equip you with the skills needed to navigate and effectively utilize Azure ML, a cloud-based tool for training, deploying, and managing ML models. You'll dive into topics ranging from MLOps and Azure ML Workspace management to model training, tuning, and deployment. Plus, we'll tackle advanced topics to elevate your ML expertise. By the end of this module, you'll be able to leverage Azure ML to build and deploy your own ML models in the cloud.

After completing this module you will be able to:

- [ ] Evaluate and apply the levels of MLOps in Azure Machine Learning, using real-world examples to enhance your understanding and implementation of machine learning operations.

- [ ] Utilize an Azure ML Workspace, understanding its functions and benefits in the context of Azure Machine Learning.

- [ ] Execute effective model training and hyperparameter tuning in Azure Machine Learning, understanding the underlying methods and approaches involved.

- [ ] Develop and implement complex machine learning workflows in Azure, including deployment of models and the use of the Azure Machine Learning CLI v2.

## 1. Levels of MLOps and ML OPs Tools ğŸ› ï¸

To streamline and classify the MLOps process, Microsoft has introduced the MLOps Maturity Model. The model outlines five levels of maturity, each of which corresponds to a different stage of implementation and sophistication of MLOps practices within an organization. You have already seen the MLOps Maturity Model in the previous module, but now we take a closer look ğŸ” at the model and how it can be applied in Azure Machine Learning using different Azure tools and services ğŸ› ï¸.

Microsoft's MLOps approach is structured around a maturity model, which helps organizations understand their current maturity level and plan their roadmap for MLOps implementationğŸ—ºï¸. The model comprises five levels:

| Level | Description | 
|---|---|
| Level 0 - No MLOps | At this stage, processes are largely manual and driven by data scientists, with little to no automation in placeğŸš§. |
| Level 1 - DevOps, no MLOps | Similar to Level 0, but some basic integration tests are addedğŸš¦. |
| Level 2 - Automated Training | Training processes are tracked, and model artifacts are captured in a repeatable way, but release processes remain manual, and app integration still relies heavily on data scientistsğŸ“Š. |
| Level 3 - Automated Model Deployment | Automation extends to model deployment, with a CI/CD pipeline set up for automated releases, but human signoff is still requiredâœ…. |
| Level 4 - Full MLOps | At this level, retraining is automated based on metrics from the application, and A/B testing is added to the CI/CD pipeline. Human signoff may still be needed, but unit and integration tests are in place, and the process is largely automatedğŸš€. |


Moving from level 0 (No MLOps) to level 4 (Full MLOps), the model guides the transition from untracked, manual processes to fully automated, version-controlled workflows with sophisticated metric tracking and testing protocols. The goal is to ensure consistent quality control, streamline the machine learning lifecycle, and enable rapid and reliable deployment of ML models.

To learn more about the MLOps Maturity Model with Azure Machine Learning, please visit [this link](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mlops-maturity-model-with-azure-machine-learning/ba-p/3520625) ğŸ”—. 

Focus on the tools and processes that are used at each level. We will introduce the relevant tools and processes in more detail in the following sections.

<div style="text-align: center;">

<img src=./images/azuremlroadmap.png width="1000"/>

</div>

We will be using Azure Machine Learning to manage the lifecycle of machine learning models. This includes creating reproducible machine learning pipelines, registering, packaging, and deploying models, tracking lineage information, sending alerts and notifications for various events in the ML lifecycle, and monitoring ML applications for operational and ML-related issues. 


## 2. Azure Machine Learning [[docs](https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2)] 

Azure Machine Learning (Azure ML) ğŸ§  is a powerful platform designed to handle every stage of the machine learning lifecycle. It provides robust capabilities that streamline and automate the end-to-end process, from managing your data ğŸ“Š to deploying your models ğŸš€. Each component within Azure ML plays a crucial role in the overall workflow and contributes to a more efficient and productive machine learning process.

<div style="text-align: center;">

<img src=https://learn.microsoft.com/en-us/training/modules/intro-to-azure-ml/media/3-people.gif width="600"/>

</div>

The journey starts with __Data__ ğŸ—‚ï¸. As the lifeblood of any machine learning project, Azure ML provides tools to manage and version your datasets. This not only keeps your data organized ğŸ“š but also ensures robust data governance ğŸ›¡ï¸.

Next, we move on to __Jobs__ ğŸ‘·. These are essentially the tasks or operations that perform the model training. Think of them as the workhorses that carry out the heavy lifting in your machine learning projects.

Then, we have __Components__ ğŸ§©. These are the modular building blocks of your machine learning pipelines in Azure. Each step of the process, whether it's data preprocessing, feature extraction, or model training, can be a component. This modular approach enhances flexibility and makes the workflow more manageable.

Up next are __Pipelines__ ğŸ­. Just as in a manufacturing assembly line, pipelines in Azure ML help orchestrate your machine learning tasks. They allow you to automate workflows, from data preprocessing to model deployment, thereby reducing manual intervention and increasing efficiency.

Dealing with software dependencies can often be a hassle, but not with Azure ML's __Environments__ ğŸŒ³. They help track and manage software dependencies, ensuring reproducibility and consistency across different stages of the machine learning lifecycle.

Now, let's talk about __Models__ ğŸ›ï¸. Azure ML offers a robust Model Registry for storing, versioning, and managing all your trained machine learning models. Each model is tracked and versioned, making it easier to manage your models' lifecycle.

Finally, we reach __Endpoints__ ğŸ¯. Once your models are ready to make predictions, they're deployed to endpoints. Models are packaged into Docker images before deployment, and they can be deployed as endpoints in the cloud or locallyğŸŒ. Deployment configurations include providing the models, an entry script, a Machine Learning environment, and other necessary assets. Azure ML supports both real-time scoring with online endpoints and large scale scoring with batch endpoints.

It's important to note that Azure Machine Learning provides the capability to track the end-to-end audit trail of all your machine learning assets using metadata. This can be particularly useful for understanding how models arrive at results for specific inputs, meeting regulatory compliance, and maintaining healthy deploymentsğŸ”’. 

Understanding these components and how they interact is key ğŸ”‘ to leveraging the full potential of Azure ML. The platform's comprehensive capabilities provide you with the tools needed to develop high-quality machine learning solutions, all while maintaining efficiency and productivity ğŸ’ª.

Please complete the following course from Microsoft to learn more about Azure Machine Learning:

- [Intro to Azure ML](https://learn.microsoft.com/en-us/training/modules/intro-to-azure-ml/) ğŸ“š

This course will give you a high-level overview of Azure Machine Learning and its capabilities. It will also introduce you to the various components of Azure ML and how they work together to streamline the machine learning lifecycle. :100:

### 2.1 Azure ML Workspace

All the assets mentioned above are stored in an Azure ML Workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. It provides data scientists, ML engineers, and IT professionals with a single view of all the experiments and models in a workspace. The workspace also provides a centralized place to manage the compute targets you use for training and inference.

Complete the following course from Microsoft to learn more about Azure ML Workspace:
- [Explore the Azure ML Workspace](https://learn.microsoft.com/en-us/training/paths/explore-azure-machine-learning-workspace/) ğŸ“š

This course will give you a high-level overview of the Azure Machine Learning workspace and how to interact with it. You will also be introduced to the Azure ML SDK, which is a Python package that allows you to interact with Azure ML programmatically. In both parts of this course, you will be first be run through the concepts and then be given a hands-on lab to practice what you've learned. Later in the module we will work through an end-to-end example of using Azure ML to train and deploy a machine learning model with the MNIST project that you have been working on. :1234: 

<div style="text-align: center;">
<img src=./images/azuremlworkspace.png width="250"/><br>
Assets available in an Azure ML Workspace
</div>

### 2.2 Data 

The first step in any machine learning project is to gather and prepare the data. Azure ML provides tools to manage and version your datasets. This not only keeps your data organized ğŸ“š but also ensures robust data governance ğŸ›¡ï¸. The two key concepts to understand are __Datastores__ and __Data Assets__.

| Concept | Description | Usage |
|---|---|---|
| Datastores | Datastores are references to a location where your data resides ğŸŒ. They act as a map ğŸ—ºï¸ pointing to your data treasure chest ğŸ“¦. They can point to various Azure services like Blob storage, Data Lake Storage, SQL Database, File Share, etc. They maintain the connection string and other authentication information for you, providing secure and private data access. When you create an Azure workspace, a default datastore (an Azure Blob storage account) is automatically created, but you can also create and use other datastores. | Datastores are used when you need to access raw data from various sources, like Azure Blob storage or Azure Data Lake. You'd typically use them early in a project when you're collecting and preparing your data. |
| Data Assets (Datasets) | Datasets are versioned references to specific subsets of data in your Datastore. Each time a Dataset is modified, a new version is created. This versioning allows you to experiment with different versions of your data ğŸ”„, and ensures reproducibility of your experiments. You can think of Datasets as the actual treasure in the chest ğŸº. | Datasets are used when you have data that is ready for use in experiments, and you want to be able to reuse this data across different experiments and pipelines. Datasets are particularly useful in later stages of a project when you are iterating on different models and need consistency and reproducibility in your data. |
| Data Assets (Data References) | Data References are pointers to the data in your Datastore. They act like bookmarks ğŸ”– in a book, allowing you quick access to a specific location in your Datastore. | Data References are used when you want to work with data directly in its original location (like during data exploration or initial analysis). They are particularly useful when you're working with large amounts of data, or when you want to work with data in its raw form without creating a Dataset. |

Remember, Datastores and Data Assets are complementary. Datastores provide a secure and scalable way to access your data, while Data Assets provide an organized and versioned way to manage and reuse subsets of that data. Using both effectively can help ensure that your machine learning projects are both efficient and reproducible.

Please complete the following course from Microsoft and refer to the linked documentation page to learn more about Azure ML Datastores and Data Assets:

- [Working with Data in Azure ML](https://learn.microsoft.com/en-us/training/paths/work-data-azure-machine-learning/):books:
- [Data Concepts in Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-data?view=azureml-api-2)
- [Azure Machine Learning Documentation - Tracking Dataset](https://docs.microsoft.com/azure/machine-learning/service/how-to-version-track-datasets).

#### Example: Creating a Datastore and Dataset

Let's take an example of using the popular MNIST dataset in Azure ML. ğŸ”¢

We will follow the following steps:

1. Download the MNIST dataset
2. Create a Datastore pointing to azure blob storage
3. Upload the MNIST dataset to the Datastore
4. Create a Train, Validate, and Test Data Assets pointing to the MNIST dataset in the Datastore

The example code is provided in the [azure_ml_data_example.ipynb](./Examples/azure_ml_data_example.ipynb) notebook.

#### Data Quick Reference :memo:

Here are some snippets of Python code you can use to accomplish each of the tasks you listed, assuming you're using the Azure Machine Learning SDK for Python:

0. **Define your workspace**
   ```python
   from azureml.core import Workspace
   from azureml.core.authentication import InteractiveLoginAuthentication

   auth = InteractiveLoginAuthentication()
   workspace = Workspace(subscription_id=subscription_id,
                      resource_group=resource_group,
                      workspace_name=workspace_name,
                      auth=auth,
                      )
   ```

1. **List all Datastores in a Workspace**
   ```python
   from azureml.core import Workspace
   
   # list all datastores registered in the current workspace
   datastores = ws.datastores
   for name, datastore in datastores.items():
       print(name, datastore.datastore_type)
   ```

2. **Create a Datastore object**
   ```python
   from azureml.core import Datastore
   
   # define a datastore to access blob storage
   datastore = Datastore(workspace, name='datastore_name')
   ```

3. **Upload data to a Datastore**
   ```python
   # upload data to the datastore
   datastore.upload(src_dir='local_directory',
                         target_path='target_path_on_datastore',
                         overwrite=True,
                         show_progress=True)
   ```

4. **Sample data from a Datastore**
   ```python
   from azureml.core import Dataset

   # Create a FileDataset from a path to a directory.
   # The directory contains a folder per class, each of which contains image files.
   data = Dataset.File.from_files(path=(datastore, 'target_path_on_datastore'))
   
   # take a random sample from the dataset
   sample = data.take_sample(0.001)

   # select the first 30 samples from that random sample
   sub_sample = sample.take(30).download()
   ```

    Let's break down the code:

    1. `data = Dataset.File.from_files(path=(datastore, 'target_path_on_datastore'))` ğŸ‘©â€ğŸ’»: This line of code creates a `FileDataset` object named `data` from files located at the 'target_path_on_datastore' in the specified `datastore`. `FileDataset` is a class in the Azure ML SDK which represents file-based datasets in the Azure ML workspace. Here, the contents of `data` would be references to the individual files (such as image files) in the specified path within the datastore.

    2. `sample = data.take_sample(0.001)` ğŸ§ª: This line takes a random sample from the `data` FileDataset. The `take_sample()` function uses a probability to determine the number of files to sample. In this case, 0.001 means it randomly selects about 0.1% of the files from the `data` object. The `sample` variable holds these selected file references.

    3. `sub_sample = sample.take(30).download()` ğŸ“¦: Here, the `take()` function selects the first 30 file references from the `sample` object. Then, `download()` downloads these files to the local environment where this code is running. The `sub_sample` variable contains the local paths to these downloaded files. 

    So in a nutshell, this code snippet is sampling a small subset of files from a larger dataset stored in an Azure datastore, and downloading them to your local environment for further inspection or processing. ğŸ”„ğŸ’¾

5. **Create Datasets from a Datastore**
   ```python
   train_set, val_set = data.random_split(0.8, seed=123)
   test_set = Dataset.File.from_files(path=(datastore, 'path_to_test_data'))
   ```
6. **Register Datasets**
   ```python
   #register the datasets
   train_reg = train_set.register(workspace=workspace, name='digits_train', description='training data')
   val_reg = val_set.register(workspace=workspace, name='digits_val', description='validation data')
   test_reg = test_set.register(workspace=workspace, name='digits_test', description='test data')
   ```
7. **List all Datasets in a Workspace**
   ```python
   from azureml.core import Dataset
   
   # list all datasets registered in the current workspace
   datasets = ws.datasets
   for name, dataset in datasets.items():
       print(name)
   ```

The above methods are to be used as examples. For actual usage, please replace placeholders (like `'local_directory'`, `'target_path_on_datastore'`, `'my_account'`, etc.) with actual values applicable to your scenario. Also, ensure that the workspace configuration is correctly set up.


### 2.3 Compute Targets

Compute targets in Azure Machine Learning are the resources or environments where you run your training scripts or host your service deployments. They can be local or cloud-based and can include your local machine, Docker containers, virtual machines, and more. One of the key benefits of using Azure ML is the ability to dynamically create and manage compute targets that scale with your needs.

Azure Machine Learning supports several types of compute targets:

- Local compute: Your local machine. ğŸ’»
- Azure Machine Learning compute instances: Development workstations that data scientists can use to work with data and models. ğŸ’¡
- Azure Machine Learning compute clusters: Multi-node clusters for scalable training of machine learning models. These clusters can automatically scale up and down according to your configurations and needs, saving costs and optimizing resources. ğŸ“ˆ
- Azure Kubernetes Service (AKS): Azure's managed Kubernetes service, ideal for deploying and managing containerized applications more easily. It provides scalable, secure, and robust orchestration services. ğŸ³
- Azure Machine Learning inference clusters: Deployment targets for real-time inference. âš¡
- Attached compute: Other Azure resources like Azure Databricks, Azure Data Lake Analytics, or virtual machines. â˜

To manage your compute targets, you can list all the available compute targets in your workspace and then select the one that suits your needs. Here is an example of how you can list your compute targets using the Azure ML Python SDK:

```python
from azureml.core import Workspace

ws = Workspace.from_config()

# List all compute targets in the workspace
for compute_name in ws.compute_targets:
    compute = ws.compute_targets[compute_name]
    print(compute.name, ":", compute.type)
```

Once you you have this list you can simply select a compute target and specify it in your training script or deployment configuration. We will cover this in more detail in the next section.

Complete the following course from Microsoft to learn more about Azure ML Compute:

- [Working with Compute in Azure ML](https://learn.microsoft.com/en-us/training/paths/work-compute-azure-machine-learning/) :books:

ğŸ’¡ Note: The above course will also show you how to create compute targets in your workspace. This is useful information but for your project the compute targets will already exist in your team workspace so you will only need to list and choose compute targets. ğŸ’¡

### 2.4 Environments 

Azure Machine Learning environments are an encapsulation of the training script, runtime, and associated packages needed for model training and scoring. Each environment consists of a Docker container and a Python environment. They play a vital role in ensuring the reproducibility and consistency of machine learning workflows across different stages and on various compute targets.

With Azure ML environments, you can:

- Specify Python packages and versions required for your training and deployment scripts.
- Manage environments by creating new ones, getting existing ones, and listing all environments in your workspace.
- Version environments to keep track of the package changes and to ensure reproducibility of your experiments.
- Reuse your environments across different experiments and deployments.

Complete the following course from Microsoft and refer to the linked documentation page to learn more about Azure ML Environments:

- [Working with Environments in Azure ML](https://learn.microsoft.com/en-us/training/modules/work-environments-azure-machine-learning/) :books:
- [Azure Machine Learning Documentation - Environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments).


#### Environments Quick Reference :memo:

1. **Create a Custom Environment with Custom Base Docker Image**

    Here we are defining a custom Docker environment for our machine learning experiment.

    ```python
    from azureml.core import Environment

    myenv = Environment(name="myenv")
    myenv.docker.enabled = True
    myenv.docker.base_image = None
    myenv.docker.base_dockerfile = """
    FROM ubuntu:18.04
    RUN apt-get update && \
        apt-get install -y python3-pip && \
        pip3 install azureml-defaults
    """
    ```
    In the script, an environment named `myenv` is created. Docker usage is enabled, and a Docker image is configured to be built based on a custom Dockerfile. The Dockerfile instructs Azure to create a container with Ubuntu 18.04 and install Python3 and some AzureML defaults. ğŸ‹ğŸ’¼

2. **Add Python Packages to the Environment**

    You can specify additional Python packages with their versions to be installed in the environment.

    ```python
    from azureml.core.conda_dependencies import CondaDependencies

    cd = CondaDependencies.create(pip_packages=['numpy==1.17.0', 'pandas==0.25.1'])
    myenv.python.conda_dependencies = cd
    ```
    The script creates a `CondaDependencies` object and adds numpy and pandas to the Python packages to be installed. The environment `myenv` is then updated with these dependencies. ğŸğŸ“¦

3. **Register the Environment**

    Environments can be registered to your workspace, allowing you to reuse them across different runs and experiments.

    ```python
    myenv.register(workspace=ws)
    ```
    This line of code registers the environment `myenv` to the workspace `ws`. Now, `myenv` can be retrieved in any experiment running in the workspace `ws`. ğŸ”„âœ…

4. **Retrieve an Environment**

    Registered environments can be retrieved for use in your training scripts or deployments.

    ```python
    from azureml.core import Environment

    retrieved_env = Environment.get(workspace=ws, name="myenv")
    ```
    This script retrieves the environment named `myenv` from the workspace `ws`. `retrieved_env` is now ready to be used in an experiment or deployment. ğŸ“¥ğŸ’¡

5. **Alternate Method Using a YAML File and Specifying a Base Image from Microsoft**

    ```python	
    from azure.ai.ml.entities import Environment

    custom_env_name = "env_name"
    version = "1.0.1"

    # Create a custom environment.
    env = Environment(
        name=custom_env_name,
        description="Environment description",
        tags={"pytorch": "2.0"}, #for example, these just give you a way to tag your environments with metadata
        conda_file="path/to/conda_dependencies.yaml",
        image= "mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest",
        version=version,
    )

    # Register the environment to the workspace.
    env.register(workspace=ws)

    print(
        f"Environment with name {env.name} is registered to workspace, the environment version is {env.version}"
    )
    ```
    This script creates a custom environment and registers it to the workspace. The environment is created using a YAML file that specifies the Python packages to be installed. The base image is specified as a Microsoft image. ğŸ‹ğŸ“¦

    You can create a YAML file to specify the Python packages to be installed in the environment using your `pyproject.toml` file as follows:

    - Install [poetry2conda](https://github.com/dojeda/poetry2conda)

    ```bash
    poetry add poetry2conda
    ```

    - Add the following to the pyproject.toml file

    ```toml
    [tool.poetry2conda]
    name = "mark-processor"
    ```

    - Create the conda environment

    ```bash
    poetry run poetry2conda pyproject.toml environment.yaml
    ```

    - Edit the environment.yaml file

    You need to edit the file to specify the python version and the channels.

    ```yaml
    name: mark-processor
    channels:
    - conda-forge
    - defaults  
    dependencies:
    - python==3.9.1
    - pip
    - pip:
        - matplotlib>=3.7.1,<4.0.0
        - numpy==1.21.1
        - torch>=2.0.0,<3.0.0
        - pre-commit>=3.2.0,<4.0.0
        - torchvision>=0.15.1,<0.16.0
        - seaborn>=0.12.2,<0.13.0
        - scikit-learn>=1.2.2,<2.0.0
        - tqdm>=4.65.0,<5.0.0
        - azure-core>=1.26.3,<2.0.0
        - azure-identity>=1.12.0,<2.0.0
        - azure-ai-ml>=1.4.0,<2.0.0
        - azureml>=0.2.7,<0.3.0
        - azureml-core>=1.49.0,<2.0.0
        - azureml-dataset-runtime  

    ```
6. List available environments

    ```python
    from azureml.core import Environment

    env_names = Environment.list(workspace=ws)
    for env_name in env_names:
        print('Name:',env_name)
    ```

These are some of the key tasks you can perform with environments in Azure ML, helping you manage dependencies efficiently and ensure the consistency of your machine learning workflows across different compute targets. Stay curious and keep exploring! ğŸš€

In this script, we create a new environment and set the `base_dockerfile` property to define the Dockerfile that will be used to create the Docker image for our environment. This way, we can use a custom Docker container. 

Remember, using Azure ML environments is key to managing software dependencies efficiently, ensuring the consistency of your machine learning workflows, and providing a high level of flexibility in your workflows. ğŸš€

:bulb: We will go into a lot more detail on containers and docker in the next section. ğŸ‹

## Additional Resources :books:

- [Azure Machine Learning Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2&viewFallbackFrom=azure-ml-py) 

- [Azure Machine Learning Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/concept-v2?view=azureml-api-2)
