---
sort: 14
---

# Azure Machine Learning II - Model Training, Components, and Pipelines

In this module, we will delve deeper into Azure Machine Learning and explore key concepts related to model training, components, and pipelines. We have previously covered essential topics such as data management, compute environments, and environments setup, which have provided us with a strong foundation. Now, let's expand our knowledge further with the following topics:

## 1. Model Training and Hyperparameter Tuning

### 1.1 Why Model Training in Azure (or any public cloud) is Essential for Data Scientists/ML Engineers

Model training is a fundamental step in the machine learning workflow where we teach our algorithms to make predictions or perform tasks based on available data. Azure Machine Learning provides a powerful platform for model training, offering numerous benefits for data scientists and ML engineers:

1. ‚ú® **Scalability**: Azure Machine Learning enables us to scale our training process seamlessly by leveraging the cloud infrastructure. We can train models on large datasets or use distributed computing to parallelize the training process, significantly reducing the time required to train complex models.

2. ‚ôªÔ∏è **Repeatability**: With Azure Machine Learning, we can create reproducible experiments. This means that we can precisely recreate the training environment, including the code, dependencies, and data, ensuring consistent and reliable results. Reproducibility is crucial for collaboration, debugging, and achieving consistent model performance.

3. ‚öôÔ∏è **Automation**: Azure Machine Learning facilitates the automation of the model training process. We can set up pipelines to automate the execution of training jobs, eliminating manual intervention and enabling us to focus on higher-level tasks. Automation saves time, reduces errors, and streamlines the end-to-end machine learning workflow.

Now, let's dive into the specific topics covered in this module.

### 1.2 Running a Script as a Command Job

A common challenge when developing machine learning models is to prepare them for production scenarios. While notebooks are excellent for experimentation and development, scripts are better suited for production workloads. You will have already gone through the task of making your code production ready. In Azure Machine Learning, you can run a script as a command job using the Python Software Development Kit (SDK) v2, this allows you to run your code as a command job in a compute environment in Azure Machine Learning.

When you submit a command job, you can configure various parameters, such as the input data and the compute environment. Azure Machine Learning assists in tracking your work when working with command jobs, making it easier to compare different workloads and assess their performance.

To gain hands-on experience with running scripts as command jobs, we will explore the official Microsoft Learn tutorial: 

- [Train models using scripts in Azure Machine Learning](https://learn.microsoft.com/en-us/training/paths/train-models-scripts-azure-machine-learning/). üìö

This tutorial will guide you through the process of running a training script as a command job using the Azure Machine Learning SDK v2 for Python.

### 1.3 Azure ML Experiments

In Azure Machine Learning, an experiment is a logical container for a series of runs. Each run is a specific execution of a script or pipeline. Azure Machine Learning helps us track and manage our experiments effectively, enabling us to compare the performance of different models and choose the best model for our specific problem.

To gain hands-on experience with experiments, we will explore the official Microsoft Learn tutorial:

- [Azure ML Experiments](https://learn.microsoft.com/en-us/training/modules/intro-to-azure-machine-learning-service/) üìö

### 1.3 Training Multiple Models with Different Hyperparameters

In real-world scenarios, training a single model with default hyperparameters may not yield optimal performance. Hyperparameter tuning involves searching for the best combination of hyperparameters to enhance model performance. Azure Machine Learning simplifies this process by allowing us to train multiple models with different hyperparameters efficiently.

One approach to achieve this is by running multiple job commands in a loop, each with a different set of hyperparameters. Azure Machine Learning helps us manage and track these training jobs effectively, enabling us to compare the performance of different models and choose the best hyperparameter configuration for our specific problem.

Throughout this course, we will explore practical examples and hands-on exercises to train multiple models with different hyperparameters, analyze their results, and select the most promising model configuration. üî¨üí°

üìù **1a** Try this out with your project training script

üìù **1b** Inspect the jobs in the Azure Machine Learning Studio this is where all the experiments or training runs as well any inference jobs will be stored.

We can also **sweep jobs** to automate the hyperparameter tuning process.

üìù **1c** Investigate sweep jobs and try this out with your project training script. You may need to make some adjustments to your training script to enable this.

## 2. The ML Client

The `MLClient` class is part of the Azure ML SDK and serves as your primary interface to the Azure Machine Learning service. It provides a simplified, streamlined way of accessing and managing resources in your Azure Machine Learning workspace. It's basically your one-stop-shop for all things related to Azure Machine Learning! üõçÔ∏è

Here is an example of how to create an `MLClient` object in Python:

```python
from azure.identity import InteractiveBrowserCredential
from azure.ml import MLClient

# Create a browser credential
credential = InteractiveBrowserCredential()

# subscription_id, resource_group, and workspace_name are strings
subscription_id = "your-subscription-id"
resource_group = "your-resource-group-name"
workspace_name = "your-workspace-name"

# Create an MLClient using the credential and workspace details
ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group,
    workspace_name=workspace_name,
)
```

You can also create an `MLClient` object using a service principal instead of a browser credential. For more information, see [this page](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#use-a-service-principal). üìñ

In the above code:

- `InteractiveBrowserCredential` will open a new browser window prompting the user to log in when it needs to authenticate. This makes it a good choice for local development.

- `MLClient` is initialized with the user's Azure subscription id, resource group name, and the name of the Azure ML workspace. The `MLClient` object `ml_client` then serves as the main entry point for interacting with your Azure ML resources.

Remember to replace "your-subscription-id", "your-resource-group-name", and "your-workspace-name" with your actual Azure details. The `ml_client` object can now be used to interact with Azure ML services in your Python code. üöÄ

## 3. Pipelines üîÅ

Before we dive into the details of pipelines, let's first understand the concept of a pipeline. A pipeline is a sequence of steps that are executed in a specific order to complete a task. Pipelines are commonly used in software development to automate the process of building, testing, and deploying applications. In Azure Machine Learning, pipelines provide a powerful platform for building and managing end-to-end machine learning workflows.

üîß Key Features of Pipelines in Azure Machine Learning:

1. **Orchestration and Automation**: Azure Machine Learning pipelines enable you to orchestrate and automate the execution of complex machine learning workflows. You can define the sequence of steps, dependencies between components, and the flow of data, allowing for efficient and reproducible execution.

2. **Reusability and Modularity**: Pipelines promote reusability and modularity by using components. Components are self-contained pieces of code that perform specific tasks. They can be easily created, shared, and reused in different pipelines, reducing duplication of effort and enhancing collaboration.

3. **Flexibility and Scalability**: Azure Machine Learning pipelines offer flexibility and scalability. You can incorporate different types of components, such as data preprocessing, model training, evaluation, and deployment, allowing you to tailor the pipeline to your specific machine learning needs. Additionally, you can leverage the scalability of the cloud by running pipelines on large datasets or using distributed computing.

4. **Monitoring and Debugging**: Pipelines in Azure Machine Learning provide built-in monitoring and logging capabilities. You can track the execution of each step, monitor resource utilization, and log metrics and outputs for each component. This helps in troubleshooting and debugging pipeline issues, ensuring the reliability and performance of your machine learning workflows.

5. **Versioning and Reproducibility**: Azure Machine Learning pipelines support versioning and reproducibility. You can track and manage different versions of pipelines, components, and data, making it easier to reproduce and compare results. This is crucial for collaboration, auditability, and achieving consistent model performance.

üîç By leveraging these key features, you can achieve efficient, scalable, and reproducible machine learning workflows in Azure Machine Learning.

Now, let's take a look at the building blocks of pipelines in Azure Machine Learning and how to put them together. üß± In azure machine learning, the steps in a pipeline are called components. A component is a self-contained piece of code that performs a specific task, like data preprocessing, model training or evaluation. Components can be reused in different pipelines, making it easier to build and maintain pipelines. We will create a component for each step in our pipeline, and then combine them to create a pipeline.

Absolutely, a component in Azure Machine Learning (using the Azure Machine Learning SDK v2) is a reusable, modular piece of code that performs a specific task in your machine learning workflow. It could be anything from data preprocessing to model training, evaluation, or deployment.

The structure of a component is defined by the following parameters:

1. **Name**: A unique name for the component. 

2. **Display Name**: The name that will be displayed in the Azure ML Studio.

3. **Description**: A brief description of what the component does.

4. **Inputs**: These are the inputs that the component requires. They are defined using the `Input` class and could be of various types, including `uri_folder`, `uri_file`, `string`, etc.

5. **Outputs**: These are the outputs generated by the component. They are defined using the `Output` class and can also be of various types.

6. **Code**: The location of the Python script that contains the code to be executed by the component.

7. **Command**: The command to be executed by the component. It often includes placeholders for the input and output parameters, which are filled in when the component is executed.

8. **Environment**: The Azure ML Environment where the component will be executed. This defines the runtime context of the component and includes aspects like the Python version, packages, and environment variables.

Here's a simplified syntax for defining a component:

```python
from azure.ai.ml import command
from azure.ai.ml import Input, Output

component = command(
    name="component_name",
    display_name="Component Display Name",
    description="Description of the component",
    inputs={
        "input1": Input(type="input_type", description="Input description"),
        # More inputs...
    },
    outputs={
        "output1": Output(type="output_type", mode="output_mode"),
        # More outputs...
    },
    code="path_to_python_script",
    command="command_to_execute_python_script",
    environment=env,
)
```
The `Input` and `Output` classes are used to define the inputs and outputs respectively. For each input/output, you specify a name, a type, and optionally a description or mode. 

The `command` parameter allows you to specify how the Python script should be executed, including how to use the inputs and outputs. It often involves referencing the inputs and outputs with placeholders.

The `environment` is an Azure ML Environment object that you've previously created or retrieved. It defines the runtime context in which the component code will be executed.

The `command` function also allows you to specify inputs as arguments. These arguments are used to pass data between different steps in a pipeline.

When you define a component, you can use placeholders for input and output arguments in your command. The placeholders are referenced as `{inputs.input_name}` and `{outputs.output_name}` in the command string. Azure ML will replace these placeholders with actual values when the component is run.

Here's an example where the command includes two input arguments (`data_asset_uri` and `data_source`) and one output argument (`data`):

```python
from azure.ai.ml import command
from azure.ai.ml import Input, Output

component = command(
    name="component_name",
    display_name="Component Display Name",
    description="Description of the component",
    inputs={
        "data_asset_uri": Input(type="uri_folder", description="Data asset URI"),
        "data_source": Input(type="string", description="Data source"),
    },
    outputs={
        "data": Output(type="uri_folder", mode="rw_mount"),
    },
    code="path_to_python_script",
    command="python script.py --data_asset_uri {inputs.data_asset_uri} --data_source {inputs.data_source} --output_dir {outputs.data}",
    environment=env,
)
```

In this command, the Python script `script.py` is executed with three command-line arguments. The arguments `--data_asset_uri` and `--data_source` are inputs, while `--output_dir` is an output. The placeholders `{inputs.data_asset_uri}`, `{inputs.data_source}`, and `{outputs.data}` will be replaced with actual values at runtime.

Using inputs and outputs as command-line arguments provides flexibility, as it allows you to easily pass data between different components in a pipeline.

After creating the components, you can use these components to build a pipeline. A pipeline is essentially a sequence of steps where each step is a component performing a specific task.

In Azure Machine Learning, a pipeline is defined as a Python function that is decorated with the `@dsl.pipeline` decorator from the Azure ML SDK v2. Inside this function, you define the sequence of components that make up your pipeline.

Here's an example of how to create a pipeline using multiple components:

```python
from azure.ai.ml import dsl

@dsl.pipeline(
    name="mnist_training_pipeline",
    description="A pipeline to train an MNIST model",
    compute=' compute_target_name',
)
def training_pipeline(data_asset_uri: str, data_source: str) -> None:
    # Step 1: Load data
    load_data_step = load_data_component(data_asset_uri=data_asset_uri)

    # Step 2: Preprocess data
    preprocess_data_step = preprocess_data_component(data=load_data_step.outputs.data)

    # Step 3: Train model
    training_step = train_model_component(data=preprocess_data_step.outputs.data, data_source=data_source)

    # Step 4: Evaluate model
    evaluate_step = evaluate_model_component(model=training_step.outputs.model)

```
In this code:

- We define a pipeline function `training_pipeline()` with the `@dsl.pipeline` decorator.
- This function has two input parameters: `data_asset_uri` and `data_source`.
- Inside the function, we create pipeline steps by calling the components that we've previously defined. Each step is a call to a component function, with the required inputs. These inputs can include outputs from previous steps, allowing data to flow through the pipeline. The output of a component is accessed using `component_step.outputs.output_name`.
- The components are called in the order they are executed in the pipeline, effectively defining the sequence of operations in the pipeline.

Once the pipeline is defined, you can create a pipeline run to execute it:

```python
# Create an instance of the pipeline
pipeline_instance = training_pipeline(data_asset_uri=digits_ds, data_source="local")

# Submit the pipeline run
pipeline_run = ml_client.jobs.create_or_update(pipeline_instance)
```
Here, we create a pipeline instance by calling the pipeline function and providing the required inputs. We then submit this instance for execution using the `ml_client.jobs.create_or_update()` function.

This way, you can create a machine learning workflow that's modular, easy to understand, and easy to modify. Each step in your pipeline corresponds to a component, making it simple to reuse code and share it across different projects.


üåä You can dive deeper into pipelines and explore multiple methods of how to create, manage, and deploy them in Azure Machine Learning with the following Microsoft Learn tutorials:

- [Azure ML Pipelines for Automation](https://learn.microsoft.com/en-us/training/paths/use-azure-machine-learning-pipelines-for-automation/)

- [Tutorial: Pipelines and the Python SDK v2](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk?view=azureml-api-2)

- [Orchestrate Machine Learning with Pipelines](https://learn.microsoft.com/en-us/training/modules/create-pipelines-in-aml/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service)

- [Create Pipelines in AML](https://learn.microsoft.com/en-us/training/modules/create-pipelines-in-aml/)

Each of these resources provides a slightly different perspective on pipelines, so we recommend exploring all of them to get a comprehensive understanding of pipelines in Azure Machine Learning. üìö

## Worked Examples - MNIST üî¢

We are going to continue with the MNIST package example that we have been using throughout this block.

Let's got through some worked examples on how to run command jobs and create components and pipelines in Azure Machine Learning.

- [Run a command job](./Examples/azure_example_jobs.html) 
- [Components and Pipelines](./Examples/azure_example_pipelines.html) 

## Additional Resources :books:

- [Azure Machine Learning Courses and Tutorials](https://learn.microsoft.com/en-us/training/browse/?filter-products=machine&products=azure-machine-learning) - Microsoft Learn provides free, online training that will help you learn everything about Azure Machine Learning and apply that knowledge to your machine learning tasks. If there is something you want to learn about Azure Machine Learning, you will find it here.

- [Azure Machine Learning Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2&viewFallbackFrom=azure-ml-py) 

- [Azure Machine Learning Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/concept-v2?view=azureml-api-2)





