---
sort: 15
---

# Creating Reproducible ML Experiments with MLFlow and Azure ML üöÄ

In the dynamic field of data science, it is crucial to have an organized and structured way to manage your machine learning (ML) projects. That's where MLFlow comes in! MLFlow is an open-source platform designed to manage the entire machine learning lifecycle, including experimentation, reproducibility, and deployment. When combined with Azure Machine Learning (AML), a cloud-based service for building, training, and deploying machine learning models, you gain access to a powerful, scalable platform for your ML projects.

After completing this module, you will be able to:

- üéØ Understand the fundamental concepts of MLFlow and Azure Machine Learning.
- üéØ Use MLFlow to manage ML experiments, including tracking, packaging, and reproducing runs.
- üéØ Integrate MLFlow with Azure ML to leverage cloud-based resources for ML project lifecycle management.
- üéØ Design and implement reproducible ML experiments using MLFlow and Azure ML.

## Why use MLFlow with Azure ML? ü§î

MLFlow and Azure ML complement each other wonderfully. While MLFlow provides a robust framework for managing your ML lifecycle, Azure ML offers scalable cloud-based resources to support training, testing, and deployment of your models. This combination allows you to have a flexible and scalable ML environment, perfect for individual projects and collaborative efforts.

## MLFlow Concepts üìò

MLFlow comprises four main components:

- **MLFlow Tracking:** This is an API and UI for logging parameters, code versions, metrics, and output files when running your ML code and for later visualizing the results. You can use it in any environment (standalone scripts, notebooks, etc.) to log results to local files or a server.

- **MLFlow Projects:** This is a standard format for packaging reusable data science code. Each project is simply a directory with code or a git repository, and uses a descriptor file to specify its dependencies and how to run the code.

- **MLFlow Models:** This is a standard format for packaging machine learning models that allows you to easily share or deploy models across a variety of ML libraries, or even as a REST API in the cloud.

- **MLFlow Registry:** This is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model.

When integrated with Azure ML, you can harness the cloud's scalability and manage MLFlow's components more efficiently. Azure ML's MLOps (DevOps for machine learning) streamline the build, training, deployment, and management of machine learning models.

This video provides a great overview of MLFlow and using it with Azure ML: üìΩÔ∏è

<div style="text-align:center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/2DLG1yo8JxM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>

For more details on MLFlow  and how to use it with Azure ML, please visit the following links: üìö

- [MLFlow in AzureML](https://learn.microsoft.com/en-us/training/modules/train-models-training-mlflow-jobs/) - Microsoft Learn Course Module

- [Azure ML Python SDK v2 and MLFlow](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow-cli-runs?view=azureml-api-2&tabs=interactive%2Ccli) - Microsoft Azure Docs

- [MLFlow in Azure ML Some Example Notebooks](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)

- [MLFlow Quickstart](https://mlflow.org/docs/latest/quickstart.html) - MLFlow Quickstart Docs

- [Data Camp Tutorial](https://www.datacamp.com/tutorial/mlflow-streamline-machine-learning-workflow) - Data Camp Tutorial

## MLFlow Cheat Sheet üìÑ

- **Start an MLflow run:** This is how you start tracking an experiment.
    ```python
    import mlflow
    mlflow.start_run()
    ```
- **Log parameters, metrics, and a model:** During a run, you can log key-value pairs of parameters and metrics. You can also save a trained model:
    ```python
    mlflow.log_param("param1", 1)
    mlflow.log_metric("metric1", 1.1)
    ```
- **Log an artifact (output file):** You can save any file as an artifact. For example, you can log a plot as an image file:
    ```python
    import matplotlib.pyplot as plt
    fig = plt.figure()
    plt.plot([0, 1], [0, 1])
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title("title")
    mlflow.log_figure(fig, "fig1.png")
    ```
- **Use autolog:** With `autolog()`, MLflow automatically logs all relevent information from supported machine learning libraries. :
    ```python
    mlflow.autolog()
    ```
   This will automatically log metrics, parameters, and a model from the ML library you are using. It's a very powerful tool to reduce manual logging and make the code cleaner. Any details that aren't logged automatically can be logged manually.

   When you call `autolog()`, MLflow tracks the following for subsequent fitting function invocations:

    - Hyperparameters: Includes any arguments passed to the fit function.
    - Output: Log model-specific metrics and save the model in an MLflow Model format that can be used for later inference.
    - Code version: Tracks the git commit hash (if applicable).
    - Environment: Captures information about the conda environment.
    - Extras: Logs additional metadata as tags.

    Here's an example of `autolog()` usage with scikit-learn:

    ```python
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import make_regression
    import mlflow.sklearn

    # Autolog
    mlflow.autolog()

    # Prepare training data
    X, y = make_regression()
    X_train, X_test, y_train, y_test = train_test_split(X, y)

    # Train the model
    rf = RandomForestRegressor()
    rf.fit(X_train, y_train)

    # The metrics, parameters, model artifact, and other details are automatically logged
    ```
   
- **End a run:** To stop tracking the experiment, simply end the run:
    ```python
    mlflow.end_run()
    ```
- **Using MLflow with Azure ML:** To utilize Azure ML's resources with MLflow, you'll first need to connect to your Azure ML workspace and then start an experiment:
    ```python
    from azureml.core import Workspace
    import mlflow

    workspace = Workspace.from_config()
    mlflow.set_tracking_uri(workspace.get_mlflow_tracking_uri())
    mlflow.start_run(experiment_id='my-experiment')
    ```

## Additional Resources üîñ

Explore these examples and code snippets to solidify your understanding and practice MLFlow and Azure ML:

- [Simple Keras Example](https://github.com/mlflow/mlflow/blob/master/examples/keras/train.py)
- [PyTorch Examples](https://github.com/mlflow/mlflow/tree/master/examples/pytorch) - These are quite detailed and a bit complex
- [More Tensorflow Examples](https://github.com/mlflow/mlflow/tree/master/examples/tensorflow) - These are quite detailed and a bit complex

## Additional Resources üîñ

Explore these examples and code snippets to solidify your understanding and practice MLFlow and Azure ML:

- [Simple Keras Example](https://github.com/mlflow/mlflow/blob/master/examples/keras/train.py)
- [PyTorch Examples](https://github.com/mlflow/mlflow/tree/master/examples/pytorch) - These are quite detailed and a bit complex
- [More Tensorflow Examples](https://github.com/mlflow/mlflow/tree/master/examples/tensorflow) - These are quite detailed and a bit complex


## Worked Example - MNIST with Keras

In this example, we'll train a simple Keras model on the MNIST dataset. We'll use `MLFlow` to track the model's performance and log the model itself. This example uses the same [code base](https://github.com/Deanis/Example-App.git) that we have been using in the previous lessons. The only difference is that we have added MLflow tracking code. First identify all the places where we have added MLflow tracking code. We will then go through each of these places and explain what they do.

```python

import os
import tensorflow as tf
import mlflow
from load_data import load_and_preprocess_data, load_and_preprocess_data_from_uri
from model import create_model
import argparse
import matplotlib.pyplot as plt

def train(use_uri,uri=None,model_path=None) -> None:
    """
    This function takes in three arguments. The use_uri flag indicates whether to use the built-in data 
    or to load it from a specified URI. The uri is a string that points to the data's location, and model_path 
    is a string specifying where to save the trained model.

    The function loads and preprocesses the MNIST data, creates and trains a model, evaluates its performance, 
    and saves the trained model.

    Args:
    use_uri : bool
        Flag that determines whether to use built-in data or load from a uri
    uri : str, optional
        Path to load the data from, by default None
    model_path : str, optional
        Path where to save the trained model, by default None
    """

    # Start MLflow tracking
    mlflow.start_run()
    mlflow.tensorflow.autolog()

    if not use_uri:
        # Load and preprocess data
        (train_images, train_labels), (test_images, test_labels) = load_and_preprocess_data()
    else:
        train_images, train_labels = load_and_preprocess_data_from_uri(uri)

    # Create the model
    model = create_model()

    # Train the model
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                  metrics=['accuracy'])
    history = model.fit(train_images, train_labels, epochs=1, validation_split=0.1)

    # Log the model summary to MLflow
    mlflow.log_text("model_summary.txt", str(model.summary()))

    # Log the model metrics to MLflow
    mlflow.log_metric("train_loss", history.history["loss"][-1])
    mlflow.log_metric("train_accuracy", history.history["accuracy"][-1])
    mlflow.log_metric("val_loss", history.history["val_loss"][-1])
    mlflow.log_metric("val_accuracy", history.history["val_accuracy"][-1])

    # plot the training and validation loss and accuracies for each epoch using matplotlib and log the image to MLflow
    fig = plt.figure()
    plt.plot(history.history["loss"], label="train_loss")
    plt.plot(history.history["val_loss"], label="val_loss")
    plt.plot(history.history["accuracy"], label="train_accuracy")
    plt.plot(history.history["val_accuracy"], label="val_accuracy")
    plt.title("Training Loss and Accuracy")
    plt.xlabel("Epoch #")
    plt.ylabel("Loss/Accuracy")
    plt.legend(loc="lower left")
    mlflow.log_figure(fig, "metrics.png")

    #check if the model directory exists
    if not os.path.exists(model_path):
        os.makedirs(model_path)

    # Save the trained model
    model.save(model_path)

    # Log the model to MLflow
    # mlflow.tensorflow.log_model(model, "model")

    # End the MLflow run
    mlflow.end_run()

def create_model():
    # Create the model architecture
    model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(28, 28)),
    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')])

    return model

if __name__ == '__main__':
    # define arguments using argparse
    parser = argparse.ArgumentParser(description='Train a model to recognize handwritten digits from the MNIST dataset.')
    parser.add_argument('--use-uri', action='store_true', help='Use the MNIST dataset from the Keras API')
    parser.add_argument('--data-path', type=str, default='data', help='Path to the MNIST dataset')
    parser.add_argument('--model-path', type=str, default='outputs/mnist_model', help='Path to the trained model')
    args = parser.parse_args()
    
    train(args.use_uri,args.data_path,args.model_path)
```
Now let's break it down piece by piece. üß± In this code, we're primarily using MLflow Tracking.

First we need to start MLflow tracking:

```python
# Start MLflow tracking
mlflow.start_run()
```
üö¶ This is like starting a stopwatch. This tells MLflow to start tracking this "run" of your machine learning code. Everything logged after this point is associated with this run.

```python
mlflow.tensorflow.autolog() # mlflow.pytorch.autolog() in the PyTorch version
```
üìù This tells MLflow to automatically log specific information. Depending on the library you use (in this case TensorFlow/PyTorch), this could include parameters, metrics, and a model. 

```python
# Log the model summary to MLflow
mlflow.log_text("model_summary.txt", str(model.summary()))
```
üóíÔ∏è This is logging a text file that contains your model's summary to MLflow. This is useful for keeping track of the model's structure.

```python
# Log the model metrics to MLflow
mlflow.log_metric("train_loss", history.history["loss"][-1])
mlflow.log_metric("train_accuracy", history.history["accuracy"][-1])
mlflow.log_metric("val_loss", history.history["val_loss"][-1])
mlflow.log_metric("val_accuracy", history.history["val_accuracy"][-1])
```
üìä This is recording your model's metrics. This includes things like accuracy and loss, for both training and validation. You can use this to track how well your model did.

```python
# plot the training and validation loss and accuracies for each epoch using matplotlib and log the image to MLflow
mlflow.log_figure(fig, "metrics.png")
```
üñºÔ∏è This saves an image of your model's metrics over time. This lets you visually see how your model's performance changed over each epoch.

```python
# Save the trained model
model.save(model_path)
```
üíæ This is saving your trained model to disk. This means you can use this model later without having to retrain it.

```python
# Log the model to MLflow
# mlflow.tensorflow.log_model(model, "model")
```
üöÄ This logs your trained model to MLflow. This is commented out in this code, but if it was uncommented it would allow you to keep track of the model you used in MLflow. You only need to use one method for saving the model.

```python
# End the MLflow run
mlflow.end_run()
```
‚èπÔ∏è This is like stopping the stopwatch. It tells MLflow that you're done with this run, and all logging after this will not be associated with the current run.

## Worked Example - MNIST with PyTorch

Now we will do the same thing, but with PyTorch. The code is very similar, but there are some minor differences related to `MLFlow` . See if you can spot them!

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from load_data import load_and_preprocess_data, load_and_preprocess_data_from_uri
import mlflow
import matplotlib.pyplot as plt
import argparse
import numpy as np

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32 * 13 * 13, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = x.view(-1, 32 * 13 * 13)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def evaluate(model, loader, criterion):
    model.eval()
    outputs = [model.validation_step(batch) for batch in loader]
    return model.validation_epoch_end(outputs)

def train(use_uri, uri=None, model_path=None):
    mlflow.start_run()
    mlflow.pytorch.autolog()

    if not use_uri:
        dataset = load_and_preprocess_data()
    else:
        dataset = load_and_preprocess_data_from_uri(uri)
        
    # split into training and validation sets
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = Net().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters())
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    for epoch in range(1): 
        model.train()
        train_losses = []
        train_accs = []
        for batch in trainloader:
            inputs, labels = batch[0].to(device), batch[1].to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # Calculate accuracy
            acc = accuracy(outputs, labels)

            train_losses.append(loss.item())
            train_accs.append(acc.item())

        # Compute validation loss and accuracy
        val_losses = []
        val_accs = []
        model.eval()
        with torch.no_grad():
            for batch in valloader:
                inputs, labels = batch[0].to(device), batch[1].to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                acc = accuracy(outputs, labels)

                val_losses.append(loss.item())
                val_accs.append(acc.item())

        history['train_loss'].append(np.mean(train_losses))
        history['train_acc'].append(np.mean(train_accs))
        history['val_loss'].append(np.mean(val_losses))
        history['val_acc'].append(np.mean(val_accs))

        # Log the metrics to MLflow
        mlflow.log_metric("train_loss", np.mean(train_losses))
        mlflow.log_metric("train_accuracy", np.mean(train_accs))
        mlflow.log_metric("val_loss", np.mean(val_losses))
        mlflow.log_metric("val_accuracy", np.mean(val_accs))

    # Plot the training and validation loss and accuracies for each epoch using matplotlib and log the image to MLflow
    fig = plt.figure()
    plt.plot(history['train_loss'], label='train_loss')
    plt.plot(history['val_loss'], label='val_loss')
    plt.plot(history['train_acc'], label='train_acc')
    plt.plot(history['val_acc'], label='val_acc')
    plt.title("Training Loss and Accuracy")
    plt.xlabel("Epoch #")
    plt.ylabel("Loss/Accuracy")
    plt.legend(loc="lower left")
    plt.savefig("metrics.png")
    mlflow.log_artifact("metrics.png")

    if not os.path.exists(model_path):
        os.makedirs(model_path)

    torch.save(model.state_dict(), os.path.join(model_path, 'model.pth'))

    mlflow.end_run()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train a model to recognize handwritten digits from the MNIST dataset.')
    parser.add_argument('--use-uri', action='store_true', help='Use the MNIST dataset from the Keras API')
    parser.add_argument('--data-path', type=str, default='data', help='Path to the MNIST dataset')
    parser.add_argument('--model-path', type=str, default='outputs/mnist_model', help='Path to the trained model')
    args = parser.parse_args()
    
    train(args.use_uri, args.data_path, args.model_path)
```