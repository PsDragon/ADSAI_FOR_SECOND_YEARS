---
sort: 18

---

# Azure Machine Learning III - Deploying Models üöÄ

Welcome to Module III of Azure Machine Learning! Here, we focus on deploying models, a critical step in the machine learning lifecycle. Once trained and registered, models need to be deployed to be used for predictions. This process is much like making a digital prediction service available to users. For instance, let's say we have trained a digit prediction model; deploying it will allow users to interact with it, provide their inputs (like an image of a handwritten digit), and receive a prediction (the digit the model thinks the image represents).

After completing this module, you will be able to:

- [ ] Understand the various methods for deploying ML models in Azure üîÑ
- [ ] Deploy a model using Managed ML Endpoints, Azure Container Instances, and Azure Container Apps üõ†Ô∏è
- [ ] Monitor deployed models using Azure Service Metrics and Azure Application Insights üìä

## Model Deployment

Azure offers several means to deploy your model, each suiting different needs and use cases. The following are some of the most common methods for deploying models in Azure:

### Managed ML Endpoint

Managed ML Endpoints are Azure's service specifically designed for machine learning model deployment. It manages the infrastructure, allowing you to focus on ensuring your digit prediction model is top-notch. The service automatically scales to accommodate the workload and integrates smoothly with the broader Azure ecosystem. Have a look at the following resources to learn more about Managed ML Endpoints:

- [Using Endpoints for Inference](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints?view=azureml-api-2) üìö
- [Azure ML Managed Online Endpoints](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints) üìö
- [Safe Rollout of Machine Learning Models - Blue Green Deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-safely-rollout-online-endpoints?view=azureml-api-2&tabs=python) üìö

Here's are some reference simple code snippets showcasing how you might deploy your model as a Managed ML Endpoint: üìÑ

1. Connect to your workspace - You know the drill by now don't forgot to connect to the MLClient and Workspace ü§ñ

2. Define an endpoint üîö

Endpoints are the URL interfaces where clients can access your deployed model. We start by defining the name of our endpoint. In the given code, we first set a static name and then dynamically generate one using the current timestamp to ensure uniqueness.

```python
# Define an endpoint name
endpoint_name = "my-endpoint"

# Example way to define a random name
import datetime

endpoint_name = "endpt-" + datetime.datetime.now().strftime("%m%d%H%M%f")

# create an online endpoint
endpoint = ManagedOnlineEndpoint(
    name = endpoint_name, 
    description="this is a sample endpoint"
    auth_mode="key"
)
```
3. The scoring script

This script processes input data, runs it through the model, and returns the model's predictions. A scoring script in machine learning model deployment is essentially a Python script that receives input data, processes this data, makes predictions using a trained model, and then returns these predictions. It's typically the primary interaction point between your deployed model and users or systems sending requests.

Here's are 2 simple examples of what a scoring script (often called `score.py`) might look like, one for Keras amd one for PyTorch:

Sure, below are examples for both Keras and PyTorch models.

- **Keras**

Let's assume that you have trained a Keras model and have registered it in Azure ML workspace. Here is how the scoring script (`score.py`) might look like:

```python
import json
import numpy as np
import tensorflow as tf
from azureml.core.model import Model

def init():
    global model
    model_path = Model.get_model_path('my-keras-model')
    model = tf.keras.models.load_model(model_path) #This command depends on the version of TF you are using

def run(raw_data):
    data = np.array(json.loads(raw_data)['data'])
    # Make prediction
    y_hat = model.predict(data)
    return json.dumps(y_hat.tolist())
```
In the `init()` function, `tf.keras.models.load_model` is used to load the Keras model from the model file.

In the `run()` function, the data is converted to a NumPy array before making predictions. Since the output of a Keras model is a NumPy array, we can directly convert it to a list before returning as JSON.

- **PyTorch**

If you have a PyTorch model registered in Azure ML workspace, here is an example scoring script:

```python
import json
import torch
import numpy as np
from azureml.core.model import Model

def init():
    global model
    model_path = Model.get_model_path('my-pytorch-model')
    model = torch.load(model_path)
    model.eval()

def run(raw_data):
    data = np.array(json.loads(raw_data)['data'])
    # Make sure the data type is correct
    data = torch.from_numpy(data).float()
    # Make prediction
    with torch.no_grad():
        y_hat = model(data)
    # The output for a PyTorch model is a tensor, so we need to convert it to a list before converting to JSON
    return json.dumps(y_hat.numpy().tolist())
```

In the `init()` function, `torch.load` is used to load the PyTorch model from the model file. The `model.eval()` method sets the model to evaluation mode. This is important when your model has layers like dropout or batch normalization, which behave differently during training and evaluation.

In the `run()` function, the data is converted to a PyTorch tensor before making predictions. We also use the `torch.no_grad()` context to prevent PyTorch from creating a computational graph during the prediction, which saves memory. Since the output of a PyTorch model is a tensor, we need to convert it to a list before returning as JSON.


In summary, the script is divided into two main parts: the `init()` and `run()` functions:

- The `init()` function is run once when the model is deployed. It's used to load the model into memory. In Azure, the `Model.get_model_path` method is used to get the path of the model file, and then the model is loaded using commands appropriate for the libraries you are using. The loaded model is saved as a global variable, which can then be accessed by the `run()` function.

- The `run()` function is executed every time a request is sent to your model. The `raw_data` parameter represents the input data. The input data needs to be preprocessed and formatted into the shape that the model expects. In this case, the input data is expected to be a JSON string with a key called 'data' which corresponds to a list of data points. This list of data points is converted to a numpy array, which is the format our model expects. After preprocessing, the model's `predict()` method is called to make predictions. The predictions are then converted back into a JSON string and returned.

Remember, this is just an example. Your scoring script would likely be more complex, especially the data preprocessing and postprocessing steps. These steps would need to be tailored to your specific model and the data it expects.

4. Define a deployment configuration

To see what models and environments are available in your workspace, you can use the following code snippet:

```python
from azureml.core.model import Model
from azureml.core.environment import Environment

# assuming ws is a workspace object
for model in Model.list(ws):
    print(model.name, 'version:', model.version)

for env in Environment.list(ws):
    print(env.name, env.version)
```

You will need to choose a model and environment to deploy. For instance, if you have a model named `my-digit-prediction-model` and an environment named `my-digit-prediction-env`, you can deploy them as follows:

```python
registered_model_name = "my-digit-prediction-model"
latest_model_version = 1
registered_environment_name = "my-digit-prediction-env"
latest_environment_version = 1

# picking the model to deploy. Here we use the latest version of our registered model
model = ml_client.models.get(name=registered_model_name, version=latest_model_version)
# picking the environment to deploy. Here we use the latest version of our registered environment
env = ml_client.environments.get(name=registered_environment_name, version=latest_environment_version)

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
```

In this step, we're choosing the model and environment to be deployed. The environment contains the runtime context where the model runs. This includes the Python interpreter, libraries, and system settings.

We're using the Model and Environment classes from Azure's ML SDK to list available models and environments. We then choose a model and environment to deploy.

In the given code, we're defining a ManagedOnlineDeployment object. This object holds all the settings required for deploying a model, like the model name, environment, the number of instances, and so on.

5. Deploy the endpoint

Finally, we're ready to deploy our endpoint. We use the begin_create_or_update method to do this. The method is asynchronous, which means it returns immediately after starting the deployment. The `result()` method is used to wait for the deployment to complete.

```python
# deploy the endpoint
blue_deployment = ml_client.begin_create_or_update(blue_deployment).result()
```

To confirm that the deployment was successful, you can check the status of the deployment:

```python
ml_client.online_endpoints.get(name=online_endpoint_name)
```


**NB** Resource Cleanup

It's a good practice to delete resources when they are no longer needed. This prevents unnecessary costs and keeps your workspace clean. In Azure, you can delete the endpoint using the `online_endpoints.begin_delete()` method.

```python   
# delete the endpoint
ml_client.online_endpoints.begin_delete(name=online_endpoint_name)
```

### Azure Container Instances

Azure Container Instances (ACI) offer another route for deploying models. ACI allows you to run containerized applications, including ML models, without heavy infrastructure management. This service is suitable for less complex applications that might not demand long-running uptime.

Below is an example of deploying a model with ACI:

```python
from azureml.core.webservice import AciWebservice, Webservice

#Make sure to connect to your workspace, you know the drill by now ü§ñ

aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)

service = Webservice.deploy_from_model(workspace=ws,
                                       name='my-aci-service',
                                       deployment_config=aci_config,
                                       models=['my_digit_prediction_model_path'],
                                       image_config='my_image_config')

service.wait_for_deployment(show_output=True)
```

#### Example Deploying an ML Model inside a Container with Azure Container Instances

### Azure Container Apps

For more complex, larger-scale applications, Azure Container Apps may be your go-to. This service is designed for deploying, running, and scaling containerized applications. 

To give an example, deploying a prediction service through Azure Container Apps would generally involve a Docker container and a YAML file for configuration. Please refer to the Azure Container Apps documentation for a step-by-step guide.

- [Azure Container Apps](https://docs.microsoft.com/en-us/azure/container-apps/overview) üìö

#### Example Deploying an ML Model inside a Container with Azure Container Apps

### Comparison

Here are some general pros and cons of using Azure ML Managed Online Endpoints, Azure Container Instances (ACI), and Azure Container Applications for machine learning applications:

Azure ML Managed Online Endpoints
- Pros:
  1. **Managed Service:** Azure takes care of the infrastructure, allowing you to focus more on model deployment and less on the management.
  2. **Auto-Scaling:** It scales automatically based on the workload, which is beneficial for models with variable usage.
  3. **Integration:** It integrates well with Azure's ecosystem like Azure Pipelines, Azure Monitor, etc. This can be advantageous for developing a unified ML pipeline.
- Cons:
  1. **Limited Customization:** There's limited room for customization beyond provided machine learning functionalities.
  2. **Cost:** Depending on usage, costs could potentially be higher compared to running your own managed services.

Azure Container Instances (ACI)
- Pros:
  1. **Simplicity:** ACI is a straightforward way to run a container without worrying about managing underlying infrastructure.
  2. **Flexibility:** It's more flexible than Azure ML Managed Endpoints and can run any containerized application, including a wide range of ML models.
- Cons:
  1. **Scalability:** ACI doesn't support auto-scaling like Azure ML Managed Endpoints do. You need to manage the scaling yourself.
  2. **Durability:** It's designed for short-lived, single-container applications, and may not be as robust for long-running applications.

Azure Container Applications
- Pros:
  1. **Multi-Container Applications:** If your machine learning application consists of multiple microservices (each potentially in its own container), this is a more suitable solution.
  2. **Scalability:** It's designed to scale out complex applications.
- Cons:
  1. **Complexity:** This service requires more management compared to ACI and Azure ML Managed Endpoints. Even though it's less than fully managing a Kubernetes cluster, it still demands more effort.
  2. **Overkill for Simple Applications:** If your application only consists of a single container, this service may provide more functionality than you need.

It's important to understand that each service has its own strengths and weaknesses. The best choice depends on the specific requirements of your machine learning application. Factors to consider include how complex your application is, how much traffic you expect, how much management you're willing to do, and what your budget is.

The table below summarizes the key differences between Azure ML Managed Online Endpoints, Azure Container Instances (ACI), and Azure Container Applications:

|    | Azure ML Managed Online Endpoints | Azure Container Instances (ACI) | Azure Container Applications |
|----|---------------------------------|---------------------------------|-----------------------------|
| Description | Managed and scalable machine learning model deployment service by Azure. | A service that allows you to run containers without managing any underlying infrastructure. | A service designed for deploying, running, and scaling containerized applications. |
| Primary Use Cases | Primarily used for deploying and inferencing machine learning models. | Primarily used for running short-lived and single-container applications, or for testing and development purposes. | Primarily used for running long-lived, multi-container applications in a production environment. |
| Scaling | Automatically scales out based on the workload. Supports both real-time and batch inferencing. | Manual scaling is needed. Each instance runs as a separate container. | Designed to scale out complex, multi-container applications. |
| Pricing Model | Pay-as-you-go based on the number of transactions and compute time. | Pay-as-you-go, based on the resources (CPU and memory) used. | Pricing depends on the resources (CPU and memory) used, with additional costs for premium features. |
| Infrastructure Management | Fully managed by Azure. | Minimal management needed. | Requires more management compared to ACI, but less than fully managing a Kubernetes cluster. |
| Integration with other Azure Services | Integrates with Azure Pipelines for CI/CD, Azure Monitor for monitoring, and Azure Active Directory for authentication. | Integrates with other Azure services, like Azure Logic Apps, Azure Functions, and more. | Integrates with Azure services, including Azure Logic Apps, Azure Functions, Azure Monitor, and more. |
| Customization | Limited customization beyond provided machine learning functionalities. | Limited customization. Primarily used for running containerized applications without additional features. | High customization. Supports deploying complex applications with multiple containers. |

Please note that this table provides a high-level comparison, and more detailed information can be found on the respective service documentation page.

- [Azure ML Managed Online Endpoints](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints) üìö
- [Azure Container Instances (ACI)](https://docs.microsoft.com/en-us/azure/container-instances/) üìö
- [Azure Container Applications](https://docs.microsoft.com/en-us/azure/container-apps/) üìö


## Azure Monitoring üëÄ (work in progress üöß)

Once your digit prediction model is up and running, predicting digits for users, you'll want to monitor its performance. This is where Azure's monitoring services come in.

### Azure Service Metrics

Azure Service Metrics offer an overview of your deployed model's performance, including metrics like the number of requests your model is handling, and the response time for these requests.

### Azure Application Insights

Azure Application Insights offers deeper insight into your deployed models, providing data on request rates, response times, failure rates, and more.

https://learn.microsoft.com/en-us/training/paths/deploy-consume-models-azure-machine-learning/

https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model?view=azureml-api-2

To understand the concept of model management and deployment in Azure Machine Learning, please visit [this link](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment?view=azureml-api-2).

Azure ACI tutorial:

Azure Web app tutorial:

<img src=https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/ai/_images/ml-ops-python.png#lightbox width="100%"/>


## Advanced

https://learn.microsoft.com/en-us/training/paths/build-first-machine-operations-workflow/

https://learn.microsoft.com/en-us/training/paths/develop-custom-object-detection-models-with-nvidia-and-azure-machine-learning/

https://learn.microsoft.com/en-us/training/paths/train-models-azure-machine-learning-cli-v2/



https://learn.microsoft.com/en-us/training/modules/intro-to-azure-machine-learning-service/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service

https://learn.microsoft.com/en-us/training/modules/train-local-model-with-azure-mls/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service

https://learn.microsoft.com/en-us/training/modules/work-with-data-in-aml/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service

https://learn.microsoft.com/en-us/training/modules/use-compute-contexts-in-aml/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service

https://learn.microsoft.com/en-us/training/modules/create-pipelines-in-aml/?ns-enrollment-type=learningpath&ns-enrollment-id=learn.data-ai.build-ai-solutions-with-azure-ml-service


Additional Resources:
https://drivendata.co/blog/missing-guide-to-azureml-part1-setting-up-your-azureml-workspace


https://learn.microsoft.com/en-us/training/modules/intro-to-azure-machine-learning-service/?WT.mc_id=azureml_tutorials


Full AML course: https://learn.microsoft.com/en-us/training/paths/build-ai-solutions-with-azure-ml-service/
