---
sort: 18

---

# Azure Machine Learning III - Deploying Models üöÄ

Welcome to Module III of Azure Machine Learning! Here, we focus on deploying models, a critical step in the machine learning lifecycle. Once trained and registered, models need to be deployed to be used for predictions. This process is much like making a digital prediction service available to users. For instance, let's say we have trained a digit prediction model; deploying it will allow users to interact with it, provide their inputs (like an image of a handwritten digit), and receive a prediction (the digit the model thinks the image represents).

After completing this module, you will be able to:

- [ ] Understand the various methods for deploying ML models in Azure üîÑ
- [ ] Deploy a model using Managed ML Endpoints, Azure Container Instances, and Azure Container Apps üõ†Ô∏è

**Index:**
- [Model Deployment](#model-deployment)
  - [Managed ML Endpoint](#managed-ml-endpoint)
  - [Azure Container Instances](#azure-container-instances)
  - [Azure Container Apps](#azure-container-apps)
  - [Comparing Deployment Methods](#comparison)
- [Additional Resources üìö](#additional-resources-üìö)

## Model Deployment

Azure offers several means to deploy your model, each suiting different needs and use cases. The following are some of the most common methods for deploying models in Azure:

### Managed ML Endpoint

Managed ML Endpoints are Azure's service specifically designed for machine learning model deployment. It manages the infrastructure, allowing you to focus on ensuring your digit prediction model is top-notch. The service automatically scales to accommodate the workload and integrates smoothly with the broader Azure ecosystem. Have a look at the following resources to learn more about Managed ML Endpoints:

- [Using Endpoints for Inference](https://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints?view=azureml-api-2) üìö
- [Azure ML Managed Online Endpoints](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints) üìö
- [Safe Rollout of Machine Learning Models - Blue Green Deployment](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-safely-rollout-online-endpoints?view=azureml-api-2&tabs=python) üìö

Here's are some reference simple code snippets showcasing how you might deploy your model as a Managed ML Endpoint: üìÑ

- **Connect to your workspace** - You know the drill by now don't forgot to connect to the MLClient and Workspace ü§ñ

- **Define an endpoint** üîö

Endpoints are the URL interfaces where clients can access your deployed model. We start by defining the name of our endpoint. In the given code, we first set a static name and then dynamically generate one using the current timestamp to ensure uniqueness.

```python
# Define an endpoint name
# Example way to define a random name
import datetime

endpoint_name = "endpt-" + datetime.datetime.now().strftime("%m%d%H%M%f")

# create an online endpoint
endpoint = ManagedOnlineEndpoint(
    name = endpoint_name, 
    description="this is a sample endpoint"
    auth_mode="key"
)
```
In Azure Machine Learning, there are two types of authentication modes that you can use for your endpoint:

1. **Key**: This is the default authentication mode. In this mode, a key is required when making API calls to your endpoint. This key is provided by Azure ML and you can obtain it from the Azure ML portal or using Azure ML SDK. It's useful when you want to control who can access your endpoint.

2. **Token**: This mode provides Azure Active Directory (Azure AD) token-based authentication. With this mode, a bearer access token is required for all HTTP requests to your endpoint. This token is obtained from Azure AD using OAuth 2.0, so you can use Azure AD to manage permissions and access to your endpoint. This option gives you more granular control over the security and access control of your endpoint.

To choose an authentication mode, you set the `auth_mode` parameter when creating your `ManagedOnlineEndpoint`, like so:

```python
endpoint = ManagedOnlineEndpoint(
    name = endpoint_name, 
    description="this is a sample endpoint",
    auth_mode="key" # or "token"
)
```

Just replace `"key"` with `"token"` if you want to use Azure AD token-based authentication. Remember, if you use token-based authentication, you will need to set up Azure AD and OAuth 2.0 to generate tokens! üîëüîí

- **The scoring script**

This script processes input data, runs it through the model, and returns the model's predictions. A scoring script in machine learning model deployment is essentially a Python script that receives input data, processes this data, makes predictions using a trained model, and then returns these predictions. It's typically the primary interaction point between your deployed model and users or systems sending requests.

Here's are 2 simple examples of what a scoring script (often called `score.py`) might look like, one for Keras amd one for PyTorch:

Sure, below are examples for both Keras and PyTorch models.

**Keras**

Let's assume that you have trained a Keras model and have registered it in Azure ML workspace. Here is how the scoring script (`score.py`) might look like:

```python
import json
import numpy as np
import tensorflow as tf
from azure.ai.ml.entities import Model

def init():
    global model
    base_path = os.getenv('AZUREML_MODEL_DIR')
    print(f"base_path: {base_path}")
    model_path = os.path.join(base_path, 'INPUT_model')
    print(f"model_path: {model_path}")
    model = tf.keras.models.load_model(model_path)#This command depends on the version of TF you are using

def run(raw_data):
    # Convert raw dat JSON to numpy array (this will depend on your data type)
    data = np.array(json.loads(raw_data)['data'])
    # Process the data so that it matches the training input
    .
    .
    .
    # Make prediction
    y_hat = model.predict(data)
    return json.dumps(y_hat.tolist())
```

‚ö†Ô∏è This code is only an example. You will need to modify it to suit your specific use case. For instance, you will need to modify the `init()` function to load your model from the model file. You will also need to modify the `run()` function to process your data and make predictions. ‚ö†Ô∏è

In the `init()` function, `tf.keras.models.load_model` is used to load the Keras model from the model file.

In the `run()` function, the data is converted to a NumPy array before making predictions. Since the output of a Keras model is a NumPy array, we can directly convert it to a list before returning as JSON.

- **PyTorch**

If you have a PyTorch model registered in Azure ML workspace, here is an example scoring script:

```python
import json
import torch
import numpy as np
from azure.ai.ml.entities import Model

def init():
    global model
    base_path = os.getenv('AZUREML_MODEL_DIR')
    print(f"base_path: {base_path}")
    model_path = os.path.join(base_path, 'INPUT_model')
    print(f"model_path: {model_path}")
    model = torch.load(model_path)
    model.eval()

def run(raw_data):
    data = np.array(json.loads(raw_data)['data'])
    # Make sure the data type is correct
    data = torch.from_numpy(data).float()
    # Process the data so that it matches the training input
    .
    .
    .
    # Make prediction
    with torch.no_grad():
        y_hat = model(data)
    # The output for a PyTorch model is a tensor, so we need to convert it to a list before converting to JSON
    return json.dumps(y_hat.numpy().tolist())
```
‚ö†Ô∏è This code is only an example. You will need to modify it to suit your specific use case. For instance, you will need to modify the `init()` function to load your model from the model file. You will also need to modify the `run()` function to process your data and make predictions. ‚ö†Ô∏è

In the `init()` function, `torch.load` is used to load the PyTorch model from the model file. The `model.eval()` method sets the model to evaluation mode. This is important when your model has layers like dropout or batch normalization, which behave differently during training and evaluation.

In the `run()` function, the data is converted to a PyTorch tensor before making predictions. We also use the `torch.no_grad()` context to prevent PyTorch from creating a computational graph during the prediction, which saves memory. Since the output of a PyTorch model is a tensor, we need to convert it to a list before returning as JSON.


In summary, the script is divided into two main parts: the `init()` and `run()` functions:

- The `init()` function is run once when the model is deployed. It's used to load the model into memory. In Azure, the `Model.get_model_path` method is used to get the path of the model file, and then the model is loaded using commands appropriate for the libraries you are using. The loaded model is saved as a global variable, which can then be accessed by the `run()` function.

- The `run()` function is executed every time a request is sent to your model. The `raw_data` parameter represents the input data. The input data needs to be preprocessed and formatted into the shape that the model expects. In this case, the input data is expected to be a JSON string with a key called 'data' which corresponds to a list of data points. This list of data points is converted to a numpy array, which is the format our model expects. After preprocessing, the model's `predict()` method is called to make predictions. The predictions are then converted back into a JSON string and returned.

Remember, this is just an example. Your scoring script would likely be more complex, especially the data preprocessing and postprocessing steps. These steps would need to be tailored to your specific model and the data it expects.

- **Define a deployment configuration**

To see what models and environments are available in your workspace, you can use the following code snippet:

```python
# List Models
for model in ml_client.models.list():
    print(model.name, 'version:', model.version)

# List Environments
for env in ml_client.environments.list():
    print(env.name, 'version:', env.version)
```

You will need to choose a model and environment to deploy. For instance, if you have a model named `my-digit-prediction-model` and an environment named `my-digit-prediction-env`, you can deploy them as follows:

```python
registered_model_name = "my-digit-prediction-model"
latest_model_version = 1
registered_environment_name = "my-digit-prediction-env"
latest_environment_version = 1

# picking the model to deploy. Here we use the latest version of our registered model
model = ml_client.models.get(name=registered_model_name, version=latest_model_version)
# picking the environment to deploy. Here we use the latest version of our registered environment
env = ml_client.environments.get(name=registered_environment_name, version=latest_environment_version)

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
```

In this step, we're choosing the model and environment to be deployed. The environment contains the runtime context where the model runs. This includes the Python interpreter, libraries, and system settings.

We're using the Model and Environment classes from Azure's ML SDK to list available models and environments. We then choose a model and environment to deploy.

In the given code, we're defining a ManagedOnlineDeployment object. This object holds all the settings required for deploying a model, like the model name, environment, the number of instances, and so on.

5. Deploy the endpoint

Finally, we're ready to deploy our endpoint. We use the begin_create_or_update method to do this. The method is asynchronous, which means it returns immediately after starting the deployment. The `result()` method is used to wait for the deployment to complete.

```python
# deploy the endpoint
blue_deployment = ml_client.begin_create_or_update(blue_deployment).result()
```

To confirm that the deployment was successful, you can check the status of the deployment:

```python
ml_client.online_endpoints.get(name=endpoint_name)
```

6. Adjust traffic

Once the deployment is complete, you can adjust the traffic being sent to the deployment, by default it is set to 0%. You can do this using the `update()` method of the `OnlineEndpoint` class. The `update()` method takes a list of `Traffic()` objects as an argument. Each `Traffic()` object contains the name of the endpoint and the percentage of traffic that should be routed to it.

```python
endpoint = ml_client.online_endpoints.get(name=endpoint_name)
# Update the traffic distribution
endpoint.traffic["blue"] = 100
```

If you want to split the traffic between multiple endpoints, you can do so by adding more `Traffic()` objects to the list. For instance, if you want to split the traffic equally between two endpoints, you can do so as follows:

```python
endpoint = ml_client.online_endpoints.get(name=endpoint_name)
# Update the traffic distribution
endpoint.traffic["blue"] = 50
endpoint.traffic["green"] = 50
```

This can also be done from the Azure portal. Simply navigate to the Endpoints tab and select the endpoint. You can then adjust the traffic using the slider.


**NB** Resource Cleanup

It's a good practice to delete resources when they are no longer needed. This prevents unnecessary costs and keeps your workspace clean. In Azure, you can delete the endpoint using the `online_endpoints.begin_delete()` method.

```python   
# delete the endpoint
ml_client.online_endpoints.begin_delete(name=endpoint_name)
```
You can also do this from the Azure portal. Simply navigate to the Endpoints tab and delete the endpoint.

#### Worked Example - Creating and Deploying a Model Endpoint with Azure ML üí°

In this example, we'll create and deploy a model endpoint using Azure ML. We'll use the models that we trained and registered in the previous example on pipelines, and we will deploy it as a managed endpoints.

We will need to create two new python scripts in order to do this:

- `score.py` - This script will be used to make predictions using the model. It will be run every time a request is sent to the endpoint. üßæ
- `create_endpoint.py` - This script will be used to create and deploy the endpoint. üëâüîö

‚úÖ**Click [here](./Examples/azure_managed_endpoint_example.html) for the full example.** ‚úÖ

### Azure Container Instances

Azure Container Instances (ACI) offer another route for deploying models. ACI allows you to run containerized applications, including ML models, without heavy infrastructure management. This service is suitable for less complex applications that might not demand long-running uptime.

All you need to do is create a container image with your model and an API, and then deploy it to ACI. You can use any containerization tool you like, such as Docker or Azure Container Registry.

The final line of your Dockerfile should be the command that starts your API. For example, if you're using FastAPI, the command would look like this:

```dockerfile
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "80"]
```

It is a good idea to first test your container locally before deploying it to ACI. Once you're ready to deploy, you can use the Azure ML SDK to deploy your container to ACI.

Below is an example of deploying a model with ACI:

```python
from azure.identity import ClientSecretCredential
from azure.mgmt.containerinstance import ContainerInstanceManagementClient
from azure.mgmt.containerinstance.models import (ContainerGroup, Container, ResourceRequests, ResourceRequirements, 
                                                 ContainerGroupNetworkProtocol, OperatingSystemTypes,
                                                 IpAddress, Port)

# Replace with your own values
SUBSCRIPTION_ID = '<your-subscription-id>'
RESOURCE_GROUP = '<your-resource-group>'
CONTAINER_NAME = '<container-name>'
IMAGE = 'deanis/example-app:latest'  # Docker Hub image
CPU_CORE_COUNT = 1.0
MEMORY_GB = 1.5
TENANT_ID = '<your-tenant-id>'
CLIENT_ID = '<your-client-id>'
CLIENT_SECRET = '<your-client-secret>'

# Get credentials
credentials = ClientSecretCredential(
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET,
    tenant_id=TENANT_ID
)

# Create a Container Instance Management client
container_client = ContainerInstanceManagementClient(credentials, SUBSCRIPTION_ID)

# Define the container
container_resource_requests = ResourceRequests(memory_in_gb=MEMORY_GB, cpu=CPU_CORE_COUNT)
container_resource_requirements = ResourceRequirements(requests=container_resource_requests)
container = Container(name=CONTAINER_NAME, image=IMAGE, resources=container_resource_requirements, ports=[Port(port=80)])

# Define the group of containers
container_group = ContainerGroup(location='westeurope', containers=[container], os_type=OperatingSystemTypes.linux, ip_address=IpAddress(ports=[Port(protocol=ContainerGroupNetworkProtocol.tcp, port=80)], type='Public'))

# Create the container group
container_client.container_groups.begin_create_or_update(RESOURCE_GROUP, CONTAINER_NAME, container_group)
```

You will need to install the Azure Container Instances Management SDK for Python. You can do this with the following command:

```bash
pip install azure-mgmt-containerinstance
```

A big drawback of ACI is that it does not support https, without some additional configuration. This means that your API will be exposed over http, which is not always secure and may not be accessible from certain networks. If you need to use https, you can use Azure Application Gateway to add https support to your ACI.

#### Example Deploying an ML Model inside a Container with Azure Container Instances üì¶

In this example, we'll deploy a model inside a container using Azure Container Instances. We'll use the same model that we trained and registered in the previous examples as well as the API that we created in the previous example on FastAPI.

We will follow these steps:

- Create a Dockerfile - to package our model and API into a container. üì¶
- Build the container image 
- Run the container image - to test our container locally. üèóÔ∏è
- Deploy the container image to ACI - to deploy our container to Azure. üöÄ
- Test the deployed container - to make sure it's working. üß™

‚úÖ**Click [here](./Examples/azure_container_instance_example.html) for the full example.** ‚úÖ

### Azure Container Apps

For more complex, larger-scale applications, Azure Container Apps may be your go-to. This service is designed for deploying, running, and scaling containerized applications. Container Apps does support https by default, which is an advantage over ACI.

To give an example, deploying a prediction service through Azure Container Apps would generally involve a Docker container and a YAML file for configuration. Please refer to the Azure Container Apps documentation for a step-by-step guide.

- [Azure Container Apps](https://docs.microsoft.com/en-us/azure/container-apps/overview) üìö

Currently there is no Python SDK for Azure Container Apps, so you can use the Azure Portal GUI or the Azure CLI to deploy your application.

#### Example Deploying an ML Model inside a Container with Azure Container Apps üì¶

In this example, we'll deploy a model inside a container using Azure Container Apps. We'll use the same model that we trained and registered in the previous examples as well as the API that we created in the previous example on FastAPI.

We will assume that the container containing the API has already been built and tested and is hosted on docker hub. From there, we will follow these steps:

- Use the Azure GUI to create a new Azure Container App. üì¶
  - Define basic settings, such as the name, resource group, and region.
  - Define the container image and port.
  - Define the ingress settings (Where the traffic will come from).
  - Create and deploy the Azure Container App.
- Test the Azure Container App. üèóÔ∏è

‚úÖ**Click [here](./Examples/azure_container_app_example.html) for the full example.** ‚úÖ

### Comparison

Here are some general pros and cons of using Azure ML Managed Online Endpoints, Azure Container Instances (ACI), and Azure Container Applications for machine learning applications:

Azure ML Managed Online Endpoints
- Pros:
  1. **Managed Service:** Azure takes care of the infrastructure, allowing you to focus more on model deployment and less on the management.
  2. **Auto-Scaling:** It scales automatically based on the workload, which is beneficial for models with variable usage.
  3. **Integration:** It integrates well with Azure's ecosystem like Azure Pipelines, Azure Monitor, etc. This can be advantageous for developing a unified ML pipeline.
- Cons:
  1. **Limited Customization:** There's limited room for customization beyond provided machine learning functionalities.
  2. **Cost:** Depending on usage, costs could potentially be higher compared to running your own managed services.

Azure Container Instances (ACI)
- Pros:
  1. **Simplicity:** ACI is a straightforward way to run a container without worrying about managing underlying infrastructure.
  2. **Flexibility:** It's more flexible than Azure ML Managed Endpoints and can run any containerized application, including a wide range of ML models.
- Cons:
  1. **Scalability:** ACI doesn't support auto-scaling like Azure ML Managed Endpoints do. You need to manage the scaling yourself.
  2. **Durability:** It's designed for short-lived, single-container applications, and may not be as robust for long-running applications.

Azure Container Applications
- Pros:
  1. **Multi-Container Applications:** If your machine learning application consists of multiple microservices (each potentially in its own container), this is a more suitable solution.
  2. **Scalability:** It's designed to scale out complex applications.
- Cons:
  1. **Complexity:** This service requires more management compared to ACI and Azure ML Managed Endpoints. Even though it's less than fully managing a Kubernetes cluster, it still demands more effort.
  2. **Overkill for Simple Applications:** If your application only consists of a single container, this service may provide more functionality than you need.

It's important to understand that each service has its own strengths and weaknesses. The best choice depends on the specific requirements of your machine learning application. Factors to consider include how complex your application is, how much traffic you expect, how much management you're willing to do, and what your budget is.

The table below summarizes the key differences between Azure ML Managed Online Endpoints, Azure Container Instances (ACI), and Azure Container Applications:

|    | Azure ML Managed Online Endpoints | Azure Container Instances (ACI) | Azure Container Applications |
|----|---------------------------------|---------------------------------|-----------------------------|
| Description | Managed and scalable machine learning model deployment service by Azure. | A service that allows you to run containers without managing any underlying infrastructure. | A service designed for deploying, running, and scaling containerized applications. |
| Primary Use Cases | Primarily used for deploying and inferencing machine learning models. | Primarily used for running short-lived and single-container applications, or for testing and development purposes. | Primarily used for running long-lived, multi-container applications in a production environment. |
| Scaling | Automatically scales out based on the workload. Supports both real-time and batch inferencing. | Manual scaling is needed. Each instance runs as a separate container. | Designed to scale out complex, multi-container applications. |
| Pricing Model | Pay-as-you-go based on the number of transactions and compute time. | Pay-as-you-go, based on the resources (CPU and memory) used. | Pricing depends on the resources (CPU and memory) used, with additional costs for premium features. |
| Infrastructure Management | Fully managed by Azure. | Minimal management needed. | Requires more management compared to ACI, but less than fully managing a Kubernetes cluster. |
| Integration with other Azure Services | Integrates with Azure Pipelines for CI/CD, Azure Monitor for monitoring, and Azure Active Directory for authentication. | Integrates with other Azure services, like Azure Logic Apps, Azure Functions, and more. | Integrates with Azure services, including Azure Logic Apps, Azure Functions, Azure Monitor, and more. |
| Customization | Limited customization beyond provided machine learning functionalities. | Limited customization. Primarily used for running containerized applications without additional features. | High customization. Supports deploying complex applications with multiple containers. |

Please note that this table provides a high-level comparison, and more detailed information can be found on the respective service documentation page.

- [Azure ML Managed Online Endpoints](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints) üìö
- [Azure Container Instances (ACI)](https://docs.microsoft.com/en-us/azure/container-instances/) üìö
- [Azure Container Applications](https://docs.microsoft.com/en-us/azure/container-apps/) üìö


## Additional Resources üìö

- [Azure Machine Learning Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/) üìö
- [Develop custom object detection models with NVIDIA and Azure Machine Learning](https://learn.microsoft.com/en-us/training/paths/develop-custom-object-detection-models-with-nvidia-and-azure-machine-learning/) üìö
- [MLOps Workflow Tutorial](https://learn.microsoft.com/en-us/training/paths/build-first-machine-operations-workflow/)




