{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "municipal-caribbean",
   "metadata": {},
   "source": [
    "In this DataLab you will implement a Naive Bayes classifier as described in Chapter 4 of the book Speech and Language Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re                                  \n",
    "import string  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-determination",
   "metadata": {},
   "source": [
    "**Tweet preprocessing**\n",
    "\n",
    "Last week you learned how to use regular expressions to process tweets. Use the function `tweet_processor()` you created in the last DataLab here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-thriller",
   "metadata": {},
   "source": [
    "And sanity check if it works.\n",
    "    \n",
    "Example tweet:\n",
    "    \n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "\n",
    "`['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = ('My beautiful sunflowers on a sunny Friday morning off :)'\n",
    "                 ' #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i')\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_processor(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-assurance",
   "metadata": {},
   "source": [
    "Before going any further, let's split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr = all_positive_tweets[:4000]\n",
    "positive_tweets_te = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr = all_negative_tweets[:4000]\n",
    "negative_tweets_te = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-jonathan",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "\n",
    "The function `tweet_processor()` expects a single tweet to process. But you have lists of tweets to process. Write a function called `tweet_processor_list()` that accept a list of strings (tweets) and returns a list of processed tweets. A processed tweet is a list of tokens. Therefore  `tweet_processor_list()` should return a list of lists.\n",
    "\n",
    "The first two items in the `positive_tweets_tr` are:\n",
    "\n",
    "```\n",
    "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
    " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!']\n",
    " ```\n",
    " \n",
    " the expected output of `tweet_processor_list()` is:\n",
    " \n",
    " ```\n",
    " [['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'],\n",
    " ['hey',\n",
    "  'jame',\n",
    "  'odd',\n",
    "  ':/',\n",
    "  'pleas',\n",
    "  'call',\n",
    "  'contact',\n",
    "  'centr',\n",
    "  '02392441234',\n",
    "  'abl',\n",
    "  'assist',\n",
    "  ':)',\n",
    "  'mani',\n",
    "  'thank']]\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor_list(tweet_list):\n",
    "    # YOUR CODE HERE #\n",
    "    return processed_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tr = tweet_processor_list(positive_tweets_tr)\n",
    "positive_tweets_te = tweet_processor_list(positive_tweets_te)\n",
    "\n",
    "negative_tweets_tr = tweet_processor_list(negative_tweets_tr)\n",
    "negative_tweets_te = tweet_processor_list(negative_tweets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-playing",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Now it is time to creative the _vocabulary_ as defined in Chapter 4, Section 4.2:\n",
    "\n",
    "> vocabulary V consists of the union of all the word types in all classes\n",
    "\n",
    "Combine all the tokens in `positive_tweets_tr` and `negative_tweets_tr` into one big list and get the unique tokens from this list.\n",
    "\n",
    "Expected length of the vocabulary is `9085` unique tokens. Notice that if you use a different train/test split or different preprocessing this number will be different.\n",
    "\n",
    "First 50 tokens in the vocabulary:\n",
    "\n",
    "```\n",
    "['(-:',\n",
    " '(:',\n",
    " '):',\n",
    " '--->',\n",
    " '-->',\n",
    " '->',\n",
    " '.\\n.',\n",
    " '.\\n.\\n.',\n",
    " '. .',\n",
    " '. . .',\n",
    " '. ..',\n",
    " '. ...',\n",
    " '..',\n",
    " '...',\n",
    " '0',\n",
    " '0-100',\n",
    " '0-2',\n",
    " '0.001',\n",
    " '0.7',\n",
    " '00',\n",
    " '00128835',\n",
    " '009',\n",
    " '00962778381',\n",
    " '01282',\n",
    " '01482',\n",
    " '01:15',\n",
    " '01:16',\n",
    " '02079',\n",
    " '02392441234',\n",
    " '0272 3306',\n",
    " '0330 333 7234',\n",
    " '0345',\n",
    " '05.15',\n",
    " '07:02',\n",
    " '07:17',\n",
    " '07:24',\n",
    " '07:25',\n",
    " '07:32',\n",
    " '07:34',\n",
    " '08',\n",
    " '0878 0388',\n",
    " '08962464174',\n",
    " '0ne',\n",
    " '1',\n",
    " '1,300',\n",
    " '1,500',\n",
    " '1-0',\n",
    " '1.300',\n",
    " '1.8',\n",
    " '1/2']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-things",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "In order to calculate the equation 4.12\n",
    "\n",
    "$P(w_i|c)=count(w_i, c)/\\Sigma_{w∈V} count(w, c)$\n",
    "\n",
    "We first need to calculate $count(w_i, c)$ which is the number of times each token in the vocabulary occurs in tweets from class c. This is also called the word frequency table.\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) |\n",
    "| ----------- | ----------- |----------- |\n",
    "|(-:|1|0|\n",
    "|(:|1|6|\n",
    "|):|6|6|\n",
    "|--->|1|0|\n",
    "|happi|161|18|\n",
    "\n",
    "\n",
    "First create a dictionary called `freq` where keys are tokens and values are lists containing positive and negative counts\n",
    "\n",
    "```\n",
    "{'(-:': [1, 0],\n",
    " '(:': [1, 6],\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "and convert it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "freqs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(freqs, orient='index', columns=['count(w_i, +)', 'count(w_i, -)'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-velvet",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "We can calculate the equation 4.12 now:\n",
    "\n",
    "$P(w_i|c)=count(w_i, c)/\\Sigma_{w∈V} count(w, c)$\n",
    "\n",
    "The denominator $\\Sigma_{w∈V} count(w, c)$ is simply sum of each column.\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) | P(w_i\\|+) | P(w_i\\|-) |\n",
    "| ----------- | ----------- |----------- |----------- |----------- |\n",
    "|(-:|1|0|0.000037|0.000000|\n",
    "|(:|1|6|0.000037|0.000222|\n",
    "|):|6|6|0.000224|0.000222|\n",
    "|--->|1|0|0.000037|0.000000|\n",
    "|happi|161|18|0.005998|0.000666|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-threshold",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "\n",
    "Apply Laplacian smoothing as described in equation 4.14\n",
    "\n",
    "$P(w_i|c)=[count(w_i, c)+1]/[\\Sigma_{w∈V} count(w, c)$+len(vocabulary)]\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) | P(w_i\\|+) | P(w_i\\|-) |P(w_i\\|+) smooth | P(w_i\\|-) smooth |\n",
    "| ----------- | ----------- |----------- |----------- |----------- |----------- |----------- |\n",
    "|(-:|1|0|0.000037|0.000000|0.000056|0.000028|\n",
    "|(:|1|6|0.000037|0.000222|0.000056|0.000194|\n",
    "|):|6|6|0.000224|0.000222|0.000195|0.000194|\n",
    "|--->|1|0|0.000037|0.000000|0.000056|0.000028|\n",
    "|happi|161|18|0.005998|0.000666|0.004509|0.000526|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-target",
   "metadata": {},
   "source": [
    "**Task 6**\n",
    "\n",
    "The final piece of the puzzle is equation 4.11\n",
    "\n",
    "$P(c) = N_c/N_{doc}$\n",
    "\n",
    "$N_c$: the number of tweet in our training data with class c\n",
    "$N_{doc}$: the total number of tweets.\n",
    "\n",
    "P(+) = number of positive tweets / number of tweets\n",
    "\n",
    "P(-) = number of negative tweets / number of tweets\n",
    "\n",
    "Calculate P(+) and P(-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-newton",
   "metadata": {},
   "source": [
    "**Task 7**\n",
    "\n",
    "Write the Naive Bayes algorithm by implementing equations 4.5/4.6\n",
    "\n",
    "Say we have a tweet with 2 tokens `['damnit', ':(']`. Probability of these tweet being positive is proportional to:\n",
    "\n",
    "`P(tweet|+)P(+)` = `P('damnit'|+) * P(':('|+) * P(+)`\n",
    "\n",
    "and negative is proportional to:\n",
    "\n",
    "`P(tweet|-)P(-)` = `P('damnit'|-) * P(':('|-) * P(-)`\n",
    "\n",
    "If `P(tweet|+)P(+)` > `P(tweet|-)P(-)`, tweet is positive and else negative.\n",
    "\n",
    "Predict whether this tweet is positive or negative using equations described above. Use the probabilities calculated using Laplacian smoothing.\n",
    "\n",
    "Remember, section 4.2 page 62\n",
    "\n",
    "> What do we do about words that occur in our test data but are not in our vocab- ulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = negative_tweets_te[3]\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pos, prob_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob_pos > prob_neg:\n",
    "    print('Class positive')\n",
    "else:\n",
    "    print('Class negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-collect",
   "metadata": {},
   "source": [
    "**Task 8**\n",
    "\n",
    "As explained in section 4.1 page 61\n",
    "\n",
    "> Naive Bayes calculations, like calculations for language modeling, are done in log space, to avoid underflow and increase speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical underflow\n",
    "print(0.5**1000)\n",
    "print(0.5**10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-raise",
   "metadata": {},
   "source": [
    "Calcuate log likelihoods for P(w_i|+)\\_smooth and P(w_i|-)\\_smooth\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) | P(w_i\\|+) | P(w_i\\|-) |P(w_i\\|+) smooth | P(w_i\\|-) smooth |log(P(w_i\\|+) smooth)|log(P(w_i\\|-) smooth)|\n",
    "| ----------- | ----------- |----------- |----------- |----------- |----------- |----------- |----------- |----------- |\n",
    "|(-:|1|0|0.000037|0.000000|0.000056|0.000028|-9.796125|-10.494519|\n",
    "|(:|1|6|0.000037|0.000222|0.000056|0.000194|-9.796125|-8.548609|\n",
    "|):|6|6|0.000224|0.000222|0.000195|0.000194|-8.543362|-8.548609|\n",
    "|--->|1|0|0.000037|0.000000|0.000056|0.000028|-9.796125|-10.494519|\n",
    "|happi|161|18|0.005998|0.000666|0.004509|0.000526|-5.401676|-7.550080|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-pension",
   "metadata": {},
   "source": [
    "**Task 9**\n",
    "\n",
    "Repeat Task 7 but this time using log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = negative_tweets_te[3]\n",
    "print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_pos, log_prob_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_prob_pos), np.exp(log_prob_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pos, prob_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_prob_pos > log_prob_neg:\n",
    "    print('Class positive')\n",
    "else:\n",
    "    print('Class negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-ceiling",
   "metadata": {},
   "source": [
    "**Task 10**\n",
    "\n",
    "Putting everything together, predict whether a tweet is positive or negative, for each tweet in the test set. Calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_preds = []\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "    \n",
    "y_preds = np.array(y_preds)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_preds == y_test)/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
