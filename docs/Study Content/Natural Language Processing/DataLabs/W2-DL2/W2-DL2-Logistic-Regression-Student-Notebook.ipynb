{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-russia",
   "metadata": {},
   "source": [
    "In the last DataLab you implemented a Naive Bayes classifier. You created a frequency table. In this DataLab you will use this table to create features for a logistic regression algorithm, and use scikit-learn to build the model.\n",
    "\n",
    "**Chapter 5 of the book \"Speech and Language Processing\" is referenced in this notebook.**\n",
    "\n",
    "First, repeat the steps you did in the last DataLab until and including Task 3. In other words create this table again:\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) |\n",
    "| ----------- | ----------- |----------- |\n",
    "|(-:|1|0|\n",
    "|(:|1|6|\n",
    "|):|6|6|\n",
    "|--->|1|0|\n",
    "|happi|161|18|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re                                  \n",
    "import string  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-dealer",
   "metadata": {},
   "source": [
    "## 1) Tweet preprocessing\n",
    "\n",
    "Again, use the function `tweet_processor()` you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-mounting",
   "metadata": {},
   "source": [
    "And sanity check if it works.\n",
    "    \n",
    "Example tweet:\n",
    "    \n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Expected output:\n",
    "\n",
    "`['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = ('My beautiful sunflowers on a sunny Friday morning off :)'\n",
    "                 ' #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i')\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_processor(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-remove",
   "metadata": {},
   "source": [
    "Before going any further, let's split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr = all_positive_tweets[:4000]\n",
    "positive_tweets_te = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr = all_negative_tweets[:4000]\n",
    "negative_tweets_te = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-heating",
   "metadata": {},
   "source": [
    "**Task 1 (From the last DataLab)**\n",
    "\n",
    "The function `tweet_processor()` expects a single tweet to process. But you have lists of tweets to process. Write a function called `tweet_processor_list()` that accept a list of strings (tweets) and returns a list of processed tweets. A processed tweet is a list of tokens. Therefore  `tweet_processor_list()` should return a list of lists.\n",
    "\n",
    "The first two items in the `positive_tweets_tr` are:\n",
    "\n",
    "```\n",
    "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
    " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!']\n",
    " ```\n",
    " \n",
    " the expected output of `tweet_processor_list()` is:\n",
    " \n",
    " ```\n",
    " [['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'],\n",
    " ['hey',\n",
    "  'jame',\n",
    "  'odd',\n",
    "  ':/',\n",
    "  'pleas',\n",
    "  'call',\n",
    "  'contact',\n",
    "  'centr',\n",
    "  '02392441234',\n",
    "  'abl',\n",
    "  'assist',\n",
    "  ':)',\n",
    "  'mani',\n",
    "  'thank']]\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_processor_list(tweet_list):\n",
    "    # YOUR CODE HERE #\n",
    "    return processed_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tr = tweet_processor_list(positive_tweets_tr)\n",
    "positive_tweets_te = tweet_processor_list(positive_tweets_te)\n",
    "\n",
    "negative_tweets_tr = tweet_processor_list(negative_tweets_tr)\n",
    "negative_tweets_te = tweet_processor_list(negative_tweets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-biodiversity",
   "metadata": {},
   "source": [
    "**Task 2  (From the last DataLab)**\n",
    "\n",
    "Now it is time to creative the _vocabulary_ as defined in Chapter 4, Section 4.2:\n",
    "\n",
    "> vocabulary V consists of the union of all the word types in all classes\n",
    "\n",
    "Combine all the tokens in `positive_tweets_tr` and `negative_tweets_tr` into one big list and get the unique tokens from this list.\n",
    "\n",
    "Expected length of the vocabulary is `9085` unique tokens. Notice that if you use a different train/test split or different preprocessing this number will be different.\n",
    "\n",
    "First 50 tokens in the vocabulary:\n",
    "\n",
    "```\n",
    "['(-:',\n",
    " '(:',\n",
    " '):',\n",
    " '--->',\n",
    " '-->',\n",
    " '->',\n",
    " '.\\n.',\n",
    " '.\\n.\\n.',\n",
    " '. .',\n",
    " '. . .',\n",
    " '. ..',\n",
    " '. ...',\n",
    " '..',\n",
    " '...',\n",
    " '0',\n",
    " '0-100',\n",
    " '0-2',\n",
    " '0.001',\n",
    " '0.7',\n",
    " '00',\n",
    " '00128835',\n",
    " '009',\n",
    " '00962778381',\n",
    " '01282',\n",
    " '01482',\n",
    " '01:15',\n",
    " '01:16',\n",
    " '02079',\n",
    " '02392441234',\n",
    " '0272 3306',\n",
    " '0330 333 7234',\n",
    " '0345',\n",
    " '05.15',\n",
    " '07:02',\n",
    " '07:17',\n",
    " '07:24',\n",
    " '07:25',\n",
    " '07:32',\n",
    " '07:34',\n",
    " '08',\n",
    " '0878 0388',\n",
    " '08962464174',\n",
    " '0ne',\n",
    " '1',\n",
    " '1,300',\n",
    " '1,500',\n",
    " '1-0',\n",
    " '1.300',\n",
    " '1.8',\n",
    " '1/2']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-destination",
   "metadata": {},
   "source": [
    "**Task 3  (From the last DataLab)**\n",
    "\n",
    "In order to calculate the equation 4.12\n",
    "\n",
    "$P(w_i|c)=count(w_i, c)/\\Sigma_{w∈V} count(w, c)$\n",
    "\n",
    "We first need to calculate $count(w_i, c)$ which is the number of times each token in the vocabulary occurs in class c. This is also called the word frequency table.\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) |\n",
    "| ----------- | ----------- |----------- |\n",
    "|(-:|1|0|\n",
    "|(:|1|6|\n",
    "|):|6|6|\n",
    "|--->|1|0|\n",
    "|happi|161|18|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count_pos = Counter(vocab_pos)\n",
    "word_count_neg = Counter(vocab_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #\n",
    "freqs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(freqs, orient='index', columns=['count(w_i, +)', 'count(w_i, -)'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-fountain",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Now it is time to create features for a logistic regression model. How can we create features from the following table?\n",
    "\n",
    "|$w_i$| count($w_i$, +) | count($w_i$, -) |\n",
    "| ----------- | ----------- |----------- |\n",
    "|(-:|1|0|\n",
    "|(:|1|6|\n",
    "|):|6|6|\n",
    "|--->|1|0|\n",
    "|happi|161|18|\n",
    "\n",
    "Let's create two features, one for the positive counts and one for the negative counts.\n",
    "\n",
    "- For each token in the tweet get count($w_i$, +) from the table\n",
    "- Calculate the sum\n",
    "- This will be your first feature ($x_1$)\n",
    "\n",
    "Similarly\n",
    "\n",
    "- For each token in the tweet get count($w_i$, -) from the table\n",
    "- Calculate the sum\n",
    "- This will be your second feature ($x_2$)\n",
    "\n",
    "Finally, repeat this for every tweet in the training and test sets. Let's get back to our example tweet to better understand what you need to do.\n",
    "\n",
    "Example tweet (raw):\n",
    "    \n",
    "`My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i`\n",
    "\n",
    "Example tweet (processed):\n",
    "\n",
    "`['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']`\n",
    "\n",
    "|tokens|count(w_i, +)|count(w_i, -)|\n",
    "|--|--|--|\n",
    "|beauti|45|10|\n",
    "|sunflow|2|0|\n",
    "|sunni|5|1|\n",
    "|friday|91|9|\n",
    "|morn|68|23|\n",
    "|:)|2847|2|\n",
    "|sunflow|2|0|\n",
    "|favourit|9|8|\n",
    "|happi|161|18|\n",
    "|friday|91|9|\n",
    "|…|31|14|\n",
    "|Total|$x_1$ = 3352|$x_2$ = 94|\n",
    "\n",
    "Sanity check your array shapes. Expected outputs:\n",
    "\n",
    "|Array|Shape|\n",
    "|--|--|\n",
    "|X_train|(8000, 2)|\n",
    "|y_train|(8000,)|\n",
    "|X_test|(2000, 2)|\n",
    "|y_test|(2000,)|\n",
    "\n",
    "\n",
    "X_train output:\n",
    "\n",
    "```\n",
    "array([[2847,    2],\n",
    "       [ 504,   94],\n",
    "       [   2,    1],\n",
    "       ...,\n",
    "       [   0,  378],\n",
    "       [   1, 3663],\n",
    "       [   1, 3663]])\n",
    "```\n",
    "\n",
    "**Task 1 (First task of this DataLab)**\n",
    "\n",
    "Create $x_1$ and $x_2$ as described above, for each tweet in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-sauce",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Now you are ready to build a logistic regression model using scikit-learn.\n",
    "\n",
    "Try with and without normalization (as described in section 5.2 page 83 of the book Speech and Language Processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-stick",
   "metadata": {},
   "source": [
    "Compare the two models you have developed (Naive Bayes and Logistic Regression) considering _5.2.4 Choosing a classifier_ on page 85 of the book Speech and Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-invitation",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "Designing new features (discussed in page 83 of the book Speech and Language Processing) is an important part of building models. You created two features in Task 1. Now, design your own features and try to improve the model performance. Check the table on page 82 for inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-adelaide",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "Use the `SentimentIntensityAnalyzer` from `nltk` to predict the sentiment of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-combine",
   "metadata": {},
   "source": [
    "It tells you if a tweet (or a text in general) is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The acting was good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-crest",
   "metadata": {},
   "source": [
    "Notice that a tweet can contain both sentiments at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The acting was good, but the story was bad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The acting was bad, but the story was good.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-majority",
   "metadata": {},
   "source": [
    "Use the `compound` score to decide whether a tweet is positive or negative. If the compound is a positive number, the prediction is positive. If it is a negative number the prediction is negative.\n",
    "\n",
    "If the `compound` is zero it is neither positive, nor negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I feel neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-highlight",
   "metadata": {},
   "source": [
    "Now calculate the accuracy of `SentimentIntensityAnalyzer` on the **raw** test tweets. Decide how you would like to handle neutral tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr_raw = all_positive_tweets[:4000]\n",
    "positive_tweets_te_raw = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr_raw = all_negative_tweets[:4000]\n",
    "negative_tweets_te_raw = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-mechanism",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "\n",
    "Use the results of `SentimentIntensityAnalyzer` as new features to your Logistic Regression model. Try with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-coach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
