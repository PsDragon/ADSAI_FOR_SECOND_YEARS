{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "directed-spouse",
   "metadata": {},
   "source": [
    "Before starting the DataLab, preprocess the tweets like you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-sheet",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        processed_tweet: a list of token\n",
    "        \n",
    "    Processing steps:\n",
    "    - Removes hyperlinks\n",
    "    - Removes # sign\n",
    "    - Tokenizes\n",
    "    - Removes stopwords and punctuation\n",
    "    - Stem tokens\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE #\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "tweet_processed = tweet_processor(tweet)\n",
    "\n",
    "print(tweet)\n",
    "print(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% training 20% testing\n",
    "positive_tweets_tr = all_positive_tweets[:4000]\n",
    "positive_tweets_te = all_positive_tweets[4000:]\n",
    "\n",
    "negative_tweets_tr = all_negative_tweets[:4000]\n",
    "negative_tweets_te = all_negative_tweets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-genius",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweet_processor_list(tweet_list):\n",
    "    # YOUR CODE HERE #\n",
    "    return processed_tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets_tr = tweet_processor_list(positive_tweets_tr)\n",
    "positive_tweets_te = tweet_processor_list(positive_tweets_te)\n",
    "\n",
    "negative_tweets_tr = tweet_processor_list(negative_tweets_tr)\n",
    "negative_tweets_te = tweet_processor_list(negative_tweets_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-builder",
   "metadata": {},
   "source": [
    "You already did the steps until here in the previous DataLab. Let's do a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(positive_tweets_tr) == 4000\n",
    "assert len(negative_tweets_tr) == 4000\n",
    "\n",
    "assert len(positive_tweets_te) == 1000\n",
    "assert len(negative_tweets_te) == 1000\n",
    "\n",
    "assert type(positive_tweets_tr) is list\n",
    "assert type(positive_tweets_tr[0]) is list\n",
    "assert type(positive_tweets_tr[0][0]) is str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-summary",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this DataLab you will train a neural network with an embedding layer for classifying tweets as positive and negative.\n",
    "\n",
    "Until now, we based our models (Logistic Regression and Naive Bayes) on counting the words. For the first time, we will try to capture meaning as well by using embeddings.\n",
    "\n",
    "To achieve this, we need to represent words as numbers because neural networks work with numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-entry",
   "metadata": {},
   "source": [
    "## 1. Converting words to numbers\n",
    "\n",
    "Before using the embedding layer, we need to convert tokens to numbers. The best way to explain how to do it is to actually show it. So let's use an example corpus with only 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = ['This is a tasty apple.',\n",
    "                      'Hello John!',\n",
    "                      'I liked the movie.',\n",
    "                      'I have a car.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-nightlife",
   "metadata": {},
   "source": [
    "Let's use `Tokenizer` from Keras to convert these documents to numbers:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-style",
   "metadata": {},
   "source": [
    "What we just did was to simply tokenize all the documents, find the unique tokens (or words) and assign a number (index) to them. You can view the results by using the `word_index` attribute on the `tokenizer` object, which in fact is our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-death",
   "metadata": {},
   "source": [
    "Now I can use the `tokenizer` object again to convert documents to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(training_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-george",
   "metadata": {},
   "source": [
    "`'Hello John!'` becomes `[7, 8]`.\n",
    "\n",
    "But what happens if you would like to convert a document with an unknown word (out of vocabulary)? For example `'Hello Mary!'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['Hello Mary!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-accreditation",
   "metadata": {},
   "source": [
    "Unknown word is ignored. This is not ideal. It is better to have a special token that indicates unknown words. Let's repeat the steps above but this time with `oov_token=\"<OOV>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-morris",
   "metadata": {},
   "source": [
    "This time we have a new token for out of vocabulary words `<OOV>` with `1` as its index. Which means whenever we have an unknown word, it will be indexed as `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(['Hello Mary!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-emergency",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "\n",
    "Write a document with a few words and convert it to numbers using `tokenizer.texts_to_sequences()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-collins",
   "metadata": {},
   "source": [
    "Let's take a look at our corpus turned into sequences again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(training_documents)\n",
    "training_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-correlation",
   "metadata": {},
   "source": [
    "Notice that naturally, the length of the sentences are different. But typically machine learning models expect a fixed input size, in other words a fixed number of features. Handling this is as easy as padding the short sentences with zeros. This can be done using `pad_sequences` from Keras.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences(training_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-pontiac",
   "metadata": {},
   "source": [
    "Zeros are added to the short sentences and now all sequences are 5 numbers long. Now let's dig deeper to understand the default arguments and how to change them.\n",
    "\n",
    "For example by default zeros are added to the left. We can use `padding='post'` to add zeros to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-amazon",
   "metadata": {},
   "source": [
    "Note that the length of the sequences are equal to the longest sentence. We can make it shorter or longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-amount",
   "metadata": {},
   "source": [
    "and as you might have guessed we can decide if we want to truncate from left or right of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences(training_sequences,\n",
    "              padding='post',\n",
    "              maxlen=3,\n",
    "              truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-blond",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "We started with a list of strings, in other words a corpus with documents.\n",
    "\n",
    "```\n",
    "training_documents = ['This is a tasty apple.',\n",
    "                      'Hello John!',\n",
    "                      'I liked the movie.',\n",
    "                      'I have a car.']\n",
    "```\n",
    "\n",
    "Then, fit a `tokenizer` to it, which assigned a number to every unique word.\n",
    "\n",
    "```\n",
    "{'<OOV>': 1,\n",
    " 'a': 2,\n",
    " 'i': 3,\n",
    " 'this': 4,\n",
    " 'is': 5,\n",
    " 'tasty': 6,\n",
    " 'apple': 7,\n",
    " 'hello': 8,\n",
    " 'john': 9,\n",
    " 'liked': 10,\n",
    " 'the': 11,\n",
    " 'movie': 12,\n",
    " 'have': 13,\n",
    " 'car': 14}\n",
    "```\n",
    "\n",
    "Finally we converted strings into numbers. We used padding to obtain a fixed length for sequences.\n",
    "\n",
    "```\n",
    "array([[ 4,  5,  2,  6,  7],\n",
    "       [ 8,  9,  0,  0,  0],\n",
    "       [ 3, 10, 11, 12,  0],\n",
    "       [ 3, 13,  2, 14,  0]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-republic",
   "metadata": {},
   "source": [
    "## 2. Converting tweets to numbers\n",
    "\n",
    "Now it is time to apply what you have learned to tweets. But let's first create `training_tweets` and `test_tweets` by combining positive and negative tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = positive_tweets_tr + negative_tweets_tr\n",
    "test_tweets = positive_tweets_te + negative_tweets_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-fundamentals",
   "metadata": {},
   "source": [
    "While we are creating our dataset, we can also create our labels. We know that first half of `training_tweets` and `test_tweets` are positive (label = 1) and second half is negative (label = 0). Therefore creating the labels is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.append(np.ones(len(positive_tweets_tr)),\n",
    "                    np.zeros(len(negative_tweets_tr)))\n",
    "\n",
    "y_test = np.append(np.ones(len(positive_tweets_te)),\n",
    "                   np.zeros(len(negative_tweets_te)))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-viewer",
   "metadata": {},
   "source": [
    "Remember that we already preprocessed and tokenized our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-yeast",
   "metadata": {},
   "source": [
    "But Keras `Tokenizer()` expects a list of strings. So let's combine tokens into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_str = []\n",
    "for tw in training_tweets:\n",
    "    training_tweets_str.append(' '.join(tw))\n",
    "    \n",
    "test_tweets_str = []\n",
    "for tw in test_tweets:\n",
    "    test_tweets_str.append(' '.join(tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-technician",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "\n",
    "Use tokenizer on `training_tweets_str`. Notice that tokenizer processes text with the `filters` parameter. Set it to `filters=''` to prevent processing because we already processed our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-continuity",
   "metadata": {},
   "source": [
    "**Task 2.2**\n",
    "\n",
    "Calculate the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = # YOUR CODE HERE #\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-brown",
   "metadata": {},
   "source": [
    "**Task 2.3**\n",
    "\n",
    "Find the numbers that represent the words `'boy'`, `'girl'`, `'man'` and `'woman'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "boy_index = # YOUR CODE HERE #\n",
    "girl_index = # YOUR CODE HERE #\n",
    "man_index = # YOUR CODE HERE #\n",
    "woman_index = # YOUR CODE HERE #\n",
    "\n",
    "print(f\"The index of 'boy' is {boy_index}\")\n",
    "print(f\"The index of 'girl' is {girl_index}\")\n",
    "print(f\"The index of 'man' is {man_index}\")\n",
    "print(f\"The index of 'woman' is {woman_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-relay",
   "metadata": {},
   "source": [
    "**Task 2.4**\n",
    "\n",
    "Convert training and test tweets to sequences and use padding.\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "`'followfriday top engag member commun week :)'`\n",
    "\n",
    "Corresponding sequence:\n",
    "\n",
    "`[347, 221, 937, 400, 286, 52, 3]`\n",
    "\n",
    "Padded sequence:\n",
    "\n",
    "`array([347, 221, 937, 400, 286,  52,   3,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "         0,   0,   0,   0], dtype=int32)`\n",
    "\n",
    "\n",
    "For padding arguments use `padding='post'` and `maxlen=30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = # YOUR CODE HERE #\n",
    "training_padded = # YOUR CODE HERE #\n",
    "\n",
    "test_sequences = # YOUR CODE HERE #\n",
    "test_padded = # YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_padded.shape == (8000, 30)\n",
    "assert test_padded.shape == (2000, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-justice",
   "metadata": {},
   "source": [
    "## 3. Build a neural network with an embedding layer\n",
    "\n",
    "You just converted tweets to numbers, now we are ready to train a neural network on this dataset. Let's first define `X_train`, `y_train`, `X_test`, `y_test`. We already created the labels and padded sequences will be our `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_padded\n",
    "X_test = test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-madison",
   "metadata": {},
   "source": [
    "**Task 3.1**\n",
    "\n",
    "Build a `Sequential` model from Keras. First layer should be an `Embedding` layer. In the `Embedding` layer define the following parameters:\n",
    "\n",
    ">input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    ">output_dim: Integer. Dimension of the dense embedding.\n",
    "\n",
    ">input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "Note that `input_dim` is `vocab_size + 1` because we are padding with zeros. For `output_dim` please use `2` because we would like to plot the embeddings. Finally for `input_length` use `30` because we used `maxlen=30` during padding.\n",
    "\n",
    "After the `Embedding` layer flatten its output and connect `Dense` layers. As a last layer, add a `Dense` layer suitable for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import # YOUR CODE HERE #\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-customer",
   "metadata": {},
   "source": [
    "**Task 3.2**\n",
    "\n",
    "Compile the model by selecting a proper loss, optimizer and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-manhattan",
   "metadata": {},
   "source": [
    "**Task 3.3**\n",
    "\n",
    "Train the model with `X_train` and `y_train`. Use `X_test` and `y_test` as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-somalia",
   "metadata": {},
   "source": [
    "You just developed a neural network for sentiment analysis congrats!\n",
    "\n",
    "**Task 3.4**\n",
    "\n",
    "Predict the class of a test tweet.\n",
    "\n",
    "Example tweet:\n",
    "`\"back thnx god i'm happi :)\"`\n",
    "\n",
    "Model prediction:\n",
    "`array([[0.99999976]], dtype=float32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-amsterdam",
   "metadata": {},
   "source": [
    "## 4. Semantic properties of embeddings\n",
    "\n",
    "Take a look at Figures 6.15 and 6.16 (Section 6.10, page 126, Speech and Language Processing). They show one of the ways embeddings capture meaning in language. Let's try if embeddings from our model learned any semantic properties.\n",
    "\n",
    "In order to achieve this we need to access the output of our trained embedding layer, which is the first layer. We can access each layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-fifty",
   "metadata": {},
   "source": [
    "and input/output of a layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-buffer",
   "metadata": {},
   "source": [
    "We can create a new model only from the trained Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "embedding_model = Model(inputs=model.layers[0].input,\n",
    "                        outputs=model.layers[0].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-tower",
   "metadata": {},
   "source": [
    "`embedding_model` accepts sequences as input and returns the output of the embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-french",
   "metadata": {},
   "source": [
    "**Task 4.1**\n",
    "\n",
    "We can use this model, just like any model. Use the `embedding_model` on the test tweet you used for Task 3.4 and get the embeddings. You should obtain two numbers for each word in the tweet. These two numbers are called word embeddings.\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "`\"back thnx god i'm happi :)\"`\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[[ 2.03354090e-01, -1.61128387e-01],\n",
    "        [ 3.02020200e-02, -4.17610519e-02],\n",
    "        [-6.28291816e-03, -3.17817833e-03],\n",
    "        [ 1.16536245e-01, -1.00184120e-01],\n",
    "        [-2.93731004e-01,  2.99061388e-01],\n",
    "        [-1.42090285e+00,  1.37957323e+00],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        [-7.92113831e-04,  2.80260388e-02],\n",
    "        ...\n",
    "        [-7.92113831e-04,  2.80260388e-02]]], dtype=float32)\n",
    "```\n",
    "\n",
    "where `'back'` is `[ 2.03354090e-01, -1.61128387e-01]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-store",
   "metadata": {},
   "source": [
    "Using the idea above, let's create a function that can give us a vector (word embedding) for any given word.\n",
    "\n",
    "**Task 4.2**\n",
    "\n",
    "Write a function that:\n",
    "- Accepts a word as a string\n",
    "```\n",
    "'man'\n",
    "```\n",
    "- Converts it into a sequence using the `tokenizer` you fitted previously\n",
    "```\n",
    "[[199]]\n",
    "```\n",
    "- Pads it\n",
    "```\n",
    "array([[199,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "          0,   0,   0,   0]], dtype=int32)\n",
    "```\n",
    "- Uses the `embedding_model` to get the embeddings\n",
    "```\n",
    "array([[[ 0.13211662,  0.11832968],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        [-0.03486646, -0.00182698],\n",
    "        ...\n",
    "        [-0.03486646, -0.00182698]]], dtype=float32)\n",
    "```\n",
    "- And finally returns the embedding (vector) that corresponds to the word e.g. \n",
    "```\n",
    "array([0.13211662, 0.11832968], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(word):\n",
    "    '''\n",
    "    Input:\n",
    "        word: a list containing the word as a string e.g. ['man']\n",
    "    Output:\n",
    "        vector: a numpy array containing the vector obtained \n",
    "                from the trained embedding layer\n",
    "    \n",
    "    '''\n",
    "    # YOUR CODE HERE #\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = word_to_vector('man')\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(vector) == np.ndarray\n",
    "assert vector.shape == (2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-report",
   "metadata": {},
   "source": [
    "**Task 4.3**\n",
    "\n",
    "Plot the embeddings of the following words `['boy', 'girl', 'man', 'woman']` and check if your model captures the male-female relation.\n",
    "\n",
    "<img src=https://i.imgur.com/CUZuvxW.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_plotter(words):\n",
    "    # YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_plotter(['boy', 'girl', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-consequence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
