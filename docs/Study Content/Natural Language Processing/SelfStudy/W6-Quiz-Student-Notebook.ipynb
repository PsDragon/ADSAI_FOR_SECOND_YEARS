{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "robust-basic",
   "metadata": {},
   "source": [
    "## Sklearn TF-IDF Tutorial\n",
    "\n",
    "Before learning how to calculate TF-IDF using sklearn, let's first start with the `CountVectorizer` as it is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-excuse",
   "metadata": {},
   "source": [
    "Imagine having the following simple corpus with 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ['This train is leaving the train station to make room for another train',\n",
    "         'You can use this room as your office.',\n",
    "         'We have no room for error in this experiment.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-guard",
   "metadata": {},
   "source": [
    "We can create the document-term matrix (https://en.wikipedia.org/wiki/Document-term_matrix) very easily as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "counts = c_vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_array = counts.toarray()\n",
    "c_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or get_feature_names_out() depending on the sklearn version\n",
    "c_tokens = c_vectorizer.get_feature_names()\n",
    "c_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.DataFrame(\n",
    "    data = c_array,\n",
    "    index = [f'Doc {i}' for i in range(c_array.shape[0])],\n",
    "    columns = c_tokens)\n",
    "c_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-reward",
   "metadata": {},
   "source": [
    "Now we can use `c_vectorizer` on new document(s) using the `transform` method instead of `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['There is no room in this train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_test = c_vectorizer.transform(test)\n",
    "c_array_test = counts_test.toarray()\n",
    "c_array_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tokens = c_vectorizer.get_feature_names()\n",
    "c_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df_test = pd.DataFrame(\n",
    "    data = c_array_test,\n",
    "    index = [f'Doc {i}' for i in range(c_array_test.shape[0])],\n",
    "    columns = c_tokens)\n",
    "\n",
    "c_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-reviewer",
   "metadata": {},
   "source": [
    "Now let's apply `TfidfVectorizer` in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ['This train is leaving the train station to make room for another train',\n",
    "         'You can use this room as your office.',\n",
    "         'We have no room for error in this experiment.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(train)\n",
    "tfidf_array = tfidf.toarray()\n",
    "tfidf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokens = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(\n",
    "    data = tfidf_array,\n",
    "    index = [f'Doc {i}' for i in range(tfidf_array.shape[0])],\n",
    "    columns = tfidf_tokens)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['There is no room in this train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_vectorizer.transform(test)\n",
    "tfidf_array_test = tfidf_test.toarray()\n",
    "tfidf_array_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokens_test = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_tokens_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_test = pd.DataFrame(\n",
    "    data = tfidf_array_test,\n",
    "    index = [f'Doc {i}' for i in range(tfidf_array_test.shape[0])],\n",
    "    columns = tfidf_tokens_test)\n",
    "tfidf_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-exposure",
   "metadata": {},
   "source": [
    "Think about why the term 'train' has a higher value than 'room', even though counts were 1 for both.\n",
    "\n",
    "Finally it is **very important** to read the `TfidfVectorizer` documentation to learn about the parameters. For example I removed the stop words using the keyword argument `stop_words='english'`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-combat",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "nltk.download('twitter_samples')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_positive = all_positive_tweets[:4000]\n",
    "test_positive = all_positive_tweets[4000:]\n",
    "\n",
    "training_negative = all_negative_tweets[:4000]\n",
    "test_negative = all_negative_tweets[4000:]\n",
    "\n",
    "training_set = training_positive + training_negative\n",
    "test_set = test_positive + test_negative\n",
    "\n",
    "print(len(training_set), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-butler",
   "metadata": {},
   "source": [
    "Let's create one big document (a string) that contains all the positive tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = ''\n",
    "for tweet in training_positive:\n",
    "    positive_tweets += ' ' + tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-principle",
   "metadata": {},
   "source": [
    "Similarly for the negative tweets let's create one big document (a string) that contains all the negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = ''\n",
    "for tweet in training_negative:\n",
    "    negative_tweets += ' ' + tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-skiing",
   "metadata": {},
   "source": [
    "and create the corpus we will analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [positive_tweets, negative_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus[0]), len(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-shadow",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Use `TfidfVectorizer` on `corpus` to calculate tf-idf values for each token in positive and negative tweets. The resulting dataframe should look like the one below where each column is a token and first row representing all positive tweets and the second row representing all the negative tweets:\n",
    "\n",
    "||00|000|001|00128835|009|00962778381838|...|ｓｅｅ|\n",
    "|--|--|--|--|--|--|--|--|--|\n",
    "|Positive|0.001657|0.010481|0.000000|0.000000|0.001165|0.001165|...|0.000000|\n",
    "|Negative|0.002383|0.000000|0.001675|0.001675|0.000000|0.000000|...|0.058611|\n",
    "2 rows × 17223 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-smell",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Calculate td-idf for the tokens `happy` and `sad`\n",
    "\n",
    "||happy|sad|\n",
    "|--|--|--|\n",
    "|Positive|0.124290|0.003314|\n",
    "|Negative|0.021447|0.119148|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-experience",
   "metadata": {},
   "source": [
    "In this final part of the quiz, your task is to do part of speech tagging for an example tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = training_positive[0]\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-model",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Find the parts of speech for the following tweet\n",
    "\n",
    "Example tweet:\n",
    "\n",
    "```\n",
    "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "[('#', '#'),\n",
    " ('FollowFriday', 'NNP'),\n",
    " ('@', 'NNP'),\n",
    " ('France_Inte', 'NNP'),\n",
    " ('@', 'NNP'),\n",
    " ('PKuchly57', 'NNP'),\n",
    " ('@', 'NNP'),\n",
    " ('Milipol_Paris', 'NNP'),\n",
    " ('for', 'IN'),\n",
    " ('being', 'VBG'),\n",
    " ('top', 'JJ'),\n",
    " ('engaged', 'VBN'),\n",
    " ('members', 'NNS'),\n",
    " ('in', 'IN'),\n",
    " ('my', 'PRP$'),\n",
    " ('community', 'NN'),\n",
    " ('this', 'DT'),\n",
    " ('week', 'NN'),\n",
    " (':', ':'),\n",
    " (')', ')')]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-feeding",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Use `nltk.help.upenn_tagset()` to get the explanation for the tags. Print the explanation for 'DT' and 'NN'.\n",
    "\n",
    "```\n",
    "DT: determiner\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those\n",
    "    \n",
    "NN: noun, common, singular or mass\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
