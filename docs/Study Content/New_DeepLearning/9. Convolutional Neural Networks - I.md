---
sort: 14
---

# Convolutional Neural Networks

Convolutional Neural Networks (CNNs) use layers specifically designed for image data. These layers capture local relationships between nearby features in an image.

Previously, in our feed-forward model, we multiplied our normalized pixels by a large weight matrix (of shape (65536, 100)) to generate our next set of features.

However, when we use a convolutional layer, we learn a set of smaller weight tensors, called filters (also known as kernels). We move each of these filters (i.e. convolve them) across the height and width of our input, to generate a new “image” of features. Each new “pixel” results from applying the filter to that location in the original image.

The interactive on the right demonstrates how we convolve a filter across a single image.

Why do convolution-based approaches work well for image data?
Convolution can reduce the size of an input image using only a few parameters.
Filters compute new features by only combining features that are near each other in the image. This operation encourages the model to look for local patterns (e.g., edges and objects).
Convolutional layers will produce similar outputs even when the objects in an image are translated (For example, if there were a giraffe in the bottom or top of the frame). This is because the same filters are applied across the entire image.
Before deep nets, researchers in computer vision would hand design these filters to capture specific information. For example, a 3x3 filter could be hard-coded to activate when convolved over pixels along a vertical or horizontal edge:

<figure>
    <img src=".\assets\CNN\Edge-detection-filters.svg" />
</figure>
<br>

However, with deep nets, we can learn each weight in our filter (along with a bias term)! As in our feed-forward layers, these weights are learnable parameters. Typically, we randomly initialize our filters and use gradient descent to learn a better set of weights. By randomly initializing our filters, we ensure that different filters learn different types of information, like vertical versus horizontal edge detectors.

## Configuring a Convolutional Layer - Filters

In Keras, we can define a Conv2D layer to handle the forward and backward passes of convolution.

```python
#Defines a convolutional layer with 4 filters, each of size 5 by 5:
 
tf.keras.layers.Conv2D(4, 5, activation="relu"))  
```

When defining a convolutional layer, we can specify the number and size of the filters that we convolve across each image.

**Number of Filters**
When using convolutional layers, we don’t just convolve one filter. Instead, we define some number of filters. We convolve each of these in turn to produce a new set of features. Then we stack these outputs (one for each filter) together in a new “image.”

<figure>
    <img src=".\assets\CNN\Stacking_features.gif" />
</figure>
<br>

Our output tensor is then (batch_size, new height, new width, number of filters). We call this last dimension number of channels ( or feature maps ). These are the result of applying a single filter across the entire image.

**Filter Size**

<figure>
    <img src=".\assets\CNN\filter_diagram.svg" />
</figure>
<br>

Beyond configuring the number of filters, we can also configure their size. Each filter has three dimensions: [Height, Width, Input Channels]

- Height: the height of our filter (in pixels)
- Width: the width of our filter (also in pixels)
- Input Channels: The number of input channels. In a black and white image, there is 1 input channel (grayscale). However, in an RGB image, there are three input channels. Note that we don’t have control over this dimension (it depends on the input), and Keras takes care of this last dimension for us.

Increasing height or width increases the number of pixels that a filter can pay attention to at each step in the convolution. However, doing so also increases the number of learnable parameters. People commonly use filters of size 5x5 and 3x3.

In total, the number of parameters in a convolution layer is:

_Number of filters×(Input Channels×Height×Width+1)_

Every filter has height, width, and thickness (The number of input channels), along with a bias term.

This convolutional layer has 8 filters, and each is 3x3.

```python
print("\n\nModel with 8 filters:")
 
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(256, 256, 1)))
 
#Adds a Conv2D layer with 8 filters, each size 3x3:
model.add(tf.keras.layers.Conv2D(8, 3,activation="relu"))
model.summary()
```

Observe the dimensionality of the output, and the number of parameters.

## Configuring a Convolutional Layer - Stride and Padding

Two other hyperparameters in a convolutional layer are Stride and Padding.

**Stride**
The stride hyperparameter is how much we move the filter each time we apply it. The default stride is 1, meaning that we move the filter across the image 1-pixel at a time. When we reach the end of a row in the image, we then go to the next one.

If we use a stride greater than 1, we do not apply our filter centered on every pixel. Instead, we move the filter multiple pixels at a time.

<figure>
    <img src=".\assets\CNN\stride.gif" />
</figure>
<br>

For example, if strides = 2, we move the filter two columns over at a time, and then skip every other row.

We can set the stride to any integer. For example, we can define a Conv2D layer with a stride of 3:

```python
#Adds a Conv2D layer with 8 filters, each size 5x5, and uses a stride of 3:
model.add(tf.keras.layers.Conv2D(8, 5,
strides=3,
activation="relu"))
```
Larger strides allow us to decrease the size of our output. In the case where our stride=2, we apply our filter to every other pixel. As a result, we will halve the height and width of our output.

**Padding**
The padding hyperparameter defines what we do once our filter gets to the end of a row/column. In other words: “what happens when we run out of image?” There are two main methods for what to do here:

We just stop (valid padding): The default option is to just stop when our kernel moves off the image. Let’s say we are convolving a 3x3 filter across a 7x7 image with stride=1. Our output will then be a 5x5 image, because we can’t process the 6th pixel without our filter hanging off the image.

<figure>
    <img src=".\assets\CNN\convolution.gif" />
</figure>
<br>

We keep going (same padding): Another option is to pad our input by surrounding our input with zeros. In this case, if we add zeros around our 7x7 image, then we can apply the 3x3 filter to every single pixel. This approach is called “same” padding, because if stride=1, the output of the layer will be the same height and width as the input.

<figure>
    <img src=".\assets\CNN\Padding.gif" />
</figure>
<br>

We can use “same” padding by setting the padding parameter:

```python
#Adds a Conv2D layer with 8 filters, each size 5x5, and uses a stride of 3:
model.add(tf.keras.layers.Conv2D(8, 5,
strides=3,
padding='same',
activation="relu"))
```

## Adding Convolutional Layers to Your Model

### Adding One Convolutional Layer

Now, we can modify our feed-forward image classification code to use a convolutional layer:

- First, we are going to replace the first two Dense layers with a Conv2D layer.
- Then, we want to move the Flatten layer between the convolutional and last dense layer. Because dense layers apply their matrix to the dimension, we will always need to flatten the output of convolutional layers before passing them into a dense layer.

### Stacking Convolutional Layers
However, we won’t stop there! The beauty of neural networks is that we can stack many layers to learn richer combinations of features. We can stack convolutional layers the same way we stacked dense layers.

For example, we can stack three convolutional layers with distinct filter shapes and strides:

```python
# 8 5x5 filters, with strides of 3
model.add(tf.keras.layers.Conv2D(8, 5, strides=3, activation="relu"))
 
# 4 3x3 filters, with strides of 3
model.add(tf.keras.layers.Conv2D(4, 3, strides=3, activation="relu"))
 
# 2 2x2 filters, with strides of 2
model.add(tf.keras.layers.Conv2D(2, 3, strides=2, activation="relu"))
```

Like with dense layers, the output of one convolutional layer can be passed as input to another. You can think of the output as a new input “image,” with a height, width, and number of channels. The number of filters used in the previous layer becomes the number of channels that we input into the next!

As with dense layers, we should use non-linear activation functions between these convolutional layers.


## Pooling

Another part of Convolutional Networks is Pooling Layers: layers that pool local information to reduce the dimensionality of intermediate convolutional outputs.

There are many different types of pooling layer, but the most common is called Max pooling:

- Like in convolution, we move windows of specified size across our input. We can specify the stride and padding in a max pooling layer.
- However, instead of multiplying each image patch by a filter, we replace the patch with its maximum value.

<figure>
    <img src=".\assets\CNN\Max-Pooling.gif" />
</figure>
<br>

For example, we can define a max pooling layer that will move a 3x3 window across the input, with a stride of 3 and valid padding:

```python
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(3, 3),   strides=(3, 3), padding='valid')
```

Beyond helping reduce the size of hidden layers (and reducing overfitting), max pooling layers have another useful property: they provide some amount of translational invariance. In other words, even if we move around objects in the input image, the output will be the same. This is very useful for classification. For example, we usually want to classify an image of a cat as a cat, regardless of how the cat is oriented in the image.

## Feature Maps

In a convolutional neural network, feature maps are the result of convolving a single filter across our input, and they provide a way to visualize a model’s internal workings. They allow us to see how our network responds to a particular image in ways that are not always apparent when we only examine the raw filter weights.

Two example filters are displayed. Darker squares correspond to more negative weights, while whiter squares are more positive ones.

<figure>
    <img src=".\assets\CNN\learned_filters.png" />
</figure>
<br>