---
sort: 11
---
# Data Lab: Creative Brief

If you had succesfully completed the workshops this week, you should
now have:

- [x] your Github contains the ```banijay_EDA``` folder with the 
    - [x] ```banijay_data_content.ipynb``` notebook which reads in the raw content data, processes it, and saves the processed data.
    - [x] ```banijay_data_ratings.ipynb``` notebook which reads in the raw ratings data.
    - [x] A ```data``` folder containing the raw and processed data files.

In **today's and Friday's data lab**, you will merge these concepts and solve the following use-cases to help the client with their data analysis. 

We will learn how such projects are typically structured and how to work with a client to solve their problems. In the industry, normally, such tasks can be broken down into sub-tasks and assigned to different team members. For example, one team member can be responsible for creating the lookup table to merge the datasets, while another team member can be responsible for creating the analysis and reports. Finally, another team member can be responsible for creating the models which use AI and ML to help the client improve their business processes.

In Block D, we will introduce team-based projects and how to work with a team to solve such problems. For now, we will work individually with the assistance of your mentors to solve the problems. 

Also in Block D, you will then learn about databases and how to use them to store, query, and analyze data (rather than creating flat files as we do in this block). However, please note that one of the medal challenges this block involves creating a ```data pipeline``` to automate the process of reading in the raw data, processing it, visualising (reporting) it, and saving it to a database for further analysis.

Before we start, please make sure you have read the sections on ```The final report``` and ```Plagiarism``` you can find in the bottom of this page.

**Let's get started: Keep the coffee flowing and the music playing!** 

:thumbsup: :headphones:  :coffee:

## Use-case 1: Data preparation and processing.

This block places emphasis on data preparation and processing, a key element of the CRISP-DM data science lifecycle. In this use-case, you will be asked to clean and process the data to prepare it for analysis.

We start with the content data and then move on to the ratings data. You have already completed the first part of this use-case in the workshops earlier this week.


### Content data preparation

Please complete the following tasks:

- [ ] Goto your analysis folder and open the notebook ```banijay_data_content.ipynb```.
- [ ] Perform the following operations to prepare the dataset for analysis:
    - [ ] Check and remove missing data
    - [ ] Check and remove duplicate data (duplicated across all columns)
    - [ ] Check the format of the ```start``` and ```end``` columns and convert them to ```datetime``` format. Hint: remove the last 3 characters from the string. 
    - [ ] Create a ```date_time_start``` and ```date_time_end``` column by combining the date and time from the ```Datum```,```start``` and ```end``` columns. Hint: use the ```pd.to_datetime()``` function.
    - [ ] split the ```id``` column into ```show_id``` and ```fragment``` columns where fragments can be 1,2,3,or 4. Hint: use the ```str.split()``` function.
    - [ ] For the non-dutch speakers, you may choose to translate the data to English before proceeding with your analysis if you haven't already done so. Click [here](https://gist.github.com/nbhushan/a88bf700e54e99b1f259275ba6f89642) for an example of how to do so using Python.
    - [ ] Feel free to explore the data and perform any additional steps if you feel they are necessary.
- [ ] use the ```to_csv()``` function to save the dataframe as a CSV file and name it ```banijay_op1data_content_processed.csv``` and upload/commit the changes to your Github repos.

### Ratings data preparation

Please complete the following tasks:

- [ ] Goto your analysis folder and open the notebook ```banijay_data_ratings.ipynb```.
- [ ] Perform the following operations to prepare the dataset for analysis:
    - [ ] Check and remove missing data
    - [ ] Check and remove duplicate data (duplicated across all columns). Hint: you might notice that the Date and Time columns occur multiple times in the dataset. But are they duplicated, or not?
    - [ ] Check the format of the ```date``` and ```time``` columns and combine them to create a ```date_time``` column and ensure that it's a ```datetime``` object. Hint: use the ```pd.to_datetime()``` function.
    - [ ] Feel free to explore the data and perform any additional steps if you feel they are necessary.

Now that we have prepped and cleaned the data, we can move on to the next use-case.

## Use-case 2: Linking Content Data to Ratings Data

 As you may have noticed, the content data and ratings data are stored in different files. In order to perform any analysis, you will need to link the two datasets. 
 
 However, merging datasets often requires a common key. 
 For example, if we have one data frame containing ```student_id``` and ```student_name``` and another data frame containing ```student_id``` and ```student_grade```, we can merge the two data frames on the ```student_id``` column and create a dataframe containing the three columns which we can use for further analysis.

 Ideally, we would like to merge the content data and ratings data on the ```content_id``` column. However, the content data contains a ```content_id``` column, while the ratings data does not. Therefore, we need to figure out a way to still be able to merge the two datasets.

To solve this use-case, you will need to:

- [ ] create a lookup table that allows you link the content and the ratings data. In simpler terms, every ratings in the ratings data should be linked to a show, and specifically a fragment in the content data.
- [ ] use the lookup table to link the content and ratings data and save it as a new file named ```banijay_op1data_content_ratings.csv```.

[Reveal answer](https://gist.github.com/nbhushan/472a53e8e9d7dc060c7084bed9a93a2f){target="_blank"}


## Use-case 3: Content Ratings Analysis

Now that you have linked the content and ratings data, you can perform some analysis on the data and generate reports that provide the client with insight. Please use visluizations to help you communicate your findings (as you did in Block A.). 



**I highly recommend you to use Python (matplotlib, seaborn & bokeh) to generate the reports and visuals. If you are more comfortable using Power BI, you can use it to generate the reports, but we will return to Python when we start building our Machine Learning Models in week 4**

If you are using Python, please create a new Jupyter notebook and ensure that every analysis part you see below is it's own block.

Answer the following questions using ```kdh1000``` as an indicator for how well a show is performing, and focus on the ```totaal``` ratings type.

### Target Audience Analysis

Let's begin by looking at the target audience of the shows and understanding the audience's preferences and how they differ from each other.

1. What is the average rating for all shows across all target groups?
2. What is the average rating for all shows, per target group?


> Feel free to dive deeper into the data and come up with more interesting insights.

Please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.

### Content Analysis

Please answer the following questions:

1. Who are the most highly rated hosts?
2. What are the most highly rated shows and what are the most highly rated fragments in those shows?
3. For the top 5 fragments in terms of ratings, what were the keywords used in the content? Bonus: Try creating a word cloud to visualise the keywords.

> Feel free to dive deeper into the data and come up with more interesting insights.

Again, please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.


### Trend Analysis

In this section, we will look at the trends in the ratings data and try to understand how the ratings have changed over time.

1. What is the average rating for all shows, per month? Do you notice any trends?
2. What is the average rating for all shows, per day of the week? Do you notice any trends?
3. Repeat the above analysis, but for the different target groups. Do you notice any trends?

> Feel free to dive deeper into the data and come up with more interesting insights.

Again, please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.


# The final report

Documentation is a very important part of the project, and you will need to ensure that you have a well-written report that clearly communicates your findings. Start writing the introduction and EDA section of your report. Do not wait until week 8 to start writing your report. I strongly recommend you to need to write the report in stages as that will help you receive feedback well in time, and you will have time in week 8 to polish your report and tie up any loose ends. 


# Plagiarism

Please note that plagiarism is a serious offence. You are encouraged to use the internet to help you with your analysis, but you must ensure that you cite your sources.

 Further, you are encouraged to discuss the use-cases with your peers, but you must ensure that you write your own code.If your code exactly resembles the code of another student, you will be reported to the board of examiners. If you are unsure about whether your code is similar to another student's code, please ask your mentor to review your code.
