---
sort: 13
---
# Data Lab: Creative Brief

In **today's data lab**, we continue with the use-cases you were introduced to last DataLab. 

## Use-Case Table

Here's a table of the use-cases and their description.

| Use-Case | Description | DataSets | Data Source | Link
|----------|-------------|-------------|-------------|-------------|
| 1 | Pre-process and clean data | Content and Ratings Data | Banijay, SKO | [clicky](#use-case-1-linking-content-data-to-ratings-data)
| 2 | Merge data | Content and Ratings Data | Banijay, SKO | [clicky](#use-case-2-linking-content-data-to-ratings-data)
| 3 | Content and Ratings exploration | Content and Ratings Data | Banijay, SKO | [clicky](#use-case-3-content-ratings-analysis)
| 4 | Integrating Social Media Buzz | Twitter_data | Twitter API | [clicky](#use-case-4-social-media-analysis)
| 5 | Machine Learning Analysis | Content, Twitter, and Ratings Data | Banijay,SKO, Twitter API | [clicky](#use-case-5-machine-learning-analysis)


:bell: Before we start, please make sure you have read the sections on [The final report](https://adsai.buas.nl/Study%20Content/Programming/11.DataLabPR3.html#the-final-report) and [Plagiarism](https://adsai.buas.nl/Study%20Content/Programming/11.DataLabPR3.html#plagiarism) you can find in the bottom of this page.

**Let's get started: Keep the coffee flowing and the music playing!**

:thumbsup: :headphones:  :coffee:

## Use-case 1: Pre-processing and cleaning data 

This block places emphasis on data preparation and processing, a key element of the CRISP-DM data science lifecycle. In this use-case, you will be asked to clean and process the data to prepare it for analysis.

We start with the content data and then move on to the ratings data. You have already completed the first part of this use-case in the workshops earlier this week.

To understand what the ratings data measure, please read the [ratings data glossary](https://edubuas.sharepoint.com/:w:/t/2022-23BFAI1.P2-01ADSAI/Ebf8wQ7u3kFNt6207XGFzxcBc0Cw768ZzJf6mgfSHeYZGg?e=Luz5gh).

Please click [here](https://adsai.buas.nl/Study%20Content/Programming/11.DataLabPR3.html#use-case-1-data-preparation-and-processing) for more details.

## Use-case 2: Linking Content Data to Ratings Data

 As you may have noticed, the content data and ratings data are stored in different files. In order to perform any analysis, you will need to link the two datasets.

 However, merging datasets often requires a common key.
 For example, if we have one data frame containing ```student_id``` and ```student_name``` and another data frame containing ```student_id``` and ```student_grade```, we can merge the two data frames on the ```student_id``` column and create a dataframe containing the three columns which we can use for further analysis.

 Ideally, we would like to merge the content data and ratings data on the ```content_id``` column. However, the content data contains a ```content_id``` column, while the ratings data does not. Therefore, we need to figure out a way to still be able to merge the two datasets.

To solve this use-case, you will need to:

- [ ] create a lookup table that allows you link the content and the ratings data. In simpler terms, every ratings in the ratings data should be linked to a show, and specifically a fragment (segment) in the content data.
- [ ] use the lookup table to link the content and ratings data and save it as a new file named ```banijay_op1data_content_ratings.csv```.

Please click [here](https://gist.github.com/nbhushan/472a53e8e9d7dc060c7084bed9a93a2f) for more details.

## Use-case 3: Content Ratings Analysis

Now that you have linked the content and ratings data, you can perform some analysis on the data and generate reports that provide the client with insight. Please use visluizations to help you communicate your findings (as you did in Block A.). 


**If you are more comfortable using Power BI, you can use it to generate the reports, but we will return to Python when we start building our Machine Learning Models in week 4**

If you are using Python, please create a new Jupyter notebook and ensure that every analysis part you see below is it's own block.

Answer the following questions using ```kdh000``` as an indicator for how well a show is performing, and focus on the ```totaal``` ratings type.

Before you can proceed with your analysis, you need to first pick a level of analysis. This implies that all the questions below should be answered at the same level of analysis. For example, if you choose to analyze the data at the show level, you should answer all the questions below at the show level. However, Banijay has requested us to analyze the data at the fragment level. 

> Note that the words 'fragment' and 'segment' are used interchangeably in this document.

> Note that the ratings data provides ratings every minute, whereas the content data provides data per fragment.

### Level of analysis

- [ ] Hint: use the ```groupby()``` function. Check what happens when you include ```as_index=False``` as an argument to the ```groupby()``` function. Read the dopcumentation for more information.

Here is an example of how to do this in Python:

```python   
# index columns are the categories we want to group by.
# group the data by index columns (Date, Show_ID, Fragment, Target_group, Ratings type, etc.) 
# and aggregate the numerical values by taking the mean
aggregated_df = df.groupby(['<add index columns here as list of columns']).mean()

aggregated_df.head()
``` 


### Target Audience Analysis

Let's begin by looking at the target audience of the shows and understanding the audience's preferences and how they differ from each other.

1. What is the average  ```kdh000```? 
2. What is the average  ```kdh000```, per target group?

> Feel free to dive deeper into the data and come up with more interesting insights.

Please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.

### Content Analysis

> For the non-dutch speakers, you may choose to translate the data to English before proceeding with your analysis if you haven't already done so. Click [here](https://gist.github.com/nbhushan/a88bf700e54e99b1f259275ba6f89642) for an example of how to do so using Python.

1. Who are the most highly rated hosts?  Visualize the top 5.
2. What are the most highly rated shows and what are the most highly rated fragments in those shows? Visualize the top 5.
3. For the top 5 fragments in terms of ratings, what were the keywords used in the content? Bonus: Try creating a word cloud to visualise the keywords.

> Feel free to dive deeper into the data and come up with more interesting insights.

Again, please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.

### Trend Analysis

In this section, we will look at the trends in the ratings data and try to understand how the ratings have changed over time.

1. What is the average rating for all shows, per month? Do you notice any trends?
2. What is the average rating for all shows, per day of the week? Do you notice any trends?
3. Repeat the above analysis, but for the target group with the highest average rating. Do you notice any trends?

> Feel free to dive deeper into the data and come up with more interesting insights.

Again, please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.

## Use-case 4: Social Media Analysis

In this use-case, we will use the social media data to understand how the shows are performing on social media. In particular, we will look at the number of tweets and the number of retweets for each show, and calculate twitter engagement for each show and fragment.

> The tweets are posted by [npo1](https://twitter.com/op1npo?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor); the broadcaster of the show Op1.

To solve this use-case, you will need to:

### Data Preparation

- [ ] prepare your environment (create a notebook, import the necessary libraries, etc.)
- [ ] read in the ```banijay_op1data_twitter_raw.json``` file from [here](https://edubuas.sharepoint.com/:u:/t/2022-23BFAI1.P2-01ADSAI/ERSv7_R_7BtCgeBjThjsMz8B8NHDKw8fngCL7SpKXq38LQ?e=ldQcjn) using the following code:

```python
import json
import pandas as pd

# read in the json file
with open('banijay_op1data_twitter_raw.json', 'rb') as f:
    twitter_json = json.load(f)

# convert the json file to a pandas dataframe
df_op1_twitter = pd.json_normalize(twitter_json)

df_op1_twitter.head()
```
> Look up the documentation of the ```json_normalize()``` function to understand what it does.

:bell: To understand why we add the ```rb``` argument to the ```open()``` function, please read [this](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files).

- [ ] Understand the columns of the dataframe and the data types of the columns. See [here](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet) for more details.
- [ ] Pre-process the data:
    - [ ] convert the ```created_at``` column to a datetime object
    - [ ] only retain the tweets that are not a reference to another tweet. You can do this by filtering out the tweets that have a ```referenced_tweets``` column with a ```NaN``` value.
- [ ] create a lookup table that allows you link the content and the social media data. In simpler terms, try to match the twitter data to the content and ratings data. You can do this by:
    - [ ] aggregating the twitter data by Date (extract the date from ```created_at```). You can do this by using the ```groupby()``` function.
    - [ ] merge the aggregated twitter data with the content (and ratings) data on the basis of the date. You can do this by using the ```pd.merge()``` function.
    - [ ] This will help you lookup the twitter metrics for a given ```Date``` in the content (and ratings) data.
    - [ ] save the lookup table as a new file named ```banijay_op1data_content_twitter_lookup.csv```.

- [ ] use the lookup table to link the content, ratings and social media data and save it as a new file named ```banijay_op1data_content_ratings_twitter.csv```.

Well done! If you have come this far, you have now created the one (flat) file to rule them all. You can now use this file to perform your analysis. 

:yellow_heart::raised_hands::dancer::snake::computer:

### Twitter Metric Analysis

See [here](https://adsai.buas.nl/Study%20Content/Programming/12.Python%20for%20Society.html#twitters-metrics) for more information regarding twitter metrics.

- [ ] calculate the twitter engagement for each show using the ```public.metrics```. 
- [ ] visualize the top 5 shows based on twitter engagement.
- [ ] investigate correlations between twitter engagement and ratings.

> Feel free to dive deeper into the data and come up with more interesting insights.

Again, please summarize your main findings either in your notebook, the notes file in your teams environment, or in a separate document. You will need these findings to help you write your final report.

## Use-case 5: Machine Learning Analysis

In this use-case, we will use all the datasets we have encountered so far to build a machine learning model that predicts the ratings of a show based on the content of the show, and twitter metrics. 

> You are free to come up with a research question of your choice. For example, you can investigate:

- [ ] can we predict the ratings of a show based on the hosts of the show?
- [ ] for this Target Group, can we predict the ratings of a show based on the content of the show and twitter metrics?
- [ ] can we predict the ratings of a show based on twitter metrics, and the day of the week?
- [ ] ....

:bell: Remember that any model you build will be useful to the business only if it can be used to make predictions in the future. Therefore, do not include any data that will not be available at the time of prediction (e.g., ```showID```, or ```Date```)


To solve this use-case, you will need to:

- [ ] Define your problem statement.
  - [ ] What is your target variable? 
  - [ ] What are the features you will use to predict the target variable?
  - [ ] Which model are you going to use?
  - [ ] What is the performance metric you will use to evaluate your model?
- [ ] Prepare your environment (create a notebook and name it ```banijay_op1_machinelearning.ipynb```
 import the necessary libraries and data, etc.)
- [ ] Perform feature engineering to prepare the data for modelling. For example, you may need to create dummy variables for categorical variables, or you may need to standardize (scale) the numerical variables.
- [ ] Split the data into training, validation, and test sets.
- [ ] Train a machine learning model using a training set.
- [ ] Try to improve the performance of your model by tuning the hyperparameters of your model using the validation set.
- [ ] Evaluate the performance of your model on the test set using the performance metric you defined earlier.
- [ ] Visualize the results of your model. For example, you can plot the predicted target value (or label)s against the actual target value (or label).
- [ ] Summarize your findings in the report


If you seek a challenge (and an excellent grade), you can:


- [ ] Try to improve the performance of your model by implementing different models and compare the results.
- [ ] Implement a linear regression algorithm using only ```numpy``` and the tools of Linear Algebra. [clicky](https://adsai.buas.nl/Year1/BlockB/Project%20Requirements/MachineLearningRequirements.html#linear-algebra-and-machine-learning)


:yellow_heart::raised_hands::dancer::snake::computer:

If you have come this far, you have now completed the entire creative brief and you are officially a data science ninja!! Congratulations! :tada::tada::tada:

> if you still have time remaining, please have a look at the [Medal Challenges](https://adsai.buas.nl/Year1/BlockB/#medal-challenges).

🔔 Please remember the add your notebook(s) and report (pdf) to the ```Final Deliverable``` folder on ```Github```. This, along with your learning and work logs will be part of your final portfolio for grading.

### The final report

If you are done with the use-cases, you can move on to documentation, ensure that you have saved and committed all your work to Github. Documentation is a very important part of the project, and you will need to ensure that you have a well-written report that clearly communicates your findings. Start writing the introduction and EDA section of your report. Do not wait until week 8 to start writing your report. You will need to write the report in stages, and you will need to submit your report in week 8.

### Plagiarism

Please note that plagiarism is a serious offence. You are encouraged to use the internet to help you with your analysis, but you must ensure that you cite your sources.

However, if your code exactly resembles the code of another student, you will be reported to the board of examiners. You are encouraged to discuss the use-case with your peers, but you must ensure that you write your own code. If you are unsure about whether your code is similar to another student's code, please ask your mentor to review your code.
