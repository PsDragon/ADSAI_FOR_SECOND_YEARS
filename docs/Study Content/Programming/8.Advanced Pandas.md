---
sort: 9
---

# Python: Advanced Pandas and file formats

During week 1, you worked with ```pandas```. You learned that ```pandas``` is a Python package providing data frames - a fast, flexible, and expressive data structure designed to make working with tabular data both easy and intuitive. It is one of the key pieces in the Python data science toolkit and yhou will use it a lot in the upcoming weeks!

A pandas dataframe is a data structure that can be best understood as a list of lists.
A data frame is a matrix of data, containing rows and columns,and ideally the first row contains the header - the name of each column.

<figure>
    <img src=".\assets\pddataframe.PNG" />
</figure>
<br>

Please watch the following video to learn more about dataframes in ```pandas```.

<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SdlaYzocgHg?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>
<!-- blank line -->


## Today's learning objectives

- [ ] Understand what are the commonly used file formats in data science.
- [ ] Explore the content data provided by Banijay.


## File formats

We will be working with a lot of data in the upcoming weeks. Data can be stored in different file formats. The most common file formats are CSV, JSON, and Pickle. In this section, we will learn about these file formats and how to read them in Python.

### CSV

CSV stands for Comma Separated Values. It is a file format that stores tabular data in plain text. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The first line of the file usually contains the names of the fields. CSV files are often used to exchange data between different applications. For example, you can export data from a spreadsheet program into a CSV file, and then import the data into a database.

Note that how CSV files are encoded can sometimes depends on the location of the file. For example, in the US, the comma is used as a decimal separator, while in Europe, the comma is used as a separator. This can cause problems when reading CSV files in Python. To avoid this, you can specify the encoding of the file when reading it in Python (e.g., ```encoding='utf-8'``` os explicitly setting the ```delimitor``` and ```decimal```) parameters when you read in a CSV using Pandas.

To read in CSV files in Python using ```pandas```, you can use the ```read_csv()``` function. The function takes the path to the CSV file as an argument. The function returns a ```pandas``` dataframe. 

Please see the example below and try to remember the different parameters you can pass into the function. The optional arguments are ```sep```, ```header```, ```index_col```, ```decimal```, ```encoding```, and ```parse_dates``` which also have default values as shown below.

```python
import pandas as pd

df = pd.read_csv(
    "<path>/<your_data>.csv",  # Replace with your CSV file path
    # The following arguments are optional and can be removed:
    sep=";",  # If columns aren't separated by commas, indicate the delimiter here (e.g., ;)
    header=0,   # If the first row is not the header, indicate the row number here
    names=[add your column names here in a list seperated by commas],   # If the first row is not the header, indicate the column names here   
    usecols=[add your column names here in a list seperated by commas],  # If you want to read only a subset of columns, indicate the column names here
    index_col= [add your column names here in a list seperated by commas],  # If you want to use one or more columns as the index, indicate the column names here 
    comment="#",    # If you want to ignore lines starting with a specific character, indicate the character here
    skiprows=None,  # If you want to skip the first n rows, indicate the number here
    na_values=["---", "unknown", "no info"],    # If you want to replace specific values with NaN, indicate the values here
    parse_dates=False,  # If you want to parse dates, indicate the column names here
    nrows=500,  # If you want to read only the first n rows, indicate the number here
    encoding="utf-8",   # If you want to specify the encoding of the file, indicate the encoding here
)
â€‹
df.head(10)  # Preview the first 10 lines
```
Please watch the following video to understand more about reading CSV files in Python.

<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/9ZW87wK9jP0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>


### JSON

JSON stands for JavaScript Object Notation. It is a file format that stores data in plain text. It is a lightweight data-interchange format. JSON is often used to exchange data between different applications. For example, you can export data from a spreadsheet program into a JSON file, and then import the data into a database. Please see [here](https://www.codecademy.com/article/what-is-json) to read more about this format or watch the following video to understand how Twitter uses JSON.

<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/qrruEhHqwqY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>


To read in JSON files in Python using ```pandas```, you can use the ```read_json()``` function. The function takes the path to the JSON file as an argument. The function returns a ```pandas``` dataframe. Please see the example below and try to remember the different parameters you can pass into the function. 

> More often than not, you will only have to provide the filename, and leave the rest of the function arguments to their default values.

```python   
import pandas as pd 

df = pd.read_json(
    "<path>/<your_data>.json",  # Replace with your JSON file path
    # The following arguments are optional and can be removed:
    orient="",  # Indicate the orientation of the data (e.g., "split", "records", "index", "columns")   
    typ="frame", # Indicate the type of the data (e.g., "frame", "series", "split", "records", "index", "columns")
    dtype=True, # Indicate the data type of the columns (e.g., "int", "float", "str", "object", "datetime64[ns]", "bool")
    convert_axes=True,  # Indicate whether to convert the axes to the proper dtypes
    convert_dates=True, # Indicate whether to convert dates to ISO 8601 format
    keep_default_dates=True, 
    numpy=False,    # Indicate whether to interpret list-like objects as numpy arrays
    precise_float=False,    # Indicate whether to enable the precise conversion of floating point numbers     
    date_unit=None, # Indicate the date unit to use when decoding datetime data
    encoding=None,  # Indicate the encoding to use when decoding strings
    lines=False,    # Indicate whether the file is a JSON Lines file    
    chunksize=None, # Indicate the number of lines to read at a time
    compression="infer",    # Indicate the compression method to use (e.g., "infer", "gzip", "bz2", "zip", "xz")
)   

df.head(10)  # Preview the first 10 lines
``` 

### Pickle
Pickle is a format that allows you to serialize and deserialize Python objects. Serialization is the process of converting an object into a byte stream. Deserialization is the process of converting a byte stream into an object. Pickle is often used to save the state of an object for later use. For example, you can train a machine learning model and save the model using pickle. You can then load the model later to make predictions. Please see [here](https://realpython.com/python-pickle-module/) to read more about this format and watch the following video.


<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Wpd6zM2nIgc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>

Jokes aside :), Pickle is sometimes used to store data in plain text. However, it is not a file format that is commonly used to store data. To read pickle files using the python package ```pickle```, you can use the ```load()``` function. The function takes the path to the pickle file as an argument. The function returns the object that was serialized. Please see the example below and try to remember the different parameters you can pass into the function.

```python
import pickle

with open("<path>/<your_data>.pickle", "rb") as f:  # Replace with your pickle file path
    pickle.load(f)
```   

To read in pickle files in Python using ```pandas```, you can use the ```read_pickle()``` function. The function takes the path to the pickle file as an argument. The function returns a ```pandas``` dataframe. Please see the example below and try to remember the different parameters you can pass into the function.

```warning
Please keep in mind that there is a risk that the read_pickle() function can fail to read the .pkl, as long as there is an incompatability between the version of pandas used to write to the .pkl, and the version of pandas used to read the .pkl. You can check your pandas version using the following code: ```pd.__version__```.
```


```python
import pandas as pd

df = pd.read_pickle(
    "<path>/<your_data>.pkl",  # Replace with your pickle file path
)   

df.head(10)  # Preview the first 10 lines
``` 



## Blended Learning - File Formats and Pandas [2 hr]

- [ ] Please complete the ```Introduction and flat files```and ```Importing data from other file types
``` module in the DataCamp course ```Introduction to Importing Data in Python``` which can be found [here](https://app.datacamp.com/learn/courses/introduction-to-importing-data-in-python).

> Feel free to complete the other module in the course if you have time. However, the above two modules are the only ones that are required for the following assignment.

## Creative Brief: Banijay Content Data [5 hours]

### Data import, description, and pre-processing

- [ ]  create a Jupyter notebook named ```banijay_data_content.ipynb```.
- [ ]  import the ```pandas``` library and use it to read in the content data which you can find [here](https://edubuas.sharepoint.com/:u:/t/2022-23BFAI1.P2-01ADSAI/EarzHHJ3iFtOowFSxFmYNJYBWzx8WWzn-iZ4fkdGO3jSyw?e=BqGy4R).
- [ ]  create a new code block, and use the ```head()``` function to preview the first 10 lines of the data.
- [ ]  use the ```info()``` function to preview the data types of the columns.
- [ ]  use the ```describe()``` function to preview the descriptive statistics of the data.
- [ ]  use the ```value_counts()``` function to preview the number of unique values in each column and identify the hosts that occur the most.
- [ ]  use the ```isna()``` function to preview the number of missing values in each column.
- [ ]  use the ```dropna()``` function to drop the rows that contain missing values.
- [ ]  use the ```drop()``` function to drop the columns that are not relevant to the analysis. Hint: Look for columns with the same value in each row.

> The following tasks are designed to be a challenge and we will revist them this Wednesday in the Datalab. Do not despair if you find them hard to solve. We will work through them together.

- [ ] translate the column names into English using a python library if required (e.g., https://pypi.org/project/deep-translator/).
- [ ] translate the column values (titles, summary, and keywords) into English using a python library.
- [ ] convert the Date and Time columns into datetime objects and create a ```date_time``` column that combines the two columns.
- [ ] split the ```id``` column into ```show_id``` and ```fragment``` columns where fragments can be 1,2,3,or 4.
- [ ] use the ```to_csv()``` function to save the dataframe as a CSV file and name it ```banijay_op1data_content_processed.csv``` and upload it your Github repos along with the notebooks.

> Ensure that the code you submit adheres to PEP-8 Guidelines and is well commented.


