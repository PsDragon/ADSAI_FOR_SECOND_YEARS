---
sort: 9
---

# Python: Advanced Pandas and file formats

During week 1, you worked with ```pandas```. You learned that ```pandas``` is a Python package providing data frames - a fast, flexible, and expressive data structure designed to make working with tabular data both easy and intuitive. It is one of the key pieces in the Python data science toolkit and yhou will use it a lot in the upcoming weeks!

A pandas dataframe is a data structure that can be best understood as a list of lists.
A data frame is a matrix of data, containing rows and columns,and ideally the first row contains the header - the name of each column.

<figure>
    <img src=".\assets\pddataframe.PNG" />
</figure>
<br>

Please watch the following video to learn more about dataframes in ```pandas```.

<!-- blank line -->
<figure class="video_container">
<iframe width="560" height="315" src="https://www.youtube.com/embed/SdlaYzocgHg?controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</figure>
<!-- blank line -->


## Today's learning objectives

- [ ] Understand what are the commonly used file formats in data science.
- [ ] Explore the content data provided by Banijay.


## File formats

We will be working with a lot of data in the upcoming weeks. Data can be stored in different file formats. The most common file formats are CSV, JSON, and Pickle. In this section, we will learn about these file formats and how to read them in Python.

### CSV

CSV stands for Comma Separated Values. It is a file format that stores tabular data in plain text. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The first line of the file usually contains the names of the fields. CSV files are often used to exchange data between different applications. For example, you can export data from a spreadsheet program into a CSV file, and then import the data into a database.

Note that how CSV files are encoded can sometimes depends on the location of the file. For example, in the US, the comma is used as a decimal separator, while in Europe, the comma is used as a separator. This can cause problems when reading CSV files in Python. To avoid this, you can specify the encoding of the file when reading it in Python (e.g., ```encoding='utf-8'``` os explicitly setting the ```delimitor``` and ```decimal```) parameters when you read in a CSV using Pandas.

To read in CSV files in Python using ```pandas```, you can use the ```read_csv()``` function. The function takes the path to the CSV file as an argument. The function returns a ```pandas``` dataframe. Please see the example below and try to remember the different parameters you can pass into the function.


```python
import pandas as pd

df = pd.read_csv(
    "<path>/<your_data>.csv",  # Replace with your CSV file path
    # The following arguments are optional and can be removed:
    # If columns aren't separated by commas, indicate the delimiter here (e.g., ;)
    sep=";",
    # Indicate which zero-indexed row number(s) have the column names
    header=0,
    # List of column names to use (useful for renaming columns)
    names=[add your column names here in a list seperated by commas],
    # If not all columns are needed, indicate which you need (useful for lower memory usage)
    usecols=[add your column names here in a list seperated by commas],
    # Indicate which column(s) to use as row labels
    index_col= [add your column names here in a list seperated by commas],
    # Lines starting with this string should be ignored (useful if there are file comments)
    comment="#",
    # Indicate the number of lines to skip at the start of the file (also useful for file comments)
    skiprows=None,
    # Indicate string(s) that should be recognized as NaN/NA
    na_values=["---", "unknown", "no info"],
    # Indicate which column(s) are date column(s)
    parse_dates=False,
    # Indicate number of rows to read (useful for large files)
    nrows=500,
    # Encoding to use when reading file
    encoding="utf-8",
)
â€‹
df.head(10)  # Preview the first 10 lines
```
### JSON

JSON stands for JavaScript Object Notation. It is a file format that stores data in plain text. It is a lightweight data-interchange format. JSON is often used to exchange data between different applications. For example, you can export data from a spreadsheet program into a JSON file, and then import the data into a database. Please see [here](https://www.codecademy.com/article/what-is-json) to read more about this format.

To read in JSON files in Python using ```pandas```, you can use the ```read_json()``` function. The function takes the path to the JSON file as an argument. The function returns a ```pandas``` dataframe. Please see the example below and try to remember the different parameters you can pass into the function.

```python   
import pandas as pd 

df = pd.read_json(
    "<path>/<your_data>.json",  # Replace with your JSON file path
    # The following arguments are optional and can be removed:
    # Indicate which column(s) to use as row labels
    orient="columns",
    # Indicate which column(s) to use as row labels
    typ="frame",
    # Indicate which column(s) to use as row labels
    dtype=True,
    # Indicate which column(s) to use as row labels
    convert_axes=True,
    # Indicate which column(s) to use as row labels
    convert_dates=True,
    # Indicate which column(s) to use as row labels
    keep_default_dates=True,
    # Indicate which column(s) to use as row labels
    numpy=False,
    # Indicate which column(s) to use as row labels
    precise_float=False,
    # Indicate which column(s) to use as row labels
    date_unit=None,
    # Indicate which column(s) to use as row labels
    encoding=None,
    # Indicate which column(s) to use as row labels
    lines=False,
    # Indicate which column(s) to use as row labels
    chunksize=None,
    # Indicate which column(s) to use as row labels
    compression="infer",
)   

df.head(10)  # Preview the first 10 lines
``` 

### Pickle
Pickle is a format that allows you to serialize and deserialize Python objects. Serialization is the process of converting an object into a byte stream. Deserialization is the process of converting a byte stream into an object. Pickle is often used to save the state of an object for later use. For example, you can train a machine learning model and save the model using pickle. You can then load the model later to make predictions. Please see [here](https://realpython.com/python-pickle-module/) to read more about this format.

Pickle is sometimes used to store data in plain text. However, it is not a file format that is commonly used to store data. To read pickle files using the python package ```pickle```, you can use the ```load()``` function. The function takes the path to the pickle file as an argument. The function returns the object that was serialized. Please see the example below and try to remember the different parameters you can pass into the function.

```python
import pickle

with open("<path>/<your_data>.pickle", "rb") as f:  # Replace with your pickle file path
    pickle.load(f)
```   

To read in pickle files in Python using ```pandas```, you can use the ```read_pickle()``` function. The function takes the path to the pickle file as an argument. The function returns a ```pandas``` dataframe. Please see the example below and try to remember the different parameters you can pass into the function.

```warning
Please keep in mind that there is a risk that the read_pickle() function can fail to read the .pkl, as long as there is an incompatability between the version of pandas used to write to the .pkl, and the version of pandas used to read the .pkl. You can check your pandas version using the following code: ```pd.__version__```.
```


```python
import pandas as pd

df = pd.read_pickle(
    "<path>/<your_data>.pkl",  # Replace with your pickle file path
)   

df.head(10)  # Preview the first 10 lines
``` 



## Blended Learning - File Formats and Pandas [2 hr]

- [ ] Please complete the ```Introduction and flat files```and ```Importing data from other file types
``` module in the DataCamp course ```Introduction to Importing Data in Python``` which can be found [here](https://app.datacamp.com/learn/courses/introduction-to-importing-data-in-python).

> Feel free to complete the other module in the course if you have time. However, the above two modules are the only ones that are required for the following assignment.

## Creative Brief: Banijay Content Data [5 hpurs]

### Data import, description, and pre-processing

- [ ]  create a Jupyter notebook named ```banijay_data_content.ipynb```.
- [ ]  import the ```pandas``` library and use it to read in the content data which you can find [here](https://edubuas.sharepoint.com/:u:/t/2022-23BFAI1.P2-01ADSAI/EarzHHJ3iFtOowFSxFmYNJYBWzx8WWzn-iZ4fkdGO3jSyw?e=BqGy4R).
- [ ]  use the ```head()``` function to preview the first 10 lines of the data.
- [ ]  use the ```info()``` function to preview the data types of the columns.
- [ ]  use the ```describe()``` function to preview the descriptive statistics of the data.
- [ ]  use the ```value_counts()``` function to preview the number of unique values in each column and identify the hosts that occur the most.
- [ ]  use the ```isna()``` function to preview the number of missing values in each column.

> The following tasks are designed to be a challenge and we will revist them this Wednesday in the Datalab. Do not despair if you find them hard to solve. We will work through them together.

- [ ] translate the column names into English using a python library if required (e.g., https://pypi.org/project/deep-translator/).
- [ ] translate the column values (titles, summary, and keywords) into English using a python library.
- [ ] convert the Date and Time columns into datetime objects and create a ```date_time``` column that combines the two columns.
- [ ] split the ```id``` column into ```show_id``` and ```fragment``` columns where fragments can be 1,2,3,or 4.
- [ ] use the ```to_csv()``` function to save the dataframe as a CSV file and name it ```banijay_op1data_content_processed.csv``` and upload it your Github repos along with the notebooks.





