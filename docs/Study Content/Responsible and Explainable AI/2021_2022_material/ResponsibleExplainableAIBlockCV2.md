---
sort: 7
---

# __Responsible & Explainable AI__

## __Assignment__

### __1.1: Responsible AI__

By now you should have mastered the basics responsible AI(Datalab 2, Week 1). The knowledge you acquired so far will enable you to complete this section of the assignment.

__Requirements:__

For this part of the assignment, we work with two levels that correspond to the proficiency levels in the assessment form. For example, to get a 'Insufficient' on this section of the assignment you should meet the competencies listed in the yellow block (i.e. Level 2)



<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #000000;; background-color: #FFAB91; border-color: #FFAB91;">
LEVEL 1 ('Insufficient')

<br>

Identify, and describe instances of bias in the dataset. See content on the different types of bias presented in the works of Mehrabi et al. (2019), Suresh and Guttag (2019), Khan et al. (2021).
</div>


<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #000000;; background-color: #FFE082; border-color: #FFE082;">
LEVEL 2 ('Sufficient')

<br>

Apply at least one responsible AI method that addresses the datasets' limitation in terms of bias, and subsequently evaluate its impact.

</div>



### __1.2: Explainable AI (XAI)__

For this part of the Project Brief, you are going to identify, and describe the limitations of an AI algorithm in terms transparency, and interpretability, and subsequently apply methods which address these limitations.

<img src="./images/grad_cam.jpg" alt="SQL meme" width="600"/> \
*Figure 1. Example of feature attribution with Grad-CAM.*

__Requirements:__

- Identify, and describe the limitations in terms of transparency and interpretability of the AI algorithm. See article by Tsimenidis (2020), and Google's Responsible AI post on interpretability.
- Apply one explainable AI method that addresses the algorithm's limitations in terms of transparency and interpretability, and subsequently evaluate its impact on the Open Images classification task. See workshop examples, the article by Linardatos et al. (2021), and the book by Molnar (2020).
- Apply multiple explainable AI methods that addresses limitations in terms of transparency and interpretability, and subsequently contrast their impact on the Open Images classification task. See previously mentioned literature.

__Deliverable(s):__

- A Jupyter Notebook that contains relevant code and explanations.

The Jupyter notebook is to be uploaded to Github no later than 5pm on last DataLab day. Confer with a lecturer beforehand if you are handing in something other than a Jupyter Notebook.

***

## __Literature__

Friedman, B., & Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems (TOIS), 14(3), 330-347. (Bias in Computer Systems.pdf (cornell.edu)

Karanasiou, A. P., & Pinotsis, D. A. (2017). A study into the layers of automated decision-making: emergent normative and legal aspects of deep learning. International Review of Law, Computers & Technology, 31(2), 170-187.

Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2021). Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1), 18.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019). A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635.

Meta-Learning. (2020, September). Retrieved July 09, 2021, from https://meta-learning.fastforwardlabs.com/#model-agnostic-meta-learning-(maml)

Minsky, M. L. (1991). Logical versus analogical or symbolic versus connectionist or neat versus scruffy. AI magazine, 12(2), 34-34. (https://www.aaai.org/ojs/index.php/aimagazine/article/view/894/812)

Mohamed, S., Png, M. T., & Isaac, W. (2020). Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology, 33(4), 659-684.

Molnar, C. (2020). Interpretable machine learning. https://christophm.github.io/interpretable-ml-book/.

Responsible AI practices: Interpretability. (n.d.). Retrieved July 09, 2021, from https://ai.google/responsibilities/responsible-ai-practices/?category=interpretability

Suresh, H., & Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002.

Tsimenidis, S. (2020). Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning. arXiv preprint arXiv:2012.15754.
