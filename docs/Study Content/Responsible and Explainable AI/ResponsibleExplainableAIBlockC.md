---
sort: 7
---

# __Responsible & Explainable AI__

## __Assignment__

For this part of the Project Brief, you are going to identify, and describe the limitations of an AI algorithm in terms of fairness, transparency, and interpretability, and subsequently apply methods which address these limitations.

<img src="./images/grad_cam.jpg" alt="SQL meme" width="600"/> \

*Figure 1. Example of feature attribution with Grad-CAM.*

__Requirements:__

- The student is able to identify, and describe instances of bias in the dataset, as stated in requirements of the Project Brief. See the works of Mehrabi et al. (2019), Suresh and Guttag (2019), Khan et al. (2021), and Castelnovo et. al (2021).  
- The student is able to identify, and describe the limitations in terms of transparency and interpretability of the AI algorithm, as stated in requirements of the Project Brief. And meeting all criteria in poor. See article by Tsimenidis (2020), and Google's Responsible AI post on interpretability.
- The student is able to propose at least one responsible AI method that addresses the datasets' limitation in terms of bias, and subsequently evaluate its impact, as stated in requirements of the Project Brief. And meeting all criteria in insufficient.
- The student is able to apply at least one explainable AI method that addresses the algorithm's limitations in terms of transparency and interpretability, and subsequently evaluate its impact, as stated in requirements of the Project Brief. And meeting all criteria in sufficient. See works by Linardatos et al. (2021), and Molnar (2020).
- The student is able to apply multiple responsible and explainable AI methods that addresses limitations in terms of bias, transparency and interpretability, and subsequently contrast their impact on the classification task, as stated in requirements of the Project Brief. And meeting all criteria in good. See previously mentioned literature.  

__Deliverable(s):__

- A Jupyter Notebook that contains relevant code and explanations.

The Jupyter notebook is to be uploaded to GitHub no later than 5pm on last DataLab day. Confer with a lecturer beforehand if you are handing in something other than a Jupyter Notebook.

***

## __Literature__

Castelnovo, A., Crupi, R., Greco, G., & Regoli, D. (2021). The zoo of Fairness metrics in Machine Learning. arXiv preprint arXiv:2106.00467.

Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2021). Explainable AI: A review of machine learning interpretability methods. Entropy, 23(1), 18.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2019). A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635.

Mohamed, S., Png, M. T., & Isaac, W. (2020). Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. Philosophy & Technology, 33(4), 659-684.

Molnar, C. (2020). Interpretable machine learning. https://christophm.github.io/interpretable-ml-book/.

Responsible AI practices: Interpretability. (n.d.). Retrieved July 09, 2021, from https://ai.google/responsibilities/responsible-ai-practices/?category=interpretability

Suresh, H., & Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002.

Tsimenidis, S. (2020). Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning. arXiv preprint arXiv:2012.15754.
