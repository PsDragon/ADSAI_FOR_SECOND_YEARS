---
sort: 20
---
# Custom Gym Environment and RL

## Goals for the day:

- Develop a custom gym environment wrapper for the RoboSuite environment

If you have extra time:

- Use Stable Baselines 3 to start training a reinforcement learning agent to control the robotic arm
- Push the training job to the cloud to speed up training

### Expected Output

- A custom gym environment that can be used to train a reinforcement learning agent to control the robotic arm

## General Plan:
- [ ] Discuss any questions about the knowledge modules so far
- [ ] Develop a custom gym environment wrapper for the RoboSuite environment

If you have extra time:

- [ ] Use Stable Baselines 3 to start training a reinforcement learning agent to control the robotic arm
- [ ] Push the training job to the cloud to speed up training

## Create a Custom Gym Environment

Our goal is to replace the baseline PID controllers for `x, y, z, yaw` that we created in previous datalabs as well as the `OSC_POSE` controller with a single RL controller. The PID controller can change where the arm goes to by just change the setpoints (desired x, y, z, yaw values). 

<div style="text-align: center">
<img src="images/rspiddiagram.png" height="300">
</div>

We want to replace this with a RL controller that can learn to control the arm in order to move the gripper to a desired location. 

<div style="text-align: center">
<img src="images/rsrlcontroller.png" height="300">
</div>

In order to do this we need to create a custom gym environment wrapper for the RoboSuite environment. We will extract only the observations we need from the environment (The proprioceptive state of the arm) and the actions we want to control (The desired x, y, z, yaw values) and this will from the new observation space. We will also need to define a reward function that will tell the agent how well it is doing.

Luckily, the RoboSuite environment is already compatible with the OpenAI Gym API, so we can create a simple wrapper to use it with Stable Baselines 3, with our own actions, observations, and reward function.

- Create a class that inherits from gym.Env

```python
import gym
from gym import spaces
import numpy as np

class RoboEnv(gym.Env):
    def __init__(self, RenderMode = True, Task = 'Lift'): # Add any arguments you need (Environment settings; Render mode  and task are used as examples)
        super(RoboEnv, self).__init__()
        # Initialize environment variables
        self.RenderMode = RenderMode
        self.Task = Task

        # Define action and observation space
        # They must be gym.spaces objects
        # Example when using discrete actions:
        self.action_space = # Define the action space
        # Example for using image as input:
        self.observation_space = # Define the observation space

        # Instantiate the environment
        self.env = suite.make(env_name= self.Task, 
                                robots="Panda",
                                has_renderer=self.RenderMode,
                                has_offscreen_renderer=False,
                                horizon=200,    
                                use_camera_obs=False,)


    def step(self, action):
        # Execute one time step within the environment
        # action = # Process the action if needed
        #Call the environment step function
        obs, reward, done, _ = self.env.step(action)
        # You may find it useful to create helper functions for the following
        obs = # Process the observation 
        reward = # Calculate the reward for your specific task
        # done = # Calculate if the episode is done if you want to terminate the episode early
        return obs, reward, done, _

    def reset(self):
        # Reset the state of the environment to an initial state
        # Call the environment reset function
        obs = self.env.reset()
        obs = # Process the observation if needed
        # Reset any variables that need to be reset
        self.target_pos=# Set the target position
        return obs

    def render(self, mode='human'):
        # Render the environment to the screen
        # Call the environment render function

    def close (self):
        # Close the environment
        # Call the environment close function
```
### Define the Action Space and Observation Space

The action space and observation space are defined in the init function. They are defined using the gym.spaces module. Both the action and observation spaces in this case are continuous spaces, so we will use the gym.spaces.Box class. At this stage we just need to define the shape (number of values) of the action and observation spaces. We will define the actual values in the step and reset functions.

Hint: The action space is that of the standard controller in robosuite, 8 joint positions. The values can vary from -1 to 1.

Hint 2: Create another python file where you just print out the observation after resetting robosuite. This will show you the full list of observations to choose from. We will only use the arm state (robot0-proprio-state) and target position (we will define the target position in the reset function - it will have 4 values: `x, y, z, yaw`). You can include other observations if you want, but this is a good starting point.

### Process the Full Observation from RoboSuite 

In the step and reset functions, you will need to process the observation from the environment. This will depend on the observation space you defined.

We need to extract the proprioceptive state of the arm from the full observation. This is the observation with the key `robot0_proprio-state`. We then need to combine this with the target position. This can be done using the np.hstack function.

We will define the target position in the reset function. The target position will be a 4 value array: `x, y, z, yaw`. For now just use a variable - i.e. `self.target_pos` or whatever you want to call it.

### Define the Reward Function

In the step function, you will need to define the reward function. A simple reward function is the negative distance to the target or the inverse of the distance to the target. You can use the `np.linalg.norm` function to calculate the distance between the current position (taken from the full observation) and the target position (defined later, just use the variable name for now). 

```python
#example reward function
reward = -np.linalg.norm(gripper_pos - self.target_pos)
#OR
reward = 1/np.linalg.norm(gripper_pos - self.target_pos)
```
These rewards only take the position into account. You can also include the orientation of the gripper in the reward function.

### Target Position
We will define the target position in the reset function. The target position will be a 4 value array: `x, y, z, yaw`. We want each of these values to be random, but within a certain range. For example, the x and y values should be between -0.5 and 0.5, and the z value should be between 0.8 and 1.3. The yaw value can be between -90 and 90. You can use the np.random.uniform function to generate random values within a range.

```python
# Example of generating random values
x = np.random.uniform(-0.5, 0.5)
y = np.random.uniform(-0.5, 0.5)
z = np.random.uniform(0.8, 1.3)
yaw = np.random.uniform(-90, 90)
```
You can then combine these values into a 4 value array using the np.array function.

```python
target_pos = np.array([x, y, z, yaw])
```

## Test the Environment

You can test the environment using SB3's check_env function.

```python
from stable_baselines3.common.env_checker import check_env
# from file_name import class_name
from RoboEnv import RoboEnv # Import the environment you created, the names may be different

# Instantiate the environment
env = RoboEnv(RenderMode = True)
# Check the environment
check_env(env)
```
This will check that the environment is compatible with SB3, and that the action and observation spaces are defined correctly. If you get an error, check the error message and make sure that the action and observation spaces are defined correctly. If you get no output, then the environment is compatible with SB3.

Some errors that you may get:
- Obs not defined correctly in reset - make sure you create the observation in the reset function in the same way as you do in the step function
- Data type of obs/action not correct - make sure you are using the correct data type for the action and observation spaces. You may need to change the data type of the observation/action to np.float64. You can do this using the `np.array` function, with the `dtype` argument set to `np.float64`.

After you have verified that the environment is compatible with SB3, you can test the environment by running it for a few steps.

```python
# Reset the environment
obs = env.reset()
# Run for x steps with random actions
for i in range(10):
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action)
    # If you are working in a remote Jupyter notebook, you need to use the alternate rendering code you have been using before
    env.render() 
    if done:
        obs = env.reset()
```

Print your actions, rewards and observations to check that they are correct.
