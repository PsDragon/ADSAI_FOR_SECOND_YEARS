---
sort: 20
---
# Custom Gym Environment and RL

## Goals for the day:

- Develop a custom gym environment wrapper for the RoboSuite environment

If you have extra time:

- Use Stable Baselines 3 to start training a reinforcement learning agent to control the robotic arm
- Push the training job to the cloud to speed up training

### Expected Output

- A custom gym environment that can be used to train a reinforcement learning agent to control the robotic arm

## General Plan:
- [ ] Discuss any questions about the knowledge modules so far
- [ ] Develop a custom gym environment wrapper for the RoboSuite environment

If you have extra time:

- [ ] Use Stable Baselines 3 to start training a reinforcement learning agent to control the robotic arm
- [ ] Push the training job to the cloud to speed up training

## Create a Custom Gym Environment

Luckily, the RoboSuite environment is already compatible with the OpenAI Gym API, so we can create a simple wrapper to use it with Stable Baselines 3, with our own actions, observations, and reward function.

- Create a class that inherits from gym.Env

```python
import gym
from gym import spaces
import numpy as np

class RoboEnv(gym.Env):
    def __init__(self, RenderMode = True, Task = 'Lift'): # Add any arguments you need (Environment settings; Render mode  and task are used as examples)
        super(RoboEnv, self).__init__()
        # Initialize environment variables
        self.RenderMode = RenderMode
        self.Task = Task

        # Define action and observation space
        # They must be gym.spaces objects
        # Example when using discrete actions:
        self.action_space = # Define the action space
        # Example for using image as input:
        self.observation_space = # Define the observation space

        # Instantiate the environment
        self.env = suite.make(env_name= self.Task, 
                                robots="Panda",
                                has_renderer=self.RenderMode,
                                has_offscreen_renderer=False,
                                horizon=200,    
                                use_camera_obs=False,)


    def step(self, action):
        # Execute one time step within the environment
        action = # Process the action if needed
        #Call the environment step function
        obs, reward, done, _ = self.env.step(action)
        # You may find it useful to create helper functions for the following
        obs = # Process the observation if needed
        reward = # Calculate the reward for your specific task
        done = # Calculate if the episode is done
        return obs, reward, done, _

    def reset(self):
        # Reset the state of the environment to an initial state
        # Call the environment reset function
        obs = self.env.reset()
        # Process the observation if needed
        # Reset any variables that need to be reset
        return obs

    def render(self, mode='human'):
        # Render the environment to the screen
        # Call the environment render function

    def close (self):
        # Close the environment
        # Call the environment close function
```

## Test the Environment

You can test the environment using SB3's check_env function.

```python
from stable_baselines3.common.env_checker import check_env
# from file_name import class_name
from RoboEnv import RoboEnv # Import the environment you created, the names may be different

# Instantiate the environment
env = RoboEnv(RenderMode = True)
# Check the environment
check_env(env)
```
This will check that the environment is compatible with SB3, and that the action and observation spaces are defined correctly. If you get an error, check the error message and make sure that the action and observation spaces are defined correctly. If you get no output, then the environment is compatible with SB3.

After you have verified that the environment is compatible with SB3, you can test the environment by running it for a few steps.

```python
# Reset the environment
obs = env.reset()
# Run for x steps with random actions
for i in range(10):
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action)
    env.render()
    if done:
        obs = env.reset()
```

Print your actions, rewards and observations to check that they are correct.
