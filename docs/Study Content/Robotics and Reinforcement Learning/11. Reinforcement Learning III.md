---
sort: 21
---
# RL III: RL Models and their Parameters WORK IN PROGRESS

In this section we well look at several commonly used RL algorithms. The goal is to understand the basic concepts behind each algorithm and how they work. √ùou don't need to understand the math behind each algorithm, or implement them from scratch, but you should be able to explain the basic concepts and how they work. We will use `stable-baselines3` to implement these algorithms. It is important to understand hyperparameters and how they affect the performance of the algorithm. The `stable-baselines3` library has default values for these hyperparameters, but you will need to understand how to tune them for your specific task. 

For the project brief you will need to compare the performance of different RL algorithms on a specific task to each other and a PID controller as a baseline. You will also need to tune the hyperparameters of the algorithms to improve performance. 

## Temporal Difference Learning (TD Learning), SARSA (State-Action-Reward-State-Action), and Q-Learning

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/AJiG3ykOxmY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## Deep Q-Networks (DQN)

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/wrBUkpiRvCA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Bcuj2fTH4_4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/0bt0SjbS3xc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## REINFORCE

REINFORCE is a policy gradient method that uses the Monte Carlo method to estimate the gradient of the expected return. The policy gradient theorem states that the gradient of the expected return is equal to the expected gradient of the return. The expected gradient of the return is the sum of the gradients of the returns of all the trajectories that the agent has experienced. The REINFORCE algorithm uses the Monte Carlo method to estimate the expected gradient of the return. The Monte Carlo method estimates the expected gradient of the return by sampling trajectories from the agent's policy and calculating the gradient of the return for each trajectory. The REINFORCE algorithm then uses the estimated expected gradient of the return to update the agent's policy.

## Deep Deterministic Policy Gradients (DDPG)

Brief explanation, here refer to these docs for more info. Here are hyperparms you can change

- SB3 docs:
- OpenAI Spinning Up docs:
- Hyperparameters Tuning for DDPG:
    

## Asynchronous Advantage Actor Critic (A3C)/ Advantage Actor Critic (A2C)

- SB3 docs:
- OpenAI Spinning Up docs:
- Hyperparameters Tuning for A3C:

## Soft Actor Critic (SAC)

- SB3 docs:
- OpenAI Spinning Up docs:
- Hyperparameters Tuning for SAC:

## Trust Region Policy Optimization (TRPO)

<div style="text-align: center">
<iframe width="640" height="360" src="https://youtu.be/5P7I-xPq8u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- SB3 docs:
- OpenAI Spinning Up docs:
- Hyperparameters Tuning for TRPO:

## Proximal Policy Optimization (PPO)

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/5P7I-xPq8u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- SB3 docs:
- OpenAI Spinning Up docs:
- Hyperparameters Tuning for PPO:

## Twin Delayed Deep Deterministic Policy Gradients (TD3)


## Implementing Different RL Algorithms using Stable Baselines 3



## Imitation Learning (Optional)


## Additional Resources

- AlphaZero: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm

<iframe width="560" height="315" src="https://www.youtube.com/embed/MgowR4pq3e8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- OpenAI Five: Dota 2 AI vs. the World
<iframe width="560" height="315" src="https://www.youtube.com/embed/0eO2TSVVP1Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

- ChatGPT

<iframe width="560" height="315" src="https://www.youtube.com/embed/2MBJOuVq380" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>