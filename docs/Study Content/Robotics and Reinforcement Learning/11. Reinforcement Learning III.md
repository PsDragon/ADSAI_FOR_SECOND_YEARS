---
sort: 21
---
# RL III: RL Models and their Parameters

In this section we well look at several commonly used RL algorithms. The goal is to understand the basic concepts behind each algorithm and how they work. √ùou don't need to understand the math behind each algorithm, or implement them from scratch, but you should be able to explain the basic concepts and how they work. We will use `stable-baselines3` to implement these algorithms. It is important to understand hyperparameters and how they affect the performance of the algorithm. The `stable-baselines3` library has default values for these hyperparameters, but you will need to understand how to tune them for your specific task. 

For the project brief you will need to compare the performance of different RL algorithms on a specific task to each other and a PID controller as a baseline. You will also need to tune the hyperparameters of the algorithms to improve performance. 

The following is a useful guide from the Stable Baselines 3 documentation: [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#current-limitations-of-rl)

**The most important thing to understand from this module is that the performance of the RL algorithms is highly dependent on the _hyperparameters_. You will need to understand which hyperparameters to tune to improve performance.**

## Temporal Difference Learning (TD Learning), SARSA (State-Action-Reward-State-Action), and Q-Learning

These are the fundemental algorithms behind RL. They are the basis for most other RL algorithms. We will look at the basic concepts behind these algorithms and how they work, but we will not use them directly in our project. We will use them as a basis for understanding other more advanced and applicable algorithms, which we will use in our project.

Watch the following video to get an understanding of the basic concepts behind Temporal Difference Learning, SARSA, and Q-Learning. The video provides some detailed examples of how these algorithms work in very simple environments. Focus on the intuition behind the algorithms and how they work, rather than the math.

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/AJiG3ykOxmY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## Deep Q-Networks (DQN)

DQN is a type of reinforcement learning algorithm that utilizes a neural network to approximate the optimal action-value function (also known as the Q function). The Q function represents the expected reward of taking a specific action in a specific state. In DQN, the neural network is trained to predict the Q function by using a set of observed state-action pairs and their associated rewards. This process is similar to how a traditional neural network is trained, except that the training data for DQN comes from an agent interacting with its environment (e.g. a robot navigating through a maze).

In DQN the neural network to approximate the Q function, while Q learning uses a lookup table. This allows DQN to handle much larger and more complex environments, as it can learn to generalize from its experience rather than relying on a pre-defined set of state-action pairs. DQN also uses experience replay, which involves storing a buffer of past experiences and randomly sampling from this buffer to update the neural network. This helps to stabilize the learning process and improve the convergence of the algorithm.

DQN is sensitive to the choice of hyperparameters, such as the learning rate and exploration rate. Finding the optimal values for these hyperparameters can be time-consuming and may require significant experimentation. Most DQN implementations only work with discrete action spaces. This means that the agent can only take a finite number of actions. This is not a problem for many environments, such as games, but it can be a problem for more complex environments, such as robotics. DQN can be extended to work with continuous action spaces, but this requires additional modifications to the algorithm. You could also convert the continuous action space of the environment to a discrete action space by discretizing the action space into a finite number of actions.

The following videos provides a good overview of DQN and how it works. It also provides a good explanation of the hyperparameters and how they affect the performance of the algorithm.

- DQN Explained:
<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/wrBUkpiRvCA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- Experience Replay Memory:
<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Bcuj2fTH4_4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- Training Process Details: 
<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/0bt0SjbS3xc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Here are the docs for the DQN implementation in `stable-baselines3`:
- [SB3 docs](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)

## Asynchronous Advantage Actor Critic (A3C)/ Advantage Actor Critic (A2C)

Actor-critic methods use two separate components: an "actor" and a "critic." The actor is responsible for deciding which actions to take in a given state, while the critic is responsible for evaluating the actor's actions and providing feedback. The actor is usually implemented as a neural network that predicts the optimal action to take in a given state. The critic is also implemented as a neural network, and is trained to predict the expected future reward of taking a specific action in a specific state.

Actor-critic methods work by training the actor and critic networks in parallel. The actor takes an action in the environment, and the critic evaluates the action and provides feedback in the form of a reward signal. The actor uses this feedback to adjust its decision-making process, while the critic uses it to improve its prediction of the expected reward. This approach a useful way to improve the performance of reinforcement learning algorithms in complex environments, as they allow the agent to learn from both its own actions and the feedback provided by the critic.

A2C (Advantage Actor-critic) and A3C (Asynchronous advantage actor-critic) are variations of the actor-critic reinforcement learning algorithm.

- A2C is an on-policy actor-critic method that uses a single agent to learn from its own experiences. It works by updating the actor and critic networks at the end of each episode, using the accumulated reward and value estimates from the previous episode to compute the advantage function (the difference between the expected reward and the expected value).

- A3C, on the other hand, is an asynchronous actor-critic method that uses multiple agents to learn from a shared environment. Each agent maintains its own copy of the actor and critic networks, and updates these networks asynchronously based on the experiences of the other agents. This allows A3C to learn more efficiently and scale better to larger environments, as it can make use of multiple CPUs or GPUs to train the networks in parallel.

This blog post nicely summarizes the main ideas behind Actor Critic methods, and A2C and A3C in particular:

- [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)

This video explains the intuition behind actor critic methods:

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/w_3mmm0P0j8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Here are the docs for the A2C implementation in `stable-baselines3`:
- [SB3 docs](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)

### Hyperparameters Tuning for A2C:

- Learning rate: This determines the size of the update applied to the actor and critic networks at each training step. A higher learning rate may lead to faster learning, but can also cause instability or over-fitting.

- Discount rate: This determines the importance of future rewards in the learning process. A higher discount rate will give more weight to long-term rewards, while a lower discount rate will focus more on immediate rewards.

- Entropy coefficient: This determines the trade-off between exploration and exploitation in the learning process. A higher entropy coefficient will encourage the agent to explore more, while a lower entropy coefficient will encourage it to exploit known rewards.

- Batch size: This determines the number of experiences used to update the actor and critic networks at each training step. A larger batch size can improve the stability of the learning process, but may also require more computational resources.

## Deep Deterministic Policy Gradients (DDPG)

Deep Deterministic Policy Gradient (DDPG) is a reinforcement learning algorithm that is used to learn a control policy for an agent interacting with a complex, continuous environment. It combines ideas from the actor-critic method and Q-learning, and uses a neural network as a function approximator to represent the actor and critic.

One important aspect of DDPG is that it uses experience replay, where the agent stores a history of its interactions with the environment in a replay buffer and samples from this buffer to learn. This can help stabilize learning and improve sample efficiency.

DDPG is essentially an extension of DQN for continuous action spaces with other improvements. This blog post nicely summarizes the main ideas behind DDPG:

[Deep Deterministic Policy Gradients Explained](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)

Again focus on the intuition behind the algorithm and how it works, rather than the math.

Here are the docs for the DDPG implementation in `stable-baselines3`:
- [SB3 docs](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html)

More information in the details of implementation of DDPG can be found in the following docs:
- [OpenAI Spinning Up docs](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)

### Hyperparameters Tuning for DDPG:

- Learning rate: This determines the step size at which the algorithm updates the parameters of the actor and critic networks. A higher learning rate means that the algorithm will make more drastic changes to the network weights, while a lower learning rate means that the changes will be more subtle.

- Discount factor: This determines the extent to which the algorithm values future rewards over immediate rewards. A higher discount factor means that the algorithm will prioritize long-term rewards more, while a lower discount factor means that it will focus more on immediate rewards.

- Soft target update rate: In DDPG, the actor and critic networks each have a target network that is used to stabilize learning. The soft target update rate determines the rate at which the weights of the target network are updated to match the weights of the main network. A higher update rate means that the target network will more closely match the main network, while a lower update rate means that the target network will change more slowly.

- Exploration rate: In order to explore the environment and try new actions, DDPG uses an exploration noise process. The exploration rate determines the magnitude of the noise added to the action taken by the actor network. A higher exploration rate means that the agent will take more random actions, while a lower exploration rate means that it will stick more closely to the actions suggested by the actor network.
    
## Soft Actor Critic (SAC)

Soft actor-critic (SAC) combines the actor-critic framework with a probabilistic approach to decision-making. In SAC, the actor is responsible for predicting a distribution over actions rather than a single action. This allows the agent to explore its environment more effectively and learn a more diverse set of behaviors. The critic in SAC is responsible for evaluating the actions taken by the actor and providing feedback in the form of a reward signal. SAC also introduces a third component, called the "entropy critic," which is responsible for evaluating the entropy (randomness) of the action distribution predicted by the actor.

The goal of SAC is to maximize the expected reward while also maximizing the entropy of the action distribution. This allows the agent to explore its environment more effectively and learn a more diverse set of behaviors, while also avoiding getting stuck in local optima.

This blog post from Google AI show SAC being applied to robotics:
- [Soft Actor-Critic: Deep Reinforcement Learning for Robotics](https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html)

Here are the docs for the SAC implementation in `stable-baselines3`:
- [SB3 docs](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html)

More information in the details of implementation of DDPG can be found in the following docs:
- [OpenAI Spinning Up docs](https://spinningup.openai.com/en/latest/algorithms/sac.html)

### Hyperparameters Tuning for SAC:
- Learning rate: This determines the size of the update applied to the actor and critic networks at each training step. A higher learning rate may lead to faster learning, but can also cause instability or over-fitting.

- Discount rate: This determines the importance of future rewards in the learning process. A higher discount rate will give more weight to long-term rewards, while a lower discount rate will focus more on immediate rewards.

- Temperature: This determines the randomness or entropy of the action distribution predicted by the actor. A higher temperature will lead to more random actions, while a lower temperature will lead to more deterministic actions.

- Number of hidden units: This determines the number of neurons in the hidden layers of the actor and critic networks. A larger number of hidden units may improve the learning performance, but may also require more computational resources.

- Batch size: This determines the number of experiences used to update the actor and critic networks at each training step. A larger batch size can improve the stability of the learning process, but may also require more computational resources.

## Trust Region Policy Optimization (TRPO)

TRPO is a method for optimizing the policy by making small updates to its parameters. These updates are based on the gradient of the expected cumulative reward with respect to the policy parameters. The gradient tells us how the expected cumulative reward changes as we make small changes to the policy parameters.

To calculate the gradient, TRPO uses the "trust region" method. This involves approximating the gradient using a quadratic model, and then adjusting the size of the update based on how well the quadratic model fits the true gradient. This helps to ensure that the updates are always "trustworthy", in the sense that they are likely to lead to an improvement in the expected cumulative reward.

TRPO is an on-policy method, meaning that it uses data collected from the current policy to update the policy. It has been shown to be effective at learning policies for a wide range of RL tasks and is often used in combination with deep neural networks to learn complex policies.

The video below explains policy gradient methods in general and TRPO in particular:

<div style="text-align: center">
<iframe width="640" height="360" src="https://youtu.be/5P7I-xPq8u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Unfortunately, TRPO is not currently implemented in `stable-baselines3`. However, you can find an implementation in the [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html) library.

### Hyperparameters Tuning for TRPO:
- Learning rate: This determines the step size of the updates to the policy parameters. A larger learning rate will result in more aggressive updates, while a smaller learning rate will result in more cautious updates.

- Trust region size: This determines the size of the region around the current policy parameters in which the quadratic model is used to approximate the gradient. A larger trust region size will allow for more aggressive updates, while a smaller trust region size will result in more cautious updates.

- Discount factor: This determines the weight of future rewards in the expected cumulative reward. A larger discount factor will give more importance to future rewards, while a smaller discount factor will give more importance to immediate rewards.

- Entropy regularization coefficient: This determines the strength of the entropy regularization term in the objective function being optimized. Entropy regularization helps to encourage exploration by adding a term to the objective that encourages the policy to be more diverse.

- Maximum KL divergence: This determines the maximum allowed KL divergence between the old policy and the updated policy. KL divergence is a measure of the difference between two probability distributions, and the maximum KL divergence determines how much the updated policy is allowed to differ from the old policy.

- Number of epochs: This determines the number of times the gradient is calculated and the policy is updated in each iteration. A larger number of epochs will result in more updates to the policy, but may also increase the computation time.

## Proximal Policy Optimization (PPO)

Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that is used to learn a control policy for an agent interacting with a complex environment. It is an on-policy algorithm, which means that it uses the current policy to generate data and then updates the policy based on this data. PPO is an improvement over traditional policy gradient methods because it uses a trust region optimization technique to ensure that the new policy is not too different from the old one, which helps stabilize learning.

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/5P7I-xPq8u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Here are the docs for the PPO implementation in `stable-baselines3`:
- [SB3 docs](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)

More information in the details of implementation of PPO can be found in the following docs:

- [OpenAI Spinning Up docs](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

### Hyperparameters Tuning for PPO:
- Learning rate: This determines the step size at which the algorithm updates the parameters of the policy network. A higher learning rate means that the algorithm will make more drastic changes to the network weights, while a lower learning rate means that the changes will be more subtle.

- Discount factor: This determines the extent to which the algorithm values future rewards over immediate rewards. A higher discount factor means that the algorithm will prioritize long-term rewards more, while a lower discount factor means that it will focus more on immediate rewards.

- Clip factor: In PPO, the policy is updated using a ratio of the new and old policies. The clip factor determines the maximum allowable change in this ratio. A higher clip factor means that the algorithm will allow for more drastic changes in the policy, while a lower clip factor means that it will be more conservative in updating the policy.

- Mini-batch size: This determines the number of samples used to compute the gradient at each step of learning. A larger mini-batch size means that the algorithm will use more data to update the policy, which can improve sample efficiency but may also increase the variance of the gradients and lead to less stable learning.

- Number of epochs: This determines the number of times the algorithm will iterate over the data when updating the policy. A larger number of epochs means that the algorithm will spend more time optimizing the policy, but it may also increase the risk of over-fitting to the training data.


## Implementing Different RL Algorithms using Stable Baselines 3

Implementing RL algorithms from scratch can be a daunting task. Fortunately, there are many libraries that make it easy to implement RL algorithms. In this section, we will look at how to implement different RL algorithms using the `stable-baselines3` library.

All you need to do is import the algorithm you want to use from the `stable-baselines3` library and then create an instance of the algorithm. For example, to create an instance of the PPO algorithm, you can use the following code:

```python
from stable_baselines3 import PPO

model = PPO('MlpPolicy', 'CartPole-v1', verbose=1)
```

To create an instance of the A2C algorithm, you can use the following code:

```python
from stable_baselines3 import A2C

model = A2C('MlpPolicy', 'CartPole-v1', verbose=1)
```

You can specify the hyperparameters for the algorithm by passing them as arguments to the constructor. For example, to create an instance of the PPO algorithm with a learning rate of 0.0001, you can use the following code:

```python
from stable_baselines3 import PPO

model = PPO('MlpPolicy', 'CartPole-v1', verbose=1, learning_rate=0.0001)
```


## Additional Resources
Here are some detailed videos on implementing some of the algorithms in this section from scratch using PyTorch:

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/hlv79rcHws0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/6Yd5WnYls_Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/OcIx_TBu90Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ioidsRlf79o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

Below are some cool implementations of RL algorithms in action:

- AlphaZero: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm

<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/MgowR4pq3e8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- OpenAI Five: Dota 2 AI vs. the World
<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/0eO2TSVVP1Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

- How PPO is used in ChatGPT
<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/2MBJOuVq380" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>