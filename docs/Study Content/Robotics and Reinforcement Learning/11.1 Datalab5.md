---
sort: 22
---
# RL Training: Local and Remote

## Goals for the day:

- Use Stable Baselines 3 to start training a reinforcement learning agent using the custom gym environment we created in the last datalab.
- Start a local training job to confirm that the environment and agent are working correctly.
- Push the training job to the cloud to speed up training.
- Run 5 training jobs in parallel with different hyperparameters to find the best performing agent.

### Expected Output

- 1 local training job that runs for 100000 episodes
- 5 remote training jobs that run for 2500000 episodes 

## General Plan:
- [ ] Discuss any questions about the knowledge modules so far
- [ ] Review the datalab prep
- [ ] Review the custom gym environment we created in the last datalab
- [ ] Discuss the reward function we created in the last datalab
- [ ] Discuss the hyperparameters we will use for training
- [ ] Use Stable Baselines 3 to start training a reinforcement learning agent to control the robotic arm
- [ ] Push the training job to the cloud to speed up training

## Steps for Local Training

Below are the steps we will use to train the agent locally. We will use the same steps to train the agent remotely, but we will use the ClearML platform to manage the remote training jobs. If you don't know how to complete a step, you can find example code in the datalab prep.

1. Import the necessary libraries (including the custom gym environment we created in the last datalab)
1. Add command line arguments for the hyperparameters using argparse
1. Create an instance of the environment
1. Create the agent/model (make sure to include the hyperparameters specified in the command line arguments)
1. Setup Weights and Biases experiment tracking and callbacks
1. Train the agent/model using the learn method
1. Save the model incrementally

## Steps for Remote Training

Below are the steps we will use to train the agent remotely. 

1. Add the following code to the top of your training script after the imports:

```python
from clearml import Task

# Replace Pendulum-v1/YourName with your own project name (Folder/YourName, e.g. 2022-Y2B-RoboSuite/Michael)
task = Task.init(project_name='2022-Y2B-RoboSuite/YourName', task_name='Experiment1')
#copy these lines exactly as they are
#setting the base docker image
task.set_base_docker('deanis/robosuite:py3.8-2')
#setting the task to run remotely on the default queue
task.execute_remotely(queue_name="default")
```
2. Run the training script in the terminal with the desired hyperparameter arguments. For example:

```bash
python train.py --learning_rate 0.0001 --batch_size 64 --n_steps 2048 --n_epochs 10
```
- Note: Replace train.py with the name of your training script, and replace the hyperparameters with the ones you want to use.

3. Go to the ClearML dashboard (http://31.204.128.128:8080/projects/84d6c9779dcc46239544839f6055075f/projects) and click on the task you just created. You should see the task running in the dashboard. You can click on the task to see the logs and other information about the task.

## Downloading and Testing the Model

1. Download the model from the weights and biases dashboard:

- Go to the weights and biases dashboard (https://wandb.ai/yourname/yourprojectname)
- Click on the experiment you want to download
- Click on the "Files" tab
- Click on the "model.zip" file
- Click on the "Download" button

1. Copy the model.zip file to the same directory as your testing script

1. Setup the testing script to load the model and run the agent in the environment
    1. Import the necessary libraries (including the custom gym environment we created in the last datalab)
    1. Create an instance of the environment
    1. Create the agent/model by loading the model from the model.zip file (use the load method). For example if you trained a PPO agent, you would use the PPO.load method to load the model.

    ```python
    model = PPO.load("model.zip", print_system_info=True)
    ```
    1. Run the agent in the environment using the predict method. For example if you trained a PPO agent, you would use the PPO.predict method to run the agent in the environment.

    ```python
    obs = env.reset()
    for i in range(1000):
        action, _states = model.predict(obs, deterministic=True)
        obs, rewards, dones, info = env.step(action)
        env.render()
    ```

    note: you may need to create a virtual environment with the correct python and library versions to run the testing script with the loaded model. You can use the following commands to create a virtual environment and install the necessary libraries:

    ```bash
    python3 -m virtualenv testenv
    source testenv/bin/activate
    ```

