# Robotics IIb: Practical Control Examples

In this section we will look at some practical examples of PID control. We will look at an example of controlling a drone in 2D, then you will create your own controller for a pendulum (datalab prep), and finally you will use the RoboSuite environment to simulate a robotic arm lifting a block (Wednesdays datalab).

We will be using open AI gym environments for these examples.

## Open AI Gym Primer

Open AI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. 

Install the Open AI Gym library by running the following command in a terminal:

```bash
pip install gym
```

This can be done in windows or linux through WSL. If you are using a linux machine you may need to use `pip3` instead of `pip`. Some of the more advanced elements of the library do not work on windows.

Once the library is installed starting a new environment is done using the make function:

- `gym.make('MyGymEnvironment-v1')` - Creates an instance of the environment

The three main functions used to interact with a gym environment are:

- `env.reset()`: Resets the environment and returns the initial state.
- `env.step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `env.render()`: Renders the environment.

The following code snippet shows how to use these functions to interact with the CartPole environment:

```python
import gym
import time

env = gym.make('CartPole-v0')

# Reset the environment and get the initial state
state = env.reset()

# Run the simulation for 1000 steps
for _ in range(1000):
    # Render the environment
    env.render()
    
    # Take a random action
    action = env.action_space.sample()
    
    # Execute the action and get the next state, reward, and whether the episode is done
    state, reward, done, _ = env.step(action)

    # add a delay to slow down the env render
    # This is purely for visualization purposes, DO NOT use this when training!
    time.sleep(0.05)
    
    # If the episode is done, reset the environment
    if done:
        state = env.reset()
```
Try to run the code above and see what happens. You should see a window pop up that shows a cart moving back and forth. 

<div style="text-align: center">
<img src="Images/cartpole.gif" height="200" />
</div>

Now try loading the MountainCar environment and see what happens. You should see a window pop up that shows a car moving up and down a hill.

<div style="text-align: center">
<img src="Images/mountaincar.gif" height="200" />
</div>

Next, try loading the Pendulum environment and see what happens. You should see a window pop up that shows a pendulum swinging back and forth. The applied torque is also shown in the window.

<div style="text-align: center">
<img src="Images/pendulum.gif" height="200" />
</div>

A full list of the standard environments can be found [here](https://www.gymlibrary.dev/).

## Applied Example - Drone Altitude Control

This example will show how to use a PID controller to control the altitude of a drone. The drone will be controlled in 2D, so the only state variable will be the altitude. 



## Applied Example - Pendulum PID Control (Datalab Prep)

The `Pendulum-V1` environment is a simple inverted pendulum environment. Look up the documentation for the environment [here](https://www.gymlibrary.dev/environments/classic_control/pendulum/). 

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
Question 1: What are the actions that can be taken in this environment?
<br>
Question 2: What are the observations that are returned by the environment?
<details> 
  <summary>Click for Answers </summary>
   Q1 solution:  The action is a ndarray with shape (1,) representing the torque applied to free end of the pendulum as a `float` variable. The possible values range from -2.0 to 2.0. <br>
   Q2 solution:  The observation is a ndarray with shape (3,) representing the state of the system. The state is defined as: <br>
    - `theta`: The angle of the pendulum as a `float` variable. The angle is measured from the vertical and increases in the clockwise direction. The range is [-pi, pi].
    - `theta_dot`: The angular velocity of the pendulum as a `float` variable. The range is [-8, 8].

    <code>print('Hello')</code>

    
</details>
</div>

Below is some Python code to implement PID control to solve the pendulum environment. 

```python
import gym
import numpy as np
import time

env = gym.make('Pendulum-v1', g=4)
obs = env.reset()
dt = 0.025
# PID control parameters
kp = 0.1
ki = 0
kd = 0.01
# PID control variables
integral_error = 0
previous_error = 0
setpoint = 0

for t in range(10000):
    env.render()
    angle = np.rad2deg(np.arccos(obs[0]))
    if obs[1] < 0:
        angle = -angle
    error = setpoint - angle
    integral_error += error
    derivative_error = (error - previous_error)/dt
    previous_error = error
    action = (error * kp + integral_error * ki + derivative_error * kd)
    action = np.clip(action, -2, 2)
    obs, reward, done, info = env.step([action])
    time.sleep(dt)
    print('angle: ', np.round(angle), 'action: ', np.round(action, 2), 'error: ', np.round(error,2))

    if done:
        obs = env.reset()
```


<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
Disclaimer: Some of this module was written using AI suggestions by GitHub Copilot.   
</div>
