# Robotics IIb: Practical Control Examples

In this section we will look at some practical examples of PID control. We will look at an example of controlling a drone in 2D, then you will create your own controller for a pendulum (datalab prep), and finally you will use the RoboSuite environment to simulate a robotic arm lifting a block (Wednesdays datalab).

We will be using open AI gym environments for these examples.

## Open AI Gym Primer

Open AI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball. 

Install the Open AI Gym library by running the following command in a terminal:

```bash
pip install gym
```

This can be done in windows or linux through WSL. If you are using a linux machine you may need to use `pip3` instead of `pip`. Some of the more advanced elements of the library do not work on windows.

Once the library is installed starting a new environment is done using the make function:

- `gym.make('MyGymEnvironment-v1')` - Creates an instance of the environment

The three main functions used to interact with a gym environment are:

- `env.reset()`: Resets the environment and returns the initial state.
- `env.step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `env.render()`: Renders the environment.

The following code snippet shows how to use these functions to interact with the CartPole environment:

```python
import gym
import time

env = gym.make('CartPole-v0')

# Reset the environment and get the initial state
state = env.reset()

# Run the simulation for 1000 steps
for _ in range(1000):
    # Render the environment
    env.render()
    
    # Take a random action
    action = env.action_space.sample()
    
    # Execute the action and get the next state, reward, and whether the episode is done
    state, reward, done, _ = env.step(action)

    # add a delay to slow down the env render
    # This is purely for visualization purposes, DO NOT use this when training!
    time.sleep(0.05)
    
    # If the episode is done, reset the environment
    if done:
        state = env.reset()
```
Try to run the code above and see what happens. You should see a window pop up that shows a cart moving back and forth. 

<div style="text-align: center">
<img src="Images/cartpole.gif" height="200" />
</div>

Now try loading the MountainCar environment and see what happens. You should see a window pop up that shows a car moving up and down a hill.

<div style="text-align: center">
<img src="Images/mountaincar.gif" height="200" />
</div>

Next, try loading the Pendulum environment and see what happens. You should see a window pop up that shows a pendulum swinging back and forth. The applied torque is also shown in the window.

<div style="text-align: center">
<img src="Images/pendulum.gif" height="200" />
</div>

A full list of the standard environments can be found [here](https://www.gymlibrary.dev/).

## Applied Example - Drone Altitude Control

This example will show how to use a PID controller to control the altitude of a drone. The drone will be controlled in 2D, so the only state variable will be the altitude. 

You do not need to recreate this example, but you should try to understand how it works. If you are interested in recreating it, you can find the code [here]()

First we need to import the necessary libraries:

```python
import drone_2d
import gym
import time
import numpy as np
```

Next we need to create the environment and reset it:

```python
env = gym.make('drone-2d-v0')
state = env.reset()
```

Now we need to create a PID controller. We will start by initializing the gains and PID variables:

```python
# PID gains
kp = 1
ki = 0
kd = 0
# PID variables
error = 0
integral_error = 0
prev_error = 0
```

Next we need to create the main loop. This loop will run until the episode is done. Inside the loop we will calculate the error, then use the error to calculate the control input. The control input will then be used to execute the action. Finally, we will render the environment.

```python
# Run the simulation for 1000 steps
for _ in range(1000):
    
    # Calculate the error
    error = state[0] - 1
    
    # Calculate the integral error
    integral_error += error
    
    # Calculate the derivative error
    derivative_error = (error - prev_error)/dt
    
    # Calculate the control input
    control_input = kp * error + ki * integral_error + kd * derivative_error
    
    # Execute the action and get the next state, reward, and whether the episode is done
    state, reward, done, _ = env.step(control_input)
    
    # add a delay to slow down the env render
    time.sleep(0.05)
    
    # Render the environment
    env.render()

    # If the episode is done, reset the environment
    if done:
        state = env.reset()
```

Try running the code above and see what happens. You should see a window pop up that shows a drone flying up and down.



Next we will try to tune the PID gains to get the drone to hover at a constant altitude. Starting with the proportional gain.

Then we will try to tune the derivative gain.

Finally we will try to tune the integral gain.



## Datalab Prep - Pendulum PID Control

Now that you have seen an example of how to use a PID controller to control a drone, you will create your own controller for an inverted pendulum.

The `Pendulum-V1` environment is a simple inverted pendulum environment. Look up the documentation for the environment [here](https://www.gymlibrary.dev/environments/classic_control/pendulum/). 

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
Question 1: What are the actions that can be taken in this environment?
<br>
Question 2: What are the observations that are returned by the environment?
<details> 
  <summary>Click for Answers </summary>
   Q1 solution:  The action is a ndarray with shape (1,) representing the torque applied to free end of the pendulum as a `float` variable. The possible values range from -2.0 to 2.0. <br>
   Q2 solution:  The observation is a ndarray with shape (3,) representing the x-y coordinates of the pendulum’s free end and its angular velocity.<br>
    - x: The x-coordinate of the pendulum’s free end. <br>
    - y: The y-coordinate of the pendulum’s free end. <br>
    - theta_dot: The angular velocity of the pendulum.
    
</details>
</div>

Head over to the datalab prep section to complete the datalab prep for the data lab on Wednesday.

[Click here to prep](https://adsai.buas.nl/Study%20Content/Robotics%20and%20Reinforcement%20Learning/Datalab1-Prep.html)

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
Disclaimer: Some of this module was written using AI suggestions by GitHub Copilot.   
</div>
