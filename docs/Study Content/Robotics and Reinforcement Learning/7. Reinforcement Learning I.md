# Intro to Reinforcement Learning (RL)
## What is RL?

Reinforcement learning is a subfield of machine learning that is concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The agent receives a reward for each action it takes, and the goal is to maximize the total reward over some time period. The agent does not know which actions will yield the most reward, and must discover this through trial and error. 


<iframe width="560" height="315" src="https://www.youtube.com/embed/nIgIv4IfJ6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/0MNVhXEX9to" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## RL Applicatgions and Milestones
## General RL Theory
### Markov decision process

A Markov decision process (MDP) is a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. An MDP is a tuple (S, A, P, R, γ), where:

- S is the set of states
- A is the set of actions
- P is the state transition probability matrix
- R is the reward function
- γ is the discount factor

### RL 'cyle'

The RL cycle is the process of taking actions in an environment, observing the results, and using the results to improve future actions. The cycle is as follows:

1. Initialize the environment and the agent
2. Observe the state of the environment
3. Select an action
4. Execute the action and observe the reward and next state
5. Update the agent's policy
6. Repeat steps 2-5 until the environment is done

### Bellman equation

The Bellman equation is a recursive equation that describes the optimal value function for a given MDP. The optimal value function is the expected value of the total reward for all future steps given a state and an action. The Bellman equation is as follows:

$$V^*(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^*(s')]$$

Where:

- $V^*(s)$ is the optimal value function
- $s$ is the state
- $a$ is the action
- $s'$ is the next state
- $r$ is the reward
- $p(s',r|s,a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ given state $s$ and action $a$
- $\gamma$ is the discount factor

### Action and Observation Spaces

An action space is the set of all possible actions that an agent can take. An observation space is the set of all possible observations that an agent can receive. The action and observation spaces are defined by the environment. For example, the action space for the CartPole environment is a discrete space with two possible actions: 0 (push cart to the left) and 1 (push cart to the right). The observation space for the CartPole environment is a continuous space with four dimensions: the cart position, the cart velocity, the pole angle, and the pole velocity at tip.

### Reward Functions

In order to train an agent, we need to define a reward function. The reward function is a function that takes in the current state and action and returns a reward. The reward function is used to train the agent to take actions that maximize the total reward over time.

For example, in the cartpole environment, the reward function is defined as:

```python
def reward_function(state, action):
    x, x_dot, theta, theta_dot = state
    r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
    r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
    return r1 + r2
```

The reward function returns a reward of 1 if the pole is balanced on the cart, and a reward of 0 if the pole falls over. The reward function returns a reward of -1 if the pole falls over and the cart moves more than 2.4 units away from the center of the track.

Rewards can be sparse or dense. Sparse rewards are rewards that are only given at the end of an episode. Dense rewards are rewards that are given at every step of an episode. Sparse rewards are more difficult to train an agent to maximize, because the agent has to wait until the end of the episode to receive a reward. Dense rewards are easier to train an agent to maximize, because the agent receives a reward at every step of the episode. However, dense rewards can cause the agent to get stuck in local optima.


### Exploration vs Exploitation

A common problem in RL is the exploration vs exploitation dilemma. The agent must balance the need to explore new actions in order to discover new rewards with the need to exploit actions that have already been discovered to yield high rewards. This is a common problem in RL and there are many solutions to this problem.

### On-policy vs off-policy

On-policy methods learn directly from the policy being followed by the agent. Off-policy methods learn from a different policy than the one being followed by the agent.


### Model based vs model free

Model based methods learn a model of the environment and use that model to make predictions about the future. Model free methods do not learn a model of the environment and instead learn directly from experience.

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;">
Disclaimer: Some of this module was written using AI suggestions by GitHub Copilot.   
</div>

