# Intro to Reinforcement Learning (RL)

In this module we will learn about reinforcement learning, which is a type of machine learning that is used to train agents to perform tasks in a simulated environment. We will learn about the basic concepts of reinforcement learning, and then we will implement a reinforcement learning algorithm to solve a simulated robotic task.

After this module, you will be able to:

- [ ] Understand the basic concepts of reinforcement learning
- [ ] Understand the basic concepts of Markov Decision Processes (MDPs)


<iframe width="640" height="360" src="https://youtu.be/5P7I-xPq8u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/zR11FLZ-O9M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/JgvyzIkgxF0" title="An introduction to Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


## What is RL?

Reinforcement learning is a subfield of machine learning that is concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The agent receives a reward for each action it takes, and the goal is to maximize the total reward over some time period. The agent does not know which actions will yield the most reward, and must discover this through trial and error. 

The reinforcement learning problem is a type of machine learning problem where an agent learns to take actions in an environment in order to maximize a reward signal. In this problem, the agent is faced with a sequence of decisions and must choose the action that will maximize its expected reward. This problem is often used in applications where an agent must learn to take actions based on its observations of the environment, such as in games, robotics, and control systems. The goal of reinforcement learning is to develop algorithms that can learn to take optimal actions in a wide range of environments and tasks.


<iframe width="560" height="315" src="https://www.youtube.com/embed/nIgIv4IfJ6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/0MNVhXEX9to" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## RL Applications and Milestones

Milestones in the history of reinforcement learning:

- 1950: The first RL algorithm was proposed by Richard Sutton and Andrew Barto in their book Reinforcement Learning: An Introduction. This algorithm was called the Monte Carlo method.
- 1992: The first RL algorithm to be applied to a real-world problem was proposed by Richard Sutton and Andrew Barto. This algorithm was called the Q-learning algorithm.

Practical Applications of Reinforcement Learning:
- RL for Robotics
- RL for Games
- RL for Finance
- RL for Healthcare
- RL for Physics Simulations
- RL for Autonomous Vehicles
- RL for Mathematics
- RL for Large Language Models

## General RL Theory

### Components of RL

The components of RL are as follows:

- Agent: The agent is the entity that takes actions in the environment. The agent is the entity that is learning.
- Environment: The environment is the entity that the agent interacts with. The environment is the entity that the agent is learning from.
- State: The state is the current state of the environment. The state is the information that the agent has about the environment.
- Action: The action is the action that the agent takes in the environment. The action is the information that the agent has about the environment.
- Reward: The reward is the reward that the agent receives for taking an action in the environment. The reward is the information that the agent has about the environment.
- Policy: The policy is the agent's strategy for choosing actions. The policy is the information that the agent has about the environment.
- Value: The value is the expected reward that the agent will receive for taking an action in the environment. The value is the information that the agent has about the environment.

RL DIAGRAM HERE

The RL cycle is the process of taking actions in an environment, observing the results, and using the results to improve future actions. The cycle is as follows:

1. Initialize the environment and the agent
2. Observe the state of the environment
3. Select an action
4. Execute the action and observe the reward and next state
5. Update the agent's policy
6. Repeat steps 2-5 until the environment is done

Other useful terms:
- Model: The model is the agent's representation of the environment. The model is the information that the agent has about the environment.
- Goal: The goal is the goal that the agent is trying to achieve. The goal is the information that the agent has about the environment.
- Episode: The episode is a sequence of actions and rewards that the agent takes in the environment. The episode is the information that the agent has about the environment.
- Trajectory: The trajectory is a sequence of states, actions, and rewards that the agent takes in the environment. The trajectory is the information that the agent has about the environment.
- Horizon: The horizon is the maximum number of steps that the agent can take in the environment. The horizon is the information that the agent has about the environment.
- Discount factor: The discount factor is the rate at which the agent discounts future rewards. The discount factor is the information that the agent has about the environment.

### RL vs. Supervised Learning

Reinforcement learning is different from supervised learning in that the agent does not receive a label for each action it takes. Instead, the agent receives a reward for each action it takes. The agent must learn to take actions that maximize the total reward over some time period. The agent does not know which actions will yield the most reward, and must discover this through trial and error. 

### Markov decision process

A Markov decision process (MDP) is a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. An MDP is a tuple (S, A, P, R, γ), where:

- S is the set of states
- A is the set of actions
- P is the state transition probability matrix
- R is the reward function
- γ is the discount factor

### Policy and Value Functions

### Bellman equations

The Bellman equation is a recursive equation that describes the optimal value function for a given MDP. The optimal value function is the expected value of the total reward for all future steps given a state and an action. The Bellman equation is as follows:

$$V^*(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^*(s')]$$

Where:

- $V^*(s)$ is the optimal value function
- $s$ is the state
- $a$ is the action
- $s'$ is the next state
- $r$ is the reward
- $p(s',r|s,a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ given state $s$ and action $a$
- $\gamma$ is the discount factor

### Action and Observation Spaces

An action space is the set of all possible actions that an agent can take. An observation space is the set of all possible observations that an agent can receive. The action and observation spaces are defined by the environment. For example, the action space for the CartPole environment is a discrete space with two possible actions: 0 (push cart to the left) and 1 (push cart to the right). The observation space for the CartPole environment is a continuous space with four dimensions: the cart position, the cart velocity, the pole angle, and the pole velocity at tip.

- Episodic vs. Continuous Environments

- Discrete vs. Continuous Action Spaces

### Reward Functions

In order to train an agent, we need to define a reward function. The reward function is a function that takes in the current state and action and returns a reward. The reward function is used to train the agent to take actions that maximize the total reward over time.

For example, in the cartpole environment, the reward function is defined as:

```python
def reward_function(state, action):
    x, x_dot, theta, theta_dot = state
    r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
    r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
    return r1 + r2
```

The reward function returns a reward of 1 if the pole is balanced on the cart, and a reward of 0 if the pole falls over. The reward function returns a reward of -1 if the pole falls over and the cart moves more than 2.4 units away from the center of the track.

Rewards can be sparse or dense. Sparse rewards are rewards that are only given at the end of an episode. Dense rewards are rewards that are given at every step of an episode. Sparse rewards are more difficult to train an agent to maximize, because the agent has to wait until the end of the episode to receive a reward. Dense rewards are easier to train an agent to maximize, because the agent receives a reward at every step of the episode. However, dense rewards can cause the agent to get stuck in local optima.

- Reward Shaping




### Exploration vs Exploitation

A common problem in RL is the exploration vs exploitation dilemma. The agent must balance the need to explore new actions in order to discover new rewards with the need to exploit actions that have already been discovered to yield high rewards. This is a common problem in RL and there are many solutions to this problem. 

One solution is to use an epsilon-greedy policy. An epsilon-greedy policy is a policy that chooses a random action with probability epsilon, and chooses the best action with probability 1-epsilon. Another solution is to use a softmax policy. 

A softmax policy is a policy that chooses an action with probability proportional to the action's value. For example, if the agent has two actions, A and B, and the values of A and B are 0.5 and 0.7 respectively, then the agent will choose action A with probability 0.5 and action B with probability 0.7. 

### RL Hierarchy

The RL hierarchy is a framework for categorizing RL algorithms. The hierarchy is as follows:

- Value-based methods
- Policy-based methods
- Model-based methods
- Model-free methods
- On-policy methods
- Off-policy methods

DIAGRAM HERE

### Value-based vs policy-based

Value-based methods learn a value function that maps states to values. Policy-based methods learn a policy that maps states to actions. 

### On-policy vs off-policy

On-policy methods learn directly from the policy being followed by the agent. Off-policy methods learn from a different policy than the one being followed by the agent.

### Model based vs model free

Model based methods learn a model of the environment and use that model to make predictions about the future. Model free methods do not learn a model of the environment and instead learn directly from experience.

### Evolutionary algorithms

Evolutionary algorithms are a class of algorithms that use the principles of natural selection to find optimal solutions to problems. Evolutionary algorithms are a type of metaheuristic, which is an algorithm that uses heuristics to find approximate solutions to problems. 

