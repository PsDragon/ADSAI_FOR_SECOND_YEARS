# RL I: Intro to Reinforcement Learning (RL) WORK IN PROGRESS

In this module we will learn about reinforcement learning, which is a subset of machine learning that is used to train agents to perform tasks in a simulated environment. We will learn about the basic concepts of reinforcement learning, application, commonly used algorithms, and then we will implement a reinforcement learning algorithm to solve a simulated robotic task.

<div style="text-align: center">
<img src="https://www.researchgate.net/publication/340218250/figure/fig3/AS:873506086604800@1585271488717/The-relationship-between-AI-ML-RL-DL-and-DRL.ppm" height="300" />
</div>

After this module, you will be able to:

- [ ] Understand the basic building blocks of reinforcement learning.
- [ ] Describe practical applications of reinforcement learning.
- [ ] Describe Markov Decision Processes (MDPs), the credit assignment problem, the exploration vs. exploitation dilemma and other key concepts of reinforcement learning.
- [ ] Understand the key concepts of common deep reinforcement learning algorithms.
- [ ] Train deep reinforcement learning algorithms using the Open AI Gym and Stable Baselines 3 libraries, and use them to solve problems in simulated environments.

Here are three videos that introduce the basic concepts of reinforcement learning. The first video is a short introduction to reinforcement learning, the second video is a good summary of all key concepts with some examples and applications, and the third video goes into a lot of detail. Watch at least one these videos before continuing with this module.

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/JgvyzIkgxF0" title="An introduction to Reinforcement Learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/zR11FLZ-O9M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/2pWv7GOvuf0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## What is RL?

As mentioned in the videos, reinforcement learning is a subfield of machine learning that is concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The agent receives a reward for each action it takes, and the goal is to maximize the total reward over some time period. The agent does not know which actions will yield the most reward, and must discover this through trial and error. 

The reinforcement learning problem is a type of machine learning problem where an agent learns to take actions in an environment in order to maximize a reward signal. In this problem, the agent is faced with a sequence of decisions and must choose the action that will maximize its expected reward. This problem is often used in applications where an agent must learn to take actions based on its observations of the environment, such as in games, robotics, and control systems. The goal of reinforcement learning is to develop algorithms that can learn to take optimal actions in a wide range of environments and tasks.

- In RL data is collected by the agent based on its actions in the environment. This data is used to train the agent to take actions that maximize the reward signal. The same agent then collects more data by taking actions in the environment, and the process repeats until the agent has learned to take actions that maximize the reward signal.

The reinforcement learning problem is a type of machine learning problem where an agent learns to take actions in an environment in order to maximize a reward signal. In this problem, the agent is faced with a sequence of decisions and must choose the action that will maximize its expected reward. This problem is often used in applications where an agent must learn to take actions based on its observations of the environment, such as in games, robotics, and control systems. The goal of reinforcement learning is to develop algorithms that can learn to take optimal actions in a wide range of environments and tasks.


<div style="text-align: center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/nyjbcRQ-uQ8?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Reinforcement Learning Series Intro - Syllabus Overview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>



## RL Applications and Milestones

Milestones in the history of reinforcement learning:

- 1950: The first RL algorithm was proposed by Richard Sutton and Andrew Barto in their book Reinforcement Learning: An Introduction. This algorithm was called the Monte Carlo method.
- 1992: The first RL algorithm to be applied to a real-world problem was proposed by Richard Sutton and Andrew Barto. This algorithm was called the Q-learning algorithm.

Practical Applications of Reinforcement Learning:
- RL for Robotics
- RL for Games
- RL for Finance
- RL for Healthcare
- RL for Physics Simulations
- RL for Autonomous Vehicles
- RL for Mathematics
- RL for Large Language Models

Robotics: Reinforcement learning has been used to develop control algorithms for robots that can learn to perform tasks such as grasping objects and navigating through environments. For example, researchers at DeepMind used reinforcement learning to develop a robotic arm that could learn to grasp objects using a camera and a robotic hand.

Games: Reinforcement learning has been used to develop game-playing algorithms that can learn to play games such as chess, Go, and poker. For example, researchers at DeepMind used reinforcement learning to develop AlphaGo, an algorithm that could learn to play the game of Go and beat human champions.

Decision making: Reinforcement learning has been used to develop decision-making algorithms that can learn to make decisions based on observations of the environment, such as in finance and healthcare. For example, reinforcement learning has been used to develop algorithms that can learn to optimize the allocation of resources in financial markets, such as stocks and bonds, based on observations of the market.

Control systems: Reinforcement learning has been used to develop control algorithms for systems such as power grids and traffic networks, where the goal is to optimize the behavior of the system based on observations of the environment. For example, reinforcement learning has been used to develop algorithms that can learn to optimize the routing of traffic in a network of roads in order to minimize congestion and maximize the flow of vehicles.

Natural language processing: Reinforcement learning has been used to develop algorithms for natural language processing tasks, such as machine translation and dialogue systems. For example, researchers at OpenAI used reinforcement learning to develop a machine translation algorithm that could learn to translate text from one language to another without the need for large amounts of labeled data.

Autonomous vehicles: Reinforcement learning has been used to develop algorithms that can learn to control autonomous vehicles, such as cars and drones. For example, researchers at NVIDIA used reinforcement learning to develop a self-driving car that could learn to navigate through a city using a camera and lidar sensors.

Healthcare: Reinforcement learning has been used to develop algorithms that can learn to make decisions in healthcare, such as recommending treatments for patients. For example, researchers at the University of California, Berkeley used reinforcement learning to develop an algorithm that could learn to recommend cancer treatments based on observations of the patient's medical history and test results.

Personalization: Reinforcement learning has been used to develop algorithms that can learn to personalize experiences for users, such as recommending products or content. For example, researchers at Amazon used reinforcement learning to develop an algorithm that could learn to recommend products to users based on their past purchases and browsing history.

Supply chain optimization: Reinforcement learning has been used to develop algorithms that can learn to optimize the flow of goods in a supply chain, such as scheduling the production of goods and the routing of trucks. For example, researchers at Alibaba used reinforcement learning to develop an algorithm that could learn to optimize the scheduling of shipments in a supply chain network in order to minimize delays and maximize efficiency.

Internet of Things (IoT): Reinforcement learning has been used to develop algorithms that can learn to control the behavior of IoT devices, such as smart home appliances and wearable devices. For example, researchers at Google used reinforcement learning to develop an algorithm that could learn to control the energy consumption of smart thermostats in order to reduce electricity usage and save money.

## General RL Theory

### Components of RL

The components of RL are as follows:

- Agent: The agent is the entity that takes actions in the environment. The agent is the entity that is learning.
- Environment: The environment is the entity that the agent interacts with. The environment is the entity that the agent is learning from.
- State: The state is the current state of the environment. The state is the information that the agent has about the environment.
- Action: The action is the action that the agent takes in the environment. The action is the information that the agent has about the environment.
- Reward: The reward is the reward that the agent receives for taking an action in the environment. The reward is the information that the agent has about the environment.
- Policy: The policy is the agent's strategy for choosing actions. The policy is the information that the agent has about the environment.
- Value: The value is the expected reward that the agent will receive for taking an action in the environment. The value is the information that the agent has about the environment.

RL DIAGRAM HERE

The RL cycle is the process of taking actions in an environment, observing the results, and using the results to improve future actions. The cycle is as follows:

1. Initialize the environment and the agent
2. Observe the state of the environment
3. Select an action
4. Execute the action and observe the reward and next state
5. Update the agent's policy
6. Repeat steps 2-5 until the environment is done

Other useful terms:
- Model: The model is the agent's representation of the environment. The model is the information that the agent has about the environment.
- Goal: The goal is the goal that the agent is trying to achieve. The goal is the information that the agent has about the environment.
- Episode: The episode is a sequence of actions and rewards that the agent takes in the environment. The episode is the information that the agent has about the environment.
- Trajectory: The trajectory is a sequence of states, actions, and rewards that the agent takes in the environment. The trajectory is the information that the agent has about the environment.
- Horizon: The horizon is the maximum number of steps that the agent can take in the environment. The horizon is the information that the agent has about the environment.
- Discount factor: The discount factor is the rate at which the agent discounts future rewards. The discount factor is the information that the agent has about the environment.

### RL vs. Supervised Learning

Reinforcement learning is different from supervised learning in that the agent does not receive a label for each action it takes. Instead, the agent receives a reward for each action it takes. The agent must learn to take actions that maximize the total reward over some time period. The agent does not know which actions will yield the most reward, and must discover this through trial and error. 

### Markov decision process

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/my207WNoeyA?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Markov Decision Processes (MDPs) - Structuring a Reinforcement Learning Problem" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

A Markov decision process (MDP) is a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. An MDP is a tuple (S, A, P, R, γ), where:

- S is the set of states
- A is the set of actions
- P is the state transition probability matrix
- R is the reward function
- γ is the discount factor

Episodic vs. Continuous Environments


### Policy and Value Functions

### Bellman equations

The Bellman equation is a recursive equation that describes the optimal value function for a given MDP. The optimal value function is the expected value of the total reward for all future steps given a state and an action. The Bellman equation is as follows:

$$V^*(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^*(s')]$$

Where:

- $V^*(s)$ is the optimal value function
- $s$ is the state
- $a$ is the action
- $s'$ is the next state
- $r$ is the reward
- $p(s',r|s,a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ given state $s$ and action $a$
- $\gamma$ is the discount factor

### Action and Observation Spaces

An action space is the set of all possible actions that an agent can take. An observation space is the set of all possible observations that an agent can receive. The action and observation spaces are defined by the environment. For example, the action space for the CartPole environment is a discrete space with two possible actions: 0 (push cart to the left) and 1 (push cart to the right). The observation space for the CartPole environment is a continuous space with four dimensions: the cart position, the cart velocity, the pole angle, and the pole velocity at tip.

- Discrete vs. Continuous Action Spaces

### Reward Functions and Expected Returns

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/a-SnJtmBtyA?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Expected Return - What Drives a Reinforcement Learning Agent in an MDP" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

In order to train an agent, we need to define a reward function. The reward function is a function that takes in the current state and action and returns a reward. The reward function is used to train the agent to take actions that maximize the total reward over time.

For example, in the cartpole environment, the reward function is defined as:

```python
def reward_function(state, action):
    x, x_dot, theta, theta_dot = state
    r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
    r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
    return r1 + r2
```

The reward function returns a reward of 1 if the pole is balanced on the cart, and a reward of 0 if the pole falls over. The reward function returns a reward of -1 if the pole falls over and the cart moves more than 2.4 units away from the center of the track.

Rewards can be sparse or dense. Sparse rewards are rewards that are only given at the end of an episode. Dense rewards are rewards that are given at every step of an episode. Sparse rewards are more difficult to train an agent to maximize, because the agent has to wait until the end of the episode to receive a reward. Dense rewards are easier to train an agent to maximize, because the agent receives a reward at every step of the episode. However, dense rewards can cause the agent to get stuck in local optima.

- Reward Shaping




### Exploration vs Exploitation

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/mo96Nqlo1L8?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Exploration vs. Exploitation - Learning the Optimal Reinforcement Learning Policy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

A common problem in RL is the exploration vs exploitation dilemma. The agent must balance the need to explore new actions in order to discover new rewards with the need to exploit actions that have already been discovered to yield high rewards. This is a common problem in RL and there are many solutions to this problem. 

One solution is to use an epsilon-greedy policy. An epsilon-greedy policy is a policy that chooses a random action with probability epsilon, and chooses the best action with probability 1-epsilon. Another solution is to use a softmax policy. 

A softmax policy is a policy that chooses an action with probability proportional to the action's value. For example, if the agent has two actions, A and B, and the values of A and B are 0.5 and 0.7 respectively, then the agent will choose action A with probability 0.5 and action B with probability 0.7. 

### RL Taxonomy

The RL taxonomy is a framework for categorizing RL algorithms. The hierarchy is as follows:

- Value-based methods
- Policy-based methods
- Model-based methods
- Model-free methods
- On-policy methods
- Off-policy methods

<div style = "text-align: center">
<img src="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" height="300" />
</div>


### Value-based vs policy-based

Value-based methods learn a value function that maps states to values. Policy-based methods learn a policy that maps states to actions. 

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/eMxOGwbdqKY?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Policies and Value Functions - Good Actions for a Reinforcement Learning Agent" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### On-policy vs off-policy

On-policy methods learn directly from the policy being followed by the agent. Off-policy methods learn from a different policy than the one being followed by the agent.

### Model based vs model free

Model based methods learn a model of the environment and use that model to make predictions about the future. Model free methods do not learn a model of the environment and instead learn directly from experience.

### Evolutionary algorithms

Evolutionary algorithms are a class of algorithms that use the principles of natural selection to find optimal solutions to problems. Evolutionary algorithms are a type of metaheuristic, which is an algorithm that uses heuristics to find approximate solutions to problems. 



## Additional Resources

- [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) by Sutton and Barto
- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/) by OpenAI
- [UCL Course on RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) by David Silver
- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) by Sergey Levine
- [Deep Reinforcement Learning: Pong from Pixels](https://karpathy.github.io/2016/05/31/rl/) by Andrej Karpathy

Probably wont use these
<iframe width="560" height="315" src="https://www.youtube.com/embed/nIgIv4IfJ6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

