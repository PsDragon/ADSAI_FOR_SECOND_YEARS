# RL Ib: General RL Theory

## Components of RL

The components of RL are as follows:

- Agent: The agent is the entity that takes actions in the environment. The agent is the entity that is learning.
- Environment: The environment is the entity that the agent interacts with. The environment is the entity that the agent is learning from.
- State: The state is the current state of the environment. The state is the information that the agent has about the environment.
- Action: The action is the action that the agent takes in the environment. The action is the information that the agent has about the environment.
- Reward: The reward is the reward that the agent receives for taking an action in the environment. The reward is the information that the agent has about the environment.
- Policy: The policy is the agent's strategy for choosing actions. The policy is the information that the agent has about the environment.
- Value: The value is the expected reward that the agent will receive for taking an action in the environment. The value is the information that the agent has about the environment.

RL DIAGRAM HERE

The RL cycle is the process of taking actions in an environment, observing the results, and using the results to improve future actions. The cycle is as follows:

1. Initialize the environment and the agent
2. Observe the state of the environment
3. Select an action
4. Execute the action and observe the reward and next state
5. Update the agent's policy
6. Repeat steps 2-5 until the environment is done

Other useful terms:
- Model: The model is the agent's representation of the environment. The model is the information that the agent has about the environment.
- Goal: The goal is the goal that the agent is trying to achieve. The goal is the information that the agent has about the environment.
- Episode: The episode is a sequence of actions and rewards that the agent takes in the environment. The episode is the information that the agent has about the environment.
- Trajectory: The trajectory is a sequence of states, actions, and rewards that the agent takes in the environment. The trajectory is the information that the agent has about the environment.
- Horizon: The horizon is the maximum number of steps that the agent can take in the environment. The horizon is the information that the agent has about the environment.
- Discount factor: The discount factor is the rate at which the agent discounts future rewards. The discount factor is the information that the agent has about the environment.

## Markov decision process

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/my207WNoeyA?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Markov Decision Processes (MDPs) - Structuring a Reinforcement Learning Problem" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="640" height="360" src="https://www.youtube.com/embed/lfHX2hHRMVQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

A Markov decision process (MDP) is a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. An MDP is a tuple (S, A, P, R, γ), where:

- S is the set of states
- A is the set of actions
- P is the state transition probability matrix
- R is the reward function
- γ is the discount factor

Episodic vs. Continuous Environments


## Policy and Value Functions

### Bellman equations

The Bellman equation is a recursive equation that describes the optimal value function for a given MDP. The optimal value function is the expected value of the total reward for all future steps given a state and an action. The Bellman equation is as follows:

$$V^*(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^*(s')]$$

Where:

- $V^*(s)$ is the optimal value function
- $s$ is the state
- $a$ is the action
- $s'$ is the next state
- $r$ is the reward
- $p(s',r|s,a)$ is the probability of transitioning to state $s'$ and receiving reward $r$ given state $s$ and action $a$
- $\gamma$ is the discount factor

## Action and Observation Spaces

An action space is the set of all possible actions that an agent can take. An observation space is the set of all possible observations that an agent can receive. The action and observation spaces are defined by the environment. For example, the action space for the CartPole environment is a discrete space with two possible actions: 0 (push cart to the left) and 1 (push cart to the right). The observation space for the CartPole environment is a continuous space with four dimensions: the cart position, the cart velocity, the pole angle, and the pole velocity at tip.

- Discrete vs. Continuous Action Spaces

## Reward Functions and Expected Returns

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/a-SnJtmBtyA?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Expected Return - What Drives a Reinforcement Learning Agent in an MDP" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

In order to train an agent, we need to define a reward function. The reward function is a function that takes in the current state and action and returns a reward. The reward function is used to train the agent to take actions that maximize the total reward over time.

For example, in the cartpole environment, the reward function is defined as:

```python
def reward_function(state, action):
    x, x_dot, theta, theta_dot = state
    r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
    r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
    return r1 + r2
```

The reward function returns a reward of 1 if the pole is balanced on the cart, and a reward of 0 if the pole falls over. The reward function returns a reward of -1 if the pole falls over and the cart moves more than 2.4 units away from the center of the track.

Rewards can be sparse or dense. Sparse rewards are rewards that are only given at the end of an episode. Dense rewards are rewards that are given at every step of an episode. Sparse rewards are more difficult to train an agent to maximize, because the agent has to wait until the end of the episode to receive a reward. Dense rewards are easier to train an agent to maximize, because the agent receives a reward at every step of the episode. However, dense rewards can cause the agent to get stuck in local optima.

- Reward Shaping




## Exploration vs Exploitation

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/mo96Nqlo1L8?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Exploration vs. Exploitation - Learning the Optimal Reinforcement Learning Policy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; 
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe  width="640" height="360" src="https://www.youtube.com/embed/sGuiWX07sKw?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" title="RL Course by David Silver - Lecture 9: Exploration and Exploitation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

A common problem in RL is the exploration vs exploitation dilemma. The agent must balance the need to explore new actions in order to discover new rewards with the need to exploit actions that have already been discovered to yield high rewards. This is a common problem in RL and there are many solutions to this problem. 

One solution is to use an epsilon-greedy policy. An epsilon-greedy policy is a policy that chooses a random action with probability epsilon, and chooses the best action with probability 1-epsilon. Another solution is to use a softmax policy. 

A softmax policy is a policy that chooses an action with probability proportional to the action's value. For example, if the agent has two actions, A and B, and the values of A and B are 0.5 and 0.7 respectively, then the agent will choose action A with probability 0.5 and action B with probability 0.7. 

## RL Taxonomy

The RL taxonomy is a framework for categorizing RL algorithms. The hierarchy is as follows:

- Value-based methods
- Policy-based methods
- Model-based methods
- Model-free methods
- On-policy methods
- Off-policy methods

<div style = "text-align: center">
<img src="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" height="300" />
</div>


## Value-based vs policy-based

Value-based methods learn a value function that maps states to values. Policy-based methods learn a policy that maps states to actions. 

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/eMxOGwbdqKY?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv" title="Policies and Value Functions - Good Actions for a Reinforcement Learning Agent" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## Model based vs model free

Model based methods learn a model of the environment and use that model to make predictions about the future. Model free methods do not learn a model of the environment and instead learn directly from experience.

## On-policy vs off-policy

On-policy methods learn directly from the policy being followed by the agent. Off-policy methods learn from a different policy than the one being followed by the agent.

<div style = "text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/0g4j2k_Ggc4?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" title="RL Course by David Silver - Lecture 5: Model Free Control" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

## Evolutionary algorithms

Evolutionary algorithms are a class of algorithms that use the principles of natural selection to find optimal solutions to problems. Evolutionary algorithms are a type of metaheuristic, which is an algorithm that uses heuristics to find approximate solutions to problems.

You can read more about evolutionary algorithms in this blog: [A visual Guide to Evolutionary Algorithms](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/) 

<div style = "text-align: center">
<img src="https://blog.otoro.net/assets/20171031/rastrigin/simplega.gif" height="200" />
</div>


## Additional Resources

- [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) by Sutton and Barto
- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/) by OpenAI
- [UCL Course on RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) by David Silver
- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) by Sergey Levine
- [Deep Reinforcement Learning: Pong from Pixels](https://karpathy.github.io/2016/05/31/rl/) by Andrej Karpathy


