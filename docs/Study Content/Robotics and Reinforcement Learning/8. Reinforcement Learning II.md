---
sort: 16
---
# RL II: Applying RL WORK IN PROGRESS

In this module we will learn about applying reinforcement learning to solve robotic tasks. We will use the OpenAI Gym library, RoboSuite, and Stable Baselines 3 to create a reinforcement learning agent that can control a robotic arm to lift a block.

## Open AI Gym	

OpenAI Gym is a toolkit developed for developing and comparing reinforcement learning algorithms. You have already used the basics in creating PID control algorithms to use a benchmarks. It provides a variety of environments that simulate different types of agents (e.g. robots, creatures, or other types of characters) and tasks for them to solve. The goal of reinforcement learning is to train an agent to take actions in an environment that will maximize some reward. OpenAI Gym allows researchers and developers to easily create and compare different reinforcement learning algorithms in a standard set of environments, which makes it easier to develop and evaluate new ideas. 

You can find the full documentation for OpenAI Gym [here](https://www.gymlibrary.dev/). 

### Revision of the Basics

You should have already installed the Open AI Gym library by running the following command in a terminal:

```bash
pip install gym==0.21.0
```

This can be done in windows or linux through WSL. If you are using a linux machine you may need to use `pip3` instead of `pip`. Some of the more advanced elements of the library do not work on windows.

We will be using the version 0.21.0 of the library.

Once the library is installed starting a new environment is done using the make function:

- `gym.make('MyGymEnvironment-v1')` - Creates an instance of the environment

The three main functions used to interact with a gym environment are:

- `env.reset()`: Resets the environment and returns the initial state.
- `env.step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `env.render()`: Renders the environment.

https://blog.paperspace.com/getting-started-with-openai-gym/

### Spaces - Action & Observation



### Custom Environment Wrappers

You can make almost any environment into a gym environment by wrapping it in a gym environment wrapper. This is useful if you want to use a custom environment with a reinforcement learning algorithm, or if you want to modify the environment in some way. Like changing the reward function or done condition.

## Stable Baselines 3	

Stable Baselines 3 is a set of improved implementations of reinforcement learning algorithms based on PyTorch. It is a fork of Stable Baselines, a set of improved implementations of reinforcement learning algorithms based on TensorFlow. It provides a high-level interface for RL algorithms such as DQN, SAC, PPO, A2C, and TRPO. It also includes implementations of common helper functions such as replay buffers, policies, and value functions.

You can find the full documentation for Stable Baselines 3 [here](https://stable-baselines3.readthedocs.io/en/master/).

### Using Stable Baselines 3

Stable Baselines 3 can be installed using pip:

```bash
pip install stable-baselines3
```

In order to use Stable Baselines 3 you need to create an instance of the RL algorithm you want to use; and pass it the policy type (NN architecture) and environment. For example, to create an instance of the PPO algorithm you would use the following code:

```python
from stable_baselines3 import PPO

model = PPO('MlpPolicy', 'CartPole-v1', verbose=1)
```

Then you can train the model using the `learn` function:

```python
model.learn(total_timesteps=10000)
```

This will train the model for 10,000 timesteps. To demonstrate the model you can use the `predict` function:

```python
obs = env.reset()
for i in range(1000):
    action = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()
```

The video below explains these steps in more detail:

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/XbWhJdQgi7E?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Reinforcement Learning with Stable Baselines 3 - Introduction (P.1)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Saving and Loading Models

You can save and load models using the `save` and `load` functions:

```python
model.save("ppo_cartpole")
```

This will save the model to a file called `ppo_cartpole.zip`. To load the model you can use the `load` function:

```python
model = PPO.load("ppo_cartpole")
```

The video below explains these steps in more detail:
<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/dLP-2Y6yu70?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Saving and Loading Models - Stable Baselines 3 Tutorial (P.2)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Creating Custom Gym Environments

You can create custom gym environments by creating a class that inherits from the gym.Env class. This class must implement the following functions:

- `reset()`: Resets the environment and returns the initial state.
- `step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `render()`: Renders the environment.
- `close()`: Closes the environment.

The video below explains how to create a custom gym environment in more detail:

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/uKnjGn8fF70?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Custom Environments - Reinforcement Learning with Stable Baselines 3 (P.3)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Tweaking Custom Environment Rewards

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/yvwxbkKX9dc?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Tweaking Custom Environment Rewards - Reinforcement Learning with Stable Baselines 3 (P.4)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Useful Callbacks

## Exercise:


## Experiment Tracking with Weights & Biases

Weights & Biases is a tool for tracking machine learning experiments. It allows you to track hyperparameters, metrics, and artifacts such as models and data. It also provides a dashboard for visualizing the results of your experiments. You can find the full documentation for Weights & Biases [here](https://docs.wandb.com/).

You should have already created a Weights & Biases account and set everything up when training your object detection models. If you haven't done that yet you can find the instructions [here](https://docs.wandb.com/library/getting-started).

To track your RL experiments using Weights & Biases you need to set your api key and project name. You can find your api key on your [profile page](https://wandb.ai/authorize). You can set your API key using an environment variable in python:

```python
import os

os.environ['WANDB_API_KEY'] = 'INSERT_API_KEY_HERE'
```
Then you need to set the project name. You can do this using the `wandb.init` function:

```python
import wandb

wandb.init(project="sb3_cartpole")
```

To use Weights & Biases with Stable Baselines 3 you need to pass a callback to the `learn` function. This callback will log the metrics to Weights & Biases. You can create a callback using the `WandbCallback` function:

```python
from stable_baselines3.common.callbacks import WandbCallback

wandb_callback = WandbCallback(project="sb3_cartpole")
```



