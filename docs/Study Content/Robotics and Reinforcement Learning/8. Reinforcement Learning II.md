---
sort: 16
---
# RL II: Applying RL

In this module we will learn about applying reinforcement learning to solve robotic tasks. We will use the OpenAI Gym library, RoboSuite, and Stable Baselines 3 to create a reinforcement learning agent that can control a robotic arm to lift a block.

## Open AI Gym	

OpenAI Gym is a toolkit developed for developing and comparing reinforcement learning algorithms. You have already used the basics in creating PID control algorithms to use as benchmarks. It provides a variety of environments that simulate different types of agents (e.g. robots, creatures, or other types of characters) and tasks for them to solve. The goal of reinforcement learning is to train an agent to take actions in an environment that will maximize some reward. OpenAI Gym allows researchers and developers to easily create and compare different reinforcement learning algorithms in a standard set of environments, which makes it easier to develop and evaluate new ideas. We will be using RoboSuite to create a custom gym environment for our robotic arm.

You can find the full documentation for OpenAI Gym [here](https://www.gymlibrary.dev/). 

### Revision of the Basics

You should have already installed the Open AI Gym library by running the following command in a terminal (it is already installed by default in the remote Jupyter notebook environment):

```bash
pip install gym==0.21.0
```

You will need to do this in your Ubuntu environment (where you are using robosuite). 

Once the library is installed starting a new environment is done using the make function:

- `gym.make('MyGymEnvironment-v1')` - Creates an instance of the environment

The three main functions used to interact with a gym environment are:

- `env.reset()`: Resets the environment and returns the initial state.
- `env.step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `env.render()`: Renders the environment.

The following tutorial gives a details overview of gym and all it's key components and functions:

[Getting Started with OpenAI Gym](https://blog.paperspace.com/getting-started-with-openai-gym/)

Please read through this tutorial and make sure you understand the key components of gym before continuing.

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
Question 1a: How do you determine the type of actions a gym environment can accept without referring to the documentation?<br>
Question 1b: Can you list 3 different types of action spaces? <br>
Question 1c: How do you determine the type of observations a gym environment returns without referring to the documentation?
Question 1d: What is a vectorized environment and why are they useful?
</div>

## Stable Baselines 3	

Stable Baselines 3 is a set of improved implementations of reinforcement learning algorithms based on PyTorch. It is a fork of Stable Baselines, a set of improved implementations of reinforcement learning algorithms based on TensorFlow. It provides a high-level interface for RL algorithms such as DQN, SAC, PPO, A2C, and TRPO. It also includes implementations of common helper functions such as replay buffers, policies, and value functions. SB3 allows you to easily implement and test any of the major RL algorithms without needing to code them yourself. You simply load in the model, define the hyperparameters and let it learn on your gym environment.

You can find the full documentation for Stable Baselines 3 [here](https://stable-baselines3.readthedocs.io/en/master/).

### Using Stable Baselines 3

Stable Baselines 3 can be installed using pip:

```bash
pip install stable-baselines3
```

In order to use Stable Baselines 3 you need to create an instance of the RL algorithm you want to use; and pass it the policy type (NN architecture) and environment. For example, to create an instance of the PPO algorithm you would use the following code:

```python
from stable_baselines3 import PPO
import gym

env = gym.make('CartPole-v1')
model = PPO('MlpPolicy', env, verbose=1)
```

The `MlpPolicy` is a simple dense neural network with 2 hidden layers of 64 neurons each. More details on the available polices can be found [here](https://stable-baselines.readthedocs.io/en/master/modules/policies.html).

Once you have defined your model you can train it using the `learn` function:

```python
model.learn(total_timesteps=10000,progress_bar=True)
```

This will train the model for 10,000 timesteps. To demonstrate the model you can use the `predict` function:

```python
obs = env.reset()
for i in range(1000):
    action, _ = model.predict(obs,deterministic=True)
    obs, reward, done, info = env.step(action)
    env.render()
    if done:
        env.reset()
```
This most likely wont produce a good agent as you would need to train for around 100,000 time steps to solve the cart pole environment. Setting the `deterministic` parameter to `True` will make the model always take the action with the highest probability. If you set it to `False` it will sample from the probability distribution of the actions.

The video below explains these steps in more detail:

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/XbWhJdQgi7E?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Reinforcement Learning with Stable Baselines 3 - Introduction (P.1)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Saving and Loading Models

Once you have successfully trained a model you can save it to your PC using the `save` function:

```python
model.save("models/ppo_cartpole")
```

This will save the model to a file called `ppo_cartpole.zip` inside the folder `models`. To load the model you can use the `load` function:

```python
model = PPO.load("models/ppo_cartpole")
```

You can then use the loaded model to predict actions to take based on the current observation in the environment using the `predict` function, as shown above.

The video below explains these steps in more detail:
<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/dLP-2Y6yu70?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Saving and Loading Models - Stable Baselines 3 Tutorial (P.2)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Creating Custom Gym Environments

You can create custom gym environments by creating a class that inherits from the gym.Env class. This class must implement the following functions:

- `reset()`: Resets the environment and returns the initial state.
- `step(action)`: Executes the given action and returns the next state, reward, and whether the episode is done.
- `render()`: Renders the environment.
- `close()`: Closes the environment.

The video below explains how to create a custom gym environment in more detail:

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/uKnjGn8fF70?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Custom Environments - Reinforcement Learning with Stable Baselines 3 (P.3)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Tweaking Custom Environment Rewards

You can tweak the rewards of a custom environment by creating a custom reward wrapper. This wrapper will modify the reward returned by the environment. The video below explains how to create a custom reward wrapper in more detail:

<div style="text-align: center">
<iframe width="640" height="360" src="https://www.youtube.com/embed/yvwxbkKX9dc?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1" title="Tweaking Custom Environment Rewards - Reinforcement Learning with Stable Baselines 3 (P.4)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

### Useful Callbacks

Stable Baselines 3 provides a number of useful callbacks that can be used to monitor the training process. These include:

- `EvalCallback`: Evaluates the model at the end of a certain number of training episodes. 

```python
from stable_baselines3.common.callbacks import EvalCallback

eval_callback = EvalCallback(env, callback_on_new_best=callback_on_new_best, eval_freq=eval_freq, best_model_save_path=best_model_save_path, log_path=log_path, deterministic=True, render=False)
```

- `CheckpointCallback`: Saves the model at the end of a certain number of training episodes.

```python
from stable_baselines3.common.callbacks import CheckpointCallback

checkpoint_callback = CheckpointCallback(save_freq=save_freq, save_path=save_path, name_prefix=name_prefix)
```

- `EarlyStoppingCallback`: Stops training if the reward does not improve for a certain number of training episodes.

```python
from stable_baselines3.common.callbacks import EarlyStoppingCallback

early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=early_stopping_patience, verbose=verbose)
```

- `CallbackList`: Combines multiple callbacks into a single callback.

```python
from stable_baselines3.common.callbacks import CallbackList

callback = CallbackList([eval_callback, checkpoint_callback, early_stopping_callback])
```

Using callbacks in the learn function:

```python
model.learn(total_timesteps=total_timesteps, callback=callback)
```

## Experiment Tracking with Weights & Biases

Weights & Biases is a tool for tracking machine learning experiments. It allows you to track hyperparameters, metrics, and artifacts such as models and data. It also provides a dashboard for visualizing the results of your experiments. You can find the full documentation for Weights & Biases [here](https://docs.wandb.com/).

You should have already created a Weights & Biases account and set everything up when training your object detection models. If you haven't done that yet you can find the instructions [here](https://docs.wandb.com/library/getting-started).

To track your RL experiments using Weights & Biases you need to set your api key and project name. You can find your api key on your [profile page](https://wandb.ai/authorize). You can set your API key using an environment variable in python:

```python
import os

os.environ['WANDB_API_KEY'] = 'INSERT_API_KEY_HERE'
```
Then you need to set the project name. You can do this using the `wandb.init` function:

```python
import wandb

wandb.init(project="sb3_cartpole")
```

To use Weights & Biases with Stable Baselines 3 you need to pass a callback to the `learn` function. This callback will log the metrics to Weights & Biases. You can create a callback using the `WandbCallback` function:

```python
from stable_baselines3.common.callbacks import WandbCallback

wandb_callback = WandbCallback(project="sb3_cartpole")
```

## Exercise:

1. Solve the pendulum environment with `g=2`  using reinforcement learning.
1. Solve the pendulum environment with `g=9.81`  using reinforcement learning. Hint: This is a more difficult task so you will need to train for longer.

<div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
Were you able to achieve this with PID control and some swinging logic?
</div>

3. Try training with different models and hyperparameters and track your experiments using Weights and Biases. 





